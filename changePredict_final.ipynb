{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20df663f-7155-479c-8f0f-f60c2e5b510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/rahmang/xcomet/COMET\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d934a4c-6a34-472c-b1c0-2f5e58d8e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 16 21:24:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     Off |   00000000:01:00.0 Off |                    0 |\n",
      "|  0%   42C    P0             80W /  300W |     327MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    964053      C   ...home/rahmang/envs/xcomet/bin/python        318MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b4a31c-3071-4c8f-a39a-b65f82883519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-457cb514-0138-7a8c-26d6-18b5a663d37f]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_sample called by: prepare_for_inference (from /auto/brno2/home/rahmang/xcomet/COMET/comet/models/base.py)\n",
      "sample in the prepare_sample:  [{'src': 'Boris Johnson teeters on edge of favour with Tory MPs', 'mt': 'Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst', 'ref': 'Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe'}]\n",
      "the stage is:  predict\n",
      "K is:  src\n",
      "K is:  mt\n",
      "K is:  ref\n",
      "inputs[\"mt\"]: ['Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst']\n",
      "================COMET.comet.endcoders.base.subword_tokenize====================\n",
      "sample:  ['Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst']\n",
      "encoder_input:  {'input_ids': [[0, 67151, 59520, 443, 1079, 6653, 53, 9, 241832, 19, 86454, 23, 122, 25706, 271, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}\n",
      "====================================\n",
      "len(sample):  1\n",
      "the value of i is:  0\n",
      "encoder_input[i],   Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])  annotations[i]:  []\n",
      "===================================\n",
      "word_ids_batch:  [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]]\n",
      "+++++++++++++++++++++++++++++++++++\n",
      "input_sequences:  [{'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2]]), 'label_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]], 'word_ids': [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++inside predict_step function+++++++++++++++++++\n",
      "batch:  ({'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,  32686,  23962,     98, 121303,    111,   1238, 141775,    678,\n",
      "           6653,     53,  10646,      7,      2]], device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]], device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}, {'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,      7,    873,  54359,     18,  16587,   1079,   6653,     53,\n",
      "              9,   9088,      7,  16158,    644,    122,   1519,   7340,      2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]],\n",
      "       device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}, {'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,  32686,  23962,     98, 121303,    111,   1238, 141775,    678,\n",
      "           6653,     53,  10646,      7,      2,      2,  67151,  59520,      7,\n",
      "            873,  54359,     18,  16587,   1079,   6653,     53,      9,   9088,\n",
      "              7,  16158,    644,    122,   1519,   7340,      2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True]], device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]})\n",
      "i am inside when len of the batch is 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword probs : tensor([[[0.3554, 0.2145, 0.2102, 0.2199],\n",
      "         [0.5846, 0.1495, 0.1520, 0.1139],\n",
      "         [0.6238, 0.1366, 0.1445, 0.0952],\n",
      "         [0.1280, 0.1513, 0.2221, 0.4986],\n",
      "         [0.3183, 0.1446, 0.2166, 0.3205],\n",
      "         [0.5808, 0.1740, 0.1477, 0.0975],\n",
      "         [0.4583, 0.2637, 0.2076, 0.0703],\n",
      "         [0.4516, 0.2554, 0.2178, 0.0751],\n",
      "         [0.2695, 0.2651, 0.2737, 0.1917],\n",
      "         [0.2902, 0.2185, 0.2517, 0.2396],\n",
      "         [0.1104, 0.1495, 0.2237, 0.5164],\n",
      "         [0.1130, 0.1312, 0.1900, 0.5659],\n",
      "         [0.1128, 0.1399, 0.2107, 0.5366],\n",
      "         [0.1422, 0.1559, 0.2223, 0.4797],\n",
      "         [0.1456, 0.1444, 0.1989, 0.5111],\n",
      "         [0.5671, 0.1140, 0.1744, 0.1444]]], device='cuda:0')\n",
      "====================== decode function ========================\n",
      "subword_probs:  tensor([[[0.3554, 0.2145, 0.2102, 0.2199],\n",
      "         [0.5846, 0.1495, 0.1520, 0.1139],\n",
      "         [0.6238, 0.1366, 0.1445, 0.0952],\n",
      "         [0.1280, 0.1513, 0.2221, 0.4986],\n",
      "         [0.3183, 0.1446, 0.2166, 0.3205],\n",
      "         [0.5808, 0.1740, 0.1477, 0.0975],\n",
      "         [0.4583, 0.2637, 0.2076, 0.0703],\n",
      "         [0.4516, 0.2554, 0.2178, 0.0751],\n",
      "         [0.2695, 0.2651, 0.2737, 0.1917],\n",
      "         [0.2902, 0.2185, 0.2517, 0.2396],\n",
      "         [0.1104, 0.1495, 0.2237, 0.5164],\n",
      "         [0.1130, 0.1312, 0.1900, 0.5659],\n",
      "         [0.1128, 0.1399, 0.2107, 0.5366],\n",
      "         [0.1422, 0.1559, 0.2223, 0.4797],\n",
      "         [0.1456, 0.1444, 0.1989, 0.5111],\n",
      "         [0.5671, 0.1140, 0.1744, 0.1444]]], device='cuda:0')\n",
      "input_ids:  tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,  32686,  23962,     98, 121303,    111,   1238, 141775,    678,\n",
      "           6653,     53,  10646,      7,      2]], device='cuda:0')\n",
      "mt_offsets:  [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]\n",
      "length of mt_offsets:  1\n",
      "the value of i is before inner for loop:  0\n",
      "seq_len:  16\n",
      "the value of i is:  0\n",
      "token_id : tensor(0, device='cuda:0') , probs:  tensor([0.3554, 0.2145, 0.2102, 0.2199], device='cuda:0') , token_offset:  (0, 0)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(67151, device='cuda:0') , probs:  tensor([0.5846, 0.1495, 0.1520, 0.1139], device='cuda:0') , token_offset:  (0, 5)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(59520, device='cuda:0') , probs:  tensor([0.6238, 0.1366, 0.1445, 0.0952], device='cuda:0') , token_offset:  (5, 13)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(443, device='cuda:0') , probs:  tensor([0.1280, 0.1513, 0.2221, 0.4986], device='cuda:0') , token_offset:  (13, 17)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(1079, device='cuda:0') , probs:  tensor([0.3183, 0.1446, 0.2166, 0.3205], device='cuda:0') , token_offset:  (17, 21)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(6653, device='cuda:0') , probs:  tensor([0.5808, 0.1740, 0.1477, 0.0975], device='cuda:0') , token_offset:  (21, 25)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(53, device='cuda:0') , probs:  tensor([0.4583, 0.2637, 0.2076, 0.0703], device='cuda:0') , token_offset:  (25, 26)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(9, device='cuda:0') , probs:  tensor([0.4516, 0.2554, 0.2178, 0.0751], device='cuda:0') , token_offset:  (26, 27)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(241832, device='cuda:0') , probs:  tensor([0.2695, 0.2651, 0.2737, 0.1917], device='cuda:0') , token_offset:  (27, 38)\n",
      "===================================================\n",
      "label:  I-major\n",
      "the value of i is:  0\n",
      "token_id : tensor(19, device='cuda:0') , probs:  tensor([0.2902, 0.2185, 0.2517, 0.2396], device='cuda:0') , token_offset:  (38, 39)\n",
      "===================================================\n",
      "label:  O\n",
      "the value of i is:  0\n",
      "token_id : tensor(86454, device='cuda:0') , probs:  tensor([0.1104, 0.1495, 0.2237, 0.5164], device='cuda:0') , token_offset:  (39, 46)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(23, device='cuda:0') , probs:  tensor([0.1130, 0.1312, 0.1900, 0.5659], device='cuda:0') , token_offset:  (46, 49)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(122, device='cuda:0') , probs:  tensor([0.1128, 0.1399, 0.2107, 0.5366], device='cuda:0') , token_offset:  (49, 53)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(25706, device='cuda:0') , probs:  tensor([0.1422, 0.1559, 0.2223, 0.4797], device='cuda:0') , token_offset:  (53, 57)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(271, device='cuda:0') , probs:  tensor([0.1456, 0.1444, 0.1989, 0.5111], device='cuda:0') , token_offset:  (57, 59)\n",
      "===================================================\n",
      "label:  I-critical\n",
      "the value of i is:  0\n",
      "token_id : tensor(2, device='cuda:0') , probs:  tensor([0.5671, 0.1140, 0.1744, 0.1444], device='cuda:0') , token_offset:  (0, 0)\n",
      "===================================================\n",
      "label:  O\n",
      "span[tokens]:  [tensor(443, device='cuda:0'), tensor(1079, device='cuda:0')]\n",
      "span[tokens]:  [tensor(241832, device='cuda:0')]\n",
      "span[tokens]:  [tensor(86454, device='cuda:0'), tensor(23, device='cuda:0'), tensor(122, device='cuda:0'), tensor(25706, device='cuda:0'), tensor(271, device='cuda:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6365968585014343]\n",
      "0.6365968585014343\n",
      "[[{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21, 'check severity': ['critical', 'critical']}, {'text': 'Abgeordnete', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 27, 'end': 38, 'check severity': ['major']}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59, 'check severity': ['critical', 'critical', 'critical', 'critical', 'critical']}]]\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "from comet.models.utils import Prediction\n",
    "from typing import Dict, Optional\n",
    "from typing import List, Dict\n",
    "from typing import Union, Tuple\n",
    "import inspect \n",
    "import torch\n",
    "import torch.nn as nn  # <-- Add this\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "\n",
    "    def prepare_sample(\n",
    "        self, sample: List[Dict[str, Union[str, float]]], stage: str = \"fit\"\n",
    "    ) -> Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Tokenizes input data and prepares targets for training.\n",
    "\n",
    "        Args:\n",
    "            sample (List[Dict[str, Union[str, float]]]): Mini-batch\n",
    "            stage (str, optional): Model stage ('train' or 'predict'). Defaults to \"fit\".\n",
    "\n",
    "        Returns:\n",
    "            Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]: Model input\n",
    "                and targets.\n",
    "        \"\"\"\n",
    "        # Get the caller's function name and file location\n",
    "        caller_frame = inspect.stack()[1]\n",
    "        caller_function = caller_frame.function\n",
    "        caller_file = caller_frame.filename\n",
    "    \n",
    "        # Print caller details\n",
    "        print(f\"prepare_sample called by: {caller_function} (from {caller_file})\")\n",
    "        print(\"sample in the prepare_sample: \", sample)\n",
    "        print(\"the stage is: \", stage)\n",
    "        for k in sample[0]:\n",
    "            print(\"K is: \", k)\n",
    "        inputs = {k: [d[k] for d in sample] for k in sample[0]}\n",
    "        print(f'''inputs[\"mt\"]: {inputs[\"mt\"]}''')\n",
    "        input_sequences = [\n",
    "            self.encoder.prepare_sample(inputs[\"mt\"], self.word_level, None),\n",
    "        ]\n",
    "        print(\"input_sequences: \", input_sequences)\n",
    "        src_input, ref_input = False, False\n",
    "        if (\"src\" in inputs) and (\"src\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"src\"]))\n",
    "            src_input = True\n",
    "\n",
    "        if (\"ref\" in inputs) and (\"ref\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"ref\"]))\n",
    "            ref_input = True\n",
    "\n",
    "        unified_input = src_input and ref_input\n",
    "        model_inputs = self.concat_inputs(input_sequences, unified_input)\n",
    "        if stage == \"predict\":\n",
    "            return model_inputs[\"inputs\"]\n",
    "\n",
    "        scores = [float(s) for s in inputs[\"score\"]]\n",
    "        targets = Target(score=torch.tensor(scores, dtype=torch.float))\n",
    "\n",
    "        if \"system\" in inputs:\n",
    "            targets[\"system\"] = inputs[\"system\"]\n",
    "\n",
    "        if self.word_level:\n",
    "            # Labels will be the same accross all inputs because we are only\n",
    "            # doing sequence tagging on the MT. We will only use the mask corresponding\n",
    "            # to the MT segment.\n",
    "            seq_len = model_inputs[\"mt_length\"].max()\n",
    "            targets[\"mt_length\"] = model_inputs[\"mt_length\"]\n",
    "            targets[\"labels\"] = model_inputs[\"inputs\"][0][\"label_ids\"][:, :seq_len]\n",
    "\n",
    "        return model_inputs[\"inputs\"], targets\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        mt_offsets: torch.Tensor,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Decode error spans from subwords.\n",
    "\n",
    "        Args:\n",
    "            subword_probs (torch.Tensor): probabilities of each label for each subword.\n",
    "            input_ids (torch.Tensor): input ids from the model.\n",
    "            mt_offsets (torch.Tensor): subword offsets.\n",
    "\n",
    "        Return:\n",
    "            List with of dictionaries with text, start, end, severity and a\n",
    "            confidence score which is the average of the probs for that label.\n",
    "        \"\"\"\n",
    "        print(\"====================== decode function ========================\")\n",
    "        print(\"subword_probs: \", subword_probs)\n",
    "        print(\"input_ids: \", input_ids)\n",
    "        print(\"mt_offsets: \", mt_offsets)\n",
    "        decoded_output = []\n",
    "        print(\"length of mt_offsets: \", len(mt_offsets))\n",
    "        for i in range(len(mt_offsets)):\n",
    "            print(\"the value of i is before inner for loop: \", i)\n",
    "            seq_len = len(mt_offsets[i])\n",
    "            print(\"seq_len: \", seq_len)\n",
    "            error_spans, in_span, span = [], False, {}\n",
    "            for token_id, probs, token_offset in zip(\n",
    "                input_ids[i, :seq_len], subword_probs[i][:seq_len], mt_offsets[i]\n",
    "            ):  \n",
    "                print(\"the value of i is: \", i)\n",
    "                print(\"token_id :\", token_id, \", probs: \", probs, \", token_offset: \", token_offset)\n",
    "                if self.decoding_threshold:\n",
    "                    if torch.sum(probs[1:]) > self.decoding_threshold:\n",
    "                        probability, label_value = torch.topk(probs[1:], 1)\n",
    "                        label_value += 1  # offset from removing label 0\n",
    "                    else:\n",
    "                        # This is just to ensure same format but at this point\n",
    "                        # we will only look at label 0 and its prob\n",
    "                        probability, label_value = torch.topk(probs[0], 1)\n",
    "                        print(\"probs[0] =============\", probs[0])\n",
    "                else:\n",
    "                    probability, label_value = torch.topk(probs, 1)\n",
    "\n",
    "                # Some torch versions topk returns a shape 1 tensor with only\n",
    "                # a item inside\n",
    "                label_value = (\n",
    "                    label_value.item()\n",
    "                    if label_value.dim() < 1\n",
    "                    else label_value[0].item()\n",
    "                )\n",
    "                label = self.label_encoder.ids_to_label.get(label_value)\n",
    "                print(\"===================================================\")\n",
    "                print(\"label: \", label)\n",
    "                # Label set:\n",
    "                # O I-minor I-major\n",
    "                # Begin of annotation span\n",
    "                if label.startswith(\"I\") and not in_span:\n",
    "                    in_span = True\n",
    "                    span[\"tokens\"] = [\n",
    "                        token_id,\n",
    "                    ]\n",
    "                    span[\"severity\"] = label.split(\"-\")[1]\n",
    "                    span[\"offset\"] = list(token_offset)\n",
    "                    span[\"confidence\"] = [\n",
    "                        probability,\n",
    "                    ]\n",
    "                    span[\"check severity\"] = [label.split(\"-\")[1]]\n",
    "\n",
    "                # Inside an annotation span\n",
    "                elif label.startswith(\"I\") and in_span:\n",
    "                    span[\"tokens\"].append(token_id)\n",
    "                    span[\"confidence\"].append(probability)\n",
    "                    # Update offset end\n",
    "                    span[\"offset\"][1] = token_offset[1]\n",
    "                    span[\"check severity\"].append(label.split(\"-\")[1])\n",
    "                # annotation span finished.\n",
    "                elif label == \"O\" and in_span:\n",
    "                    error_spans.append(span)\n",
    "                    in_span, span = False, {}\n",
    "\n",
    "            sentence_output = []\n",
    "            for span in error_spans:\n",
    "                print(\"span[tokens]: \", span[\"tokens\"])\n",
    "                sentence_output.append(\n",
    "                    {\n",
    "                        \n",
    "                        \"text\": self.encoder.tokenizer.decode(span[\"tokens\"]),\n",
    "                        \"confidence\": torch.concat(span[\"confidence\"]).mean().item(),\n",
    "                        \"severity\": span[\"severity\"],\n",
    "                        \"start\": span[\"offset\"][0],\n",
    "                        \"end\": span[\"offset\"][1],\n",
    "                        \"check severity\": span[\"check severity\"]\n",
    "                    }\n",
    "                )\n",
    "            decoded_output.append(sentence_output)\n",
    "        return decoded_output\n",
    "    \n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: Dict[str, torch.Tensor],\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Prediction:\n",
    "        \"\"\"PyTorch Lightning predict_step\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, torch.Tensor]): The output of your prepare_sample function\n",
    "            batch_idx (Optional[int], optional): Integer displaying which batch this is\n",
    "                Defaults to None.\n",
    "            dataloader_idx (Optional[int], optional): Integer displaying which\n",
    "                dataloader this is. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Prediction: Model Prediction\n",
    "        \"\"\"\n",
    "        if len(batch) == 3:\n",
    "            print(\"++++++++++++++++++++++inside predict_step function+++++++++++++++++++\")\n",
    "            print(\"batch: \", batch)\n",
    "            print(\"i am inside when len of the batch is 3\")\n",
    "            predictions = [self.forward(**input_seq) for input_seq in batch]\n",
    "            #print(\"predictions: \", predictions)\n",
    "            avg_scores = torch.stack([pred.score for pred in predictions], dim=0).mean(dim=0)\n",
    "            #print(\"avg scores\", avg_scores)\n",
    "            batch_prediction = Prediction(\n",
    "                scores=avg_scores,\n",
    "                metadata=Prediction(\n",
    "                    src_scores=predictions[0].score,\n",
    "                    ref_scores=predictions[1].score,\n",
    "                    unified_scores=predictions[2].score,\n",
    "                ),\n",
    "            )\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = [\n",
    "                    nn.functional.softmax(o.logits, dim=2)[:, :seq_len, :] * w\n",
    "                    for w, o in zip(self.input_weights_spans, predictions)\n",
    "                ]\n",
    "                subword_probs = torch.sum(torch.stack(subword_probs), dim=0)\n",
    "                print(\"subword probs :\", subword_probs)\n",
    "                tokenizer = self.encoder.tokenizer\n",
    "                # check if you can access the word ids\n",
    "                # print(\"batch[word_ids] :==================\")\n",
    "                # for sample in batch:\n",
    "                #     print(\"word_ids:\", sample[\"word_ids\"])  # Access per-sample word_ids\n",
    "                # ====== Reconstruct MT sentence from batch ======\n",
    "                # input_ids = batch[0][\"input_ids\"]  # Tokenized MT input\n",
    "                # mt_sentence = self.encoder.tokenizer.decode(\n",
    "                #         input_ids[0],\n",
    "                #         skip_special_tokens=True,\n",
    "                #         clean_up_tokenization_spaces=True\n",
    "                # )\n",
    "\n",
    "                # # Tokenize the MT sentence to get subword-to-token alignment\n",
    "                # tokenized = self.encoder.tokenizer(\n",
    "                #         mt_sentence,\n",
    "                #         return_offsets_mapping=True,\n",
    "                #         return_tensors=\"pt\",\n",
    "                #         truncation=True,\n",
    "                # )\n",
    "\n",
    "                # subword_ids = tokenized.word_ids()\n",
    "                # print(\"subword_ids after mt sentence extractions: \", subword_ids)\n",
    "                # # Group subword probabilities by original tokens\n",
    "                # token_probs = {}\n",
    "                # print(\"subword_probs[0]: \", subword_probs[0])\n",
    "                # for idx, prob in enumerate(subword_probs[0]):\n",
    "                #     subword_idx = subword_ids[idx]\n",
    "                #     if subword_idx is None:  # Skip special tokens\n",
    "                #         continue\n",
    "                #     if subword_idx not in token_probs:\n",
    "                #         token_probs[subword_idx] = []\n",
    "                #     token_probs[subword_idx].append(prob.cpu().numpy())\n",
    "                # print(\"token_probs: \", token_probs)\n",
    "                # # Aggregate probabilities ( max for each class)\n",
    "                # token_level_probs = []\n",
    "                # for token_idx in sorted(token_probs.keys()):\n",
    "                #     probs = torch.stack([torch.tensor(p) for p in token_probs[token_idx]])\n",
    "                #     max_probs, _ = torch.max(probs, dim=0)\n",
    "                #     token_level_probs.append(max_probs.numpy())\n",
    "                # print(\"token_level_probs: \", token_level_probs)\n",
    "                \n",
    "                # # Extract word IDs (index of the original word for each token)\n",
    "                # word_ids = tokenized.word_ids()[:mt_length]    #  [None, 0, 0, 1, 1, 2, ...]\n",
    "\n",
    "                # # Convert token IDs to tokens (subwords)\n",
    "                # tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "                # print(\"tokens: \",tokens)\n",
    "                # # Group tokens by their word ID\n",
    "                # word_to_tokens = {}\n",
    "                # print(\"word_ids: \", word_ids)\n",
    "                # for idx, word_id in enumerate(word_ids):\n",
    "                #     if word_id is None:\n",
    "                #         continue  # Skip special tokens like [CLS], [SEP]\n",
    "                #     if word_id not in word_to_tokens:\n",
    "                #         word_to_tokens[word_id] = []\n",
    "                #     word_to_tokens[word_id].append(tokens[idx])\n",
    "                # print(\"word_to_tokens: \", word_to_tokens)\n",
    "\n",
    "                # print(\"sorted(word_to_tokens.keys()) :\", sorted(word_to_tokens.keys()))\n",
    "                # # Reconstruct original words from grouped tokens\n",
    "                # word_mapping = []\n",
    "                # for word_id in sorted(word_to_tokens.keys()):\n",
    "                #     tokens = word_to_tokens[word_id]\n",
    "                #     # Merge subwords into a single string (handles ## prefixes)\n",
    "                #     word = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "                #     word_mapping.append(word)\n",
    "\n",
    "                # # Print results\n",
    "                # print(\"Tokenized Words:\", word_mapping)\n",
    "                # # Map tokens to probabilities\n",
    "                # token_predictions = [\n",
    "                #         {\"token\": token, \"probabilities\": probs.tolist()}\n",
    "                #         for token, probs in zip(word_mapping, token_level_probs)\n",
    "                # ]\n",
    "\n",
    "                # print(\"Token-Level Probabilities:\")\n",
    "                # for pred in token_predictions:\n",
    "                #     print(f\"{pred['token']}: {pred['probabilities']}\")\n",
    "\n",
    "\n",
    "\n",
    "                ########################################################3\n",
    "                ## create error span using decode function\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction.metadata[\"error_spans\"] = error_spans\n",
    "        else:\n",
    "            print(\"i am inside when len of the batch is not 3\")\n",
    "            model_output = self.forward(**batch[0])\n",
    "            batch_prediction = Prediction(scores=model_output.score)\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = nn.functional.softmax(model_output.logits, dim=2)[:, :seq_len, :]\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction = Prediction(\n",
    "                    scores=model_output.score,\n",
    "                    metadata=Prediction(error_spans=error_spans),\n",
    "                )\n",
    "        return batch_prediction\n",
    "\n",
    "# Load checkpoint into your custom class\n",
    "path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(path,strict = False)\n",
    "data = [\n",
    "    {\n",
    "        \"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n",
    "        \"mt\": \"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\",\n",
    "        \"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n",
    "    }\n",
    "]\n",
    "# data = [\n",
    "#     {\n",
    "#         \"src\": \"10 到 15 分钟可以送到吗\",\n",
    "#         \"mt\": \"Can I receive my food in 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"src\": \"Pode ser entregue dentro de 10 a 15 minutos?\",\n",
    "#         \"mt\": \"Can you send it for 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     }\n",
    "# ]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a52f66-0d17-4873-8278-00c9fe643a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
