{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31984b68-9df0-48bd-b0e9-54024ad469e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f207b3-818e-4a2b-9daa-47dc0f241754",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_edit_file = \"/storage/brno2/home/rahmang/xcomet/arafat_comet/COMET_GR/postedition_aligned.community.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a45a169-a0e0-4220-a93d-2f29a55f8561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id_hal  Translation_id  Postedit_id  line_id  \\\n",
      "0  1988871            7033          402        0   \n",
      "1  1988871            7033          402        1   \n",
      "2  1988871            7033          402        2   \n",
      "3  1988871            7033          402        3   \n",
      "4  1988871            7033          402        4   \n",
      "5  1988871            7033          402        5   \n",
      "6  1821049            6181          159        0   \n",
      "7  1821049            6181          159        1   \n",
      "8  1821049            6181          159        2   \n",
      "9  1821049            6181          159        3   \n",
      "\n",
      "                                              source  \\\n",
      "0  Transforming Dependency Structures to LTAG Der...   \n",
      "1  We propose a new algorithm for parsing Lexical...   \n",
      "2  That is, given a sentence and its correspondin...   \n",
      "3  Moreover, we prove that this algorithm has a l...   \n",
      "4  This algorithm returns all compatible derivati...   \n",
      "5  This result is of practical interest to the de...   \n",
      "6  Corpus Based Machine Translation for Scientifi...   \n",
      "7  From many years, machine translation and compu...   \n",
      "8  In order to fulfill the goal of machine transl...   \n",
      "9  All of these translation methods differ in the...   \n",
      "\n",
      "                                         translation  \\\n",
      "0  Transformation de structures de dépendances en...   \n",
      "1  Nous proposons un nouvel algorithme pour analy...   \n",
      "2  C'est-à-dire, compte tenu d'une phrase et de s...   \n",
      "3  Par ailleurs, on prouve que cet algorithme pré...   \n",
      "4  Cet algorithme retourne tous les arbres de dér...   \n",
      "5  Ce résultat présente un intérêt pratique pour ...   \n",
      "6  Traduction automatique basée sur Corpus pour t...   \n",
      "7  Depuis de nombreuses années, la communauté de ...   \n",
      "8  Afin d'atteindre l'objectif de la traduction a...   \n",
      "9  Toutes ces méthodes de traduction diffèrent da...   \n",
      "\n",
      "                                         postedition  \n",
      "0  Transformation de structures de dépendances en...  \n",
      "1  Nous proposons un nouvel algorithme pour analy...  \n",
      "2  C'est-à-dire, étant donné une phrase et de sa ...  \n",
      "3  Par ailleurs, on prouve que cet algorithme a u...  \n",
      "4  Cet algorithme retourne tous les arbres de dér...  \n",
      "5  Ce résultat présente un intérêt pratique pour ...  \n",
      "6  Traduction automatique basée sur corpus pour l...  \n",
      "7  Depuis de nombreuses années, la communauté des...  \n",
      "8  Afin d'atteindre l'objectif de la traduction a...  \n",
      "9  Toutes ces méthodes de traduction diffèrent au...  \n"
     ]
    }
   ],
   "source": [
    "# Read TSV file into DataFrame\n",
    "df = pd.read_csv(post_edit_file, sep='\\t')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe9409f-bff5-4c79-a0cc-b85a577134ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_hal', 'Translation_id', 'Postedit_id', 'line_id', 'source',\n",
       "       'translation', 'postedition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7382b811-31c7-4604-a1c4-93eef3078120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362e2236-1ce0-40d3-b700-d5e430f06cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Postedit_id and line_id, collect translations into lists\n",
    "grouped = df.groupby(['Postedit_id', 'line_id'])['translation'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c1eed7-5a9e-49e1-a17d-b67d85dc0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postedit_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bien que les récents changements apportés à l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>[Cet article propose un nouveau type d'évaluat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>[Notre approche utilise des paires de phrases ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>[Cette méthodologie est utilisée pour comparer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Postedit_id  line_id                                        translation\n",
       "0           60        0  [Évaluation de la compétence morphologique des...\n",
       "1           60        1  [Bien que les récents changements apportés à l...\n",
       "2           60        2  [Cet article propose un nouveau type d'évaluat...\n",
       "3           60        3  [Notre approche utilise des paires de phrases ...\n",
       "4           60        4  [Cette méthodologie est utilisée pour comparer..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec77fd90-65c1-46e1-8c4c-39d8d7d547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by Postedit_id and aggregate translations/postedits into flat lists\n",
    "grouped_df = df.groupby(\"Postedit_id\").agg(\n",
    "    source=(\"source\", list), # list of source sentences against the same Postedit_id\n",
    "    translation=(\"translation\", list),  # Collect all translations\n",
    "    postedition=(\"postedition\", list)           # Collect all posteds (replace \"postedit\" with your actual column name)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313d428b-ad63-494d-88dd-08ca835e24c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postedit_id</th>\n",
       "      <th>source</th>\n",
       "      <th>translation</th>\n",
       "      <th>postedition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>[Evaluating the morphological competence of Ma...</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>[One Source, Two Targets: Challenges and Rewar...</td>\n",
       "      <td>[Une source, deux cibles: Défis et récompenses...</td>\n",
       "      <td>[Une source, deux cibles: Défis et bénéfices d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>[Neural Baselines for Word Alignments, Word al...</td>\n",
       "      <td>[Lignes de base neuronales pour les alignement...</td>\n",
       "      <td>[Modèles neuronaux de base pour l'alignement d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>[Multilingual Lexicalized Constituency Parsing...</td>\n",
       "      <td>[Lexicalized Constituency Parsing multilingue ...</td>\n",
       "      <td>[Tâches auxiliaires au niveau des mots pour l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>[Novel elicitation and annotation schemes for ...</td>\n",
       "      <td>[Nouveaux schémas d'élicitation et d'annotatio...</td>\n",
       "      <td>[Nouveaux schémas d'élicitation et d'annotatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Postedit_id                                             source  \\\n",
       "0           60  [Evaluating the morphological competence of Ma...   \n",
       "1           61  [One Source, Two Targets: Challenges and Rewar...   \n",
       "2           62  [Neural Baselines for Word Alignments, Word al...   \n",
       "3           80  [Multilingual Lexicalized Constituency Parsing...   \n",
       "4           82  [Novel elicitation and annotation schemes for ...   \n",
       "\n",
       "                                         translation  \\\n",
       "0  [Évaluation de la compétence morphologique des...   \n",
       "1  [Une source, deux cibles: Défis et récompenses...   \n",
       "2  [Lignes de base neuronales pour les alignement...   \n",
       "3  [Lexicalized Constituency Parsing multilingue ...   \n",
       "4  [Nouveaux schémas d'élicitation et d'annotatio...   \n",
       "\n",
       "                                         postedition  \n",
       "0  [Évaluation de la compétence morphologique des...  \n",
       "1  [Une source, deux cibles: Défis et bénéfices d...  \n",
       "2  [Modèles neuronaux de base pour l'alignement d...  \n",
       "3  [Tâches auxiliaires au niveau des mots pour l'...  \n",
       "4  [Nouveaux schémas d'élicitation et d'annotatio...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bbc2b5-f7dd-429a-b49b-b7eb8f18d400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36fcb59-37eb-460f-bb52-78bceb2c2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Postedit IDs in the original df: 95\n"
     ]
    }
   ],
   "source": [
    "num_unique_ids = df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the original df: {num_unique_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e98ca14-18df-4bfc-a430-5e9f5c52abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Postedit IDs in the grouped_df: 95\n"
     ]
    }
   ],
   "source": [
    "num_unique_ids_grouped_df = grouped_df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the grouped_df: {num_unique_ids_grouped_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4bac853-fb10-425e-8238-ca1e8a8949a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neural Baselines for Word Alignments',\n",
       " 'Word alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation.',\n",
       " 'In most areas of natural language processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word alignment models.',\n",
       " 'In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models.',\n",
       " 'We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts.',\n",
       " 'We also analyze typical alignment errors of the baselines that our models overcome to illustrate the benefits --- and the limitations --- of these new models for morphologically rich languages.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['source'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc38d56-e964-4826-ae16-7851a0f37b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lignes de base neuronales pour les alignements de mots',\n",
       " \"Les alignements de mots identifient les correspondances traductionnelles entre les mots d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour effectuer une estimation de la qualité.\",\n",
       " \"Dans la plupart des domaines du traitement du langage naturel, les modèles de réseaux neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.\",\n",
       " \"Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux pour l'alignement de mots non supervisé pour quatre paires de langues, en comparant plusieurs variantes de modèles neuronaux.\",\n",
       " 'Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalents discrets.',\n",
       " \"Nous analysons également les erreurs d'alignement typiques des lignes de base que nos modèles surmontent pour illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['translation'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634ca612-ee12-4cdc-b911-e481be087ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_df['translation'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e6e105-9ab4-4155-acf2-d4d0fd1f9e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Modèles neuronaux de base pour l'alignement de mots\",\n",
       " \"Les alignements de mots identifient les correspondances traductionnelles entre mots au sein d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour estimer la qualité d'une traduction.\",\n",
       " \"Dans la plupart des domaines du traitement des langues, les modèles neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.\",\n",
       " \"Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux d'alignement de mots non supervisés pour quatre paires de langues, en comparant plusieurs variantes de ces modèles.\",\n",
       " 'Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalentes discrètes.',\n",
       " \"Nous analysons également les erreurs d'alignement typiques des modèles de base que les versions neuronales surmontent afin d'illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['postedition'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86af496a-84ce-46ff-9db6-90a5b317593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "    \n",
    "# # Load checkpoint into your custom class\n",
    "# path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "# model = load_from_checkpoint(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0226a9f-0aa1-48de-b555-623dc1082972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "# model = load_from_checkpoint(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6abfbfd-7b45-4719-adca-6c259502e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239d024a-ecca-4293-b9bd-a407410ae368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "#model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9b0a31-cd4a-497d-9847-2919a92bcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom unified_metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "    print(\"custom unified_metric\")\n",
    "    \n",
    "# Load checkpoint into your custom class\n",
    "#path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(model_path,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a306c7e-1615-41ec-bd98-fbe5cfc66bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-2bdbe79f-ecba-d60e-2e29-3a1ae7c23a1a]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_ids:  [[None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 11, 11, 11, 12, 13, 14, 14, 15, 15, 15, 16, 16, 17, 18, 19, 19, 20, 20, 20, None]]\n",
      "track_spans:  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 14, -1, 16, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 31, 32, -1, -1]\n",
      "mt offsets:  [(0, 0), (0, 4), (4, 7), (7, 13), (13, 14), (14, 19), (19, 29), (29, 32), (32, 39), (39, 42), (42, 53), (53, 57), (57, 61), (61, 70), (70, 72), (72, 73), (73, 76), (76, 82), (82, 87), (87, 91), (91, 104), (104, 105), (105, 111), (111, 116), (116, 118), (118, 125), (125, 127), (127, 132), (132, 136), (136, 140), (140, 146), (146, 149), (149, 151), (151, 152), (0, 0)]\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 4]}, 1: {'subwords': [2], 'offsets': [4, 7]}, 2: {'subwords': [3, 4], 'offsets': [7, 14]}, 3: {'subwords': [5], 'offsets': [14, 19]}, 4: {'subwords': [6], 'offsets': [19, 29]}, 5: {'subwords': [7], 'offsets': [29, 32]}, 6: {'subwords': [8], 'offsets': [32, 39]}, 7: {'subwords': [9], 'offsets': [39, 42]}, 8: {'subwords': [10, 11], 'offsets': [42, 57]}, 9: {'subwords': [12], 'offsets': [57, 61]}, 10: {'subwords': [13], 'offsets': [61, 70]}, 11: {'subwords': [14, 15, 16, 17], 'offsets': [70, 82]}, 12: {'subwords': [18], 'offsets': [82, 87]}, 13: {'subwords': [19], 'offsets': [87, 91]}, 14: {'subwords': [20, 21], 'offsets': [91, 105]}, 15: {'subwords': [22, 23, 24], 'offsets': [105, 118]}, 16: {'subwords': [25, 26], 'offsets': [118, 127]}, 17: {'subwords': [27], 'offsets': [127, 132]}, 18: {'subwords': [28], 'offsets': [132, 136]}, 19: {'subwords': [29, 30], 'offsets': [136, 146]}, 20: {'subwords': [31, 32, 33], 'offsets': [146, 152]}}\n",
      "Tokenized_Words:  ['Pour', 'ce', 'faire,', 'nous', 'proposons', 'de', 'mettre', 'en', 'correspondance', 'les', 'éléments', \"d'ontologie\", 'avec', 'des', 'connaissances', 'multilingues', 'définies', 'dans', 'une', 'ontologie', 'SKOS.']\n",
      "index number is:  14\n",
      "word:  11\n",
      "text is:   d'ontologie\n",
      "index number is:  16\n",
      "index number is:  17\n",
      "index number is:  31\n",
      "word:  20\n",
      "index number is:  32\n",
      "text is:   SKOS.\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': \"d'ontologie\", 'start': 70, 'end': 82}), 1: defaultdict(None, {'text': 'SKOS.', 'start': 146, 'end': 152})})\n",
      "[{'tokens': [tensor(104, device='cuda:0')], 'severity': 'minor', 'offset': [70, 72], 'confidence': [tensor([0.4109], device='cuda:0')], 'check severity': ['minor']}, {'tokens': [tensor(7934, device='cuda:0'), tensor(18709, device='cuda:0')], 'severity': 'minor', 'offset': [73, 82], 'confidence': [tensor([0.3630], device='cuda:0'), tensor([0.3506], device='cuda:0')], 'check severity': ['minor', 'minor']}, {'tokens': [tensor(17882, device='cuda:0'), tensor(7285, device='cuda:0')], 'severity': 'minor', 'offset': [146, 151], 'confidence': [tensor([0.3807], device='cuda:0'), tensor([0.3994], device='cuda:0')], 'check severity': ['minor', 'minor']}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m data = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTo do so, we propose to map the ontol-ogy elements to multilingual knowledge defined in a SKOS ontology.\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     }\n\u001b[32m      7\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Segment-level scores\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m (model_output.scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/arafat_comet/COMET_GR/comet/models/base.py:655\u001b[39m, in \u001b[36mCometModel.predict\u001b[39m\u001b[34m(self, samples, batch_size, gpus, devices, mc_dropout, progress_bar, accelerator, num_workers, length_batching)\u001b[39m\n\u001b[32m    646\u001b[39m trainer = ptl.Trainer(\n\u001b[32m    647\u001b[39m     devices=devices,\n\u001b[32m    648\u001b[39m     logger=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    652\u001b[39m     enable_progress_bar=enable_progress_bar,\n\u001b[32m    653\u001b[39m )\n\u001b[32m    654\u001b[39m return_predictions = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_predictions\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m:\n\u001b[32m    659\u001b[39m     torch.distributed.barrier()  \u001b[38;5;66;03m# Waits for all processes to finish predict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    857\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    894\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    895\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    896\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    900\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:124\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:253\u001b[39m, in \u001b[36m_PredictionLoop._predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[32m    248\u001b[39m step_args = (\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    252\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m predictions = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:319\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    322\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:437\u001b[39m, in \u001b[36mStrategy.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/arafat_comet/COMET_GR/comet/models/multitask/unified_metric.py:964\u001b[39m, in \u001b[36mUnifiedMetric.predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx)\u001b[39m\n\u001b[32m    962\u001b[39m MT_dict = batch[-\u001b[32m1\u001b[39m].copy() \u001b[38;5;66;03m# contains the dictionary with word_ids, \u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[38;5;66;03m#mt_sentences,mt_sentences_tokenized\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m error_spans, word_level_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubword_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmt_offsets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMT_dict\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33merror_spans\u001b[39m\u001b[33m\"\u001b[39m] = error_spans\n\u001b[32m    968\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33mword_level_probability\u001b[39m\u001b[33m\"\u001b[39m]=word_level_prob\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/arafat_comet/COMET_GR/comet/models/multitask/unified_metric.py:909\u001b[39m, in \u001b[36mUnifiedMetric.decode\u001b[39m\u001b[34m(self, subword_probs, input_ids, mt_offsets, MT_dict)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33merror_spans: \u001b[39m\u001b[33m\"\u001b[39m, error_spans)\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m error_spans:                \n\u001b[32m    907\u001b[39m     sentence_output.append(\n\u001b[32m    908\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mword_level_error_span\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    910\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m\"\u001b[39m: torch.concat(span[\u001b[33m\"\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m\"\u001b[39m]).mean().item(),\n\u001b[32m    911\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m\"\u001b[39m: span[\u001b[33m\"\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    912\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m: word_level_error_span[count][\u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    913\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m: word_level_error_span[count][\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    914\u001b[39m         }\n\u001b[32m    915\u001b[39m     )\n\u001b[32m    916\u001b[39m     count += \u001b[32m1\u001b[39m\n\u001b[32m    917\u001b[39m decoded_output.append(sentence_output)\n",
      "\u001b[31mKeyError\u001b[39m: 2"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": \"To do so, we propose to map the ontol-ogy elements to multilingual knowledge defined in a SKOS ontology.\", \n",
    "        \"mt\": \"Pour ce faire, nous proposons de mettre en correspondance les éléments d'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\", \n",
    "        \"ref\": \"Pour ce faire, nous proposons de mettre en correspondance les éléments de l'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\"\n",
    "    }\n",
    "]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94880a06-3832-433c-8055-ac6f07cb4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "# class CustomXCOMET(UnifiedMetric):\n",
    "#     print(\"custom unified_metric\")\n",
    "    \n",
    "# # Load checkpoint into your custom class\n",
    "# path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "# model = CustomXCOMET.load_from_checkpoint(path,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159f82-7f70-460f-8dff-03cd5782fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate and extract values from two columns\n",
    "# data = []\n",
    "# for index, row in df.iterrows():\n",
    "#     Postedit_id = row['Postedit_id']\n",
    "#     source      = row['source']\n",
    "#     translation = row['translation']\n",
    "#     postedition  = row['postedition']\n",
    "#     #print(\"index number of the dataset: \", index)\n",
    "#     #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "    \n",
    "#     src = sentence\n",
    "#     mt  = translation[i]\n",
    "#     ref = postedition[i]\n",
    "#     data.append({\n",
    "#         \"src\": src,\n",
    "#         \"mt\" : mt,\n",
    "#         \"ref\": ref,\n",
    "#     })\n",
    "\n",
    "# model_output = model.predict(data, batch_size=8, gpus=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79245868-01e0-491c-ba7e-bcc8529dfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (model_output.scores)\n",
    "\n",
    "# # System-level score\n",
    "# print (model_output.system_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1509b2-d37b-4b8c-b267-6ba2760134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a69c1-dfa6-4f55-bfda-b81554f785e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "paragraph = defaultdict()\n",
    "for index, row in df.iterrows():\n",
    "    if row['Postedit_id'] not in paragraph.keys():\n",
    "        source      = row['source']\n",
    "        translation = row['translation']\n",
    "        postedition  = row['postedition']\n",
    "        paragraph[row['Postedit_id']] = {'src': source,\n",
    "                                         'mt': translation,\n",
    "                                         'ref': postedition\n",
    "                                        }\n",
    "    else:\n",
    "        source      += row['source']\n",
    "        translation += row['translation']\n",
    "        postedition  += row['postedition']\n",
    "        paragraph[row['Postedit_id']] = {'src': source,\n",
    "                                         'mt': translation,\n",
    "                                         'ref': postedition\n",
    "                                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0b3b7-08ef-412d-99a0-17557274cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679fcb5-1019-40e5-bb00-825d49ca5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and extract values from two columns\n",
    "data = []\n",
    "count = 0\n",
    "for index, value in paragraph.items():\n",
    "    data.append(value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c737683-30d2-4a3f-8a0e-f010d63b3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89606a3-781a-447b-9f95-2c37a225674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in data:\n",
    "    print(\"=====================count=========================:\", count)\n",
    "    count += 1\n",
    "    model_output = model.predict([item], batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592298b-e1c3-4fd9-adb6-e3e5f62748ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and extract values from two columns\n",
    "data = []\n",
    "count = 0\n",
    "for index, row in grouped_df.iterrows():\n",
    "    Postedit_id = row['Postedit_id']\n",
    "    source      = row['source']\n",
    "    translation = row['translation']\n",
    "    postedition  = row['postedition']\n",
    "    #print(\"index number of the dataset: \", index)\n",
    "    #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "    src = \"\"\n",
    "    mt  = \"\"\n",
    "    ref = \"\"\n",
    "    for i, sentence in enumerate(source):\n",
    "        src += sentence\n",
    "        mt  += translation[i]\n",
    "        ref += postedition[i]\n",
    "    data.append({\n",
    "        \"src\": src,\n",
    "        \"mt\" : mt,\n",
    "        \"ref\": ref,\n",
    "    })\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6dd0e-2733-44d2-ac2c-f8c582447667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [data[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3b007-3248-4c77-961e-dffeddf08933",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d400f61-233b-4a0f-bfd4-79179a1ec5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.predict(data, batch_size=8, gpus=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
