{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31984b68-9df0-48bd-b0e9-54024ad469e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f207b3-818e-4a2b-9daa-47dc0f241754",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_edit_file = \"/storage/brno2/home/rahmang/new-interaction-for-MT/postedition_aligned.community.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45a169-a0e0-4220-a93d-2f29a55f8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TSV file into DataFrame\n",
    "df = pd.read_csv(post_edit_file, sep='\\t')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9409f-bff5-4c79-a0cc-b85a577134ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382b811-31c7-4604-a1c4-93eef3078120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e2236-1ce0-40d3-b700-d5e430f06cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Postedit_id and line_id, collect translations into lists\n",
    "grouped = df.groupby(['Postedit_id', 'line_id'])['translation'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1eed7-5a9e-49e1-a17d-b67d85dc0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77fd90-65c1-46e1-8c4c-39d8d7d547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by Postedit_id and aggregate translations/postedits into flat lists\n",
    "grouped_df = df.groupby(\"Postedit_id\").agg(\n",
    "    source=(\"source\", list), # list of source sentences against the same Postedit_id\n",
    "    translation=(\"translation\", list),  # Collect all translations\n",
    "    postedition=(\"postedition\", list)           # Collect all posteds (replace \"postedit\" with your actual column name)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d428b-ad63-494d-88dd-08ca835e24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbc2b5-f7dd-429a-b49b-b7eb8f18d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fcb59-37eb-460f-bb52-78bceb2c2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_ids = df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the original df: {num_unique_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98ca14-18df-4bfc-a430-5e9f5c52abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_ids_grouped_df = grouped_df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the grouped_df: {num_unique_ids_grouped_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bac853-fb10-425e-8238-ca1e8a8949a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['source'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc38d56-e964-4826-ae16-7851a0f37b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['translation'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ca612-ee12-4cdc-b911-e481be087ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_df['translation'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6e105-9ab4-4155-acf2-d4d0fd1f9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['postedition'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af496a-84ce-46ff-9db6-90a5b317593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "    \n",
    "# # Load checkpoint into your custom class\n",
    "# path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "# model = load_from_checkpoint(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0226a9f-0aa1-48de-b555-623dc1082972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "# model = load_from_checkpoint(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abfbfd-7b45-4719-adca-6c259502e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94880a06-3832-433c-8055-ac6f07cb4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "    print(\"custom unified_metric\")\n",
    "    \n",
    "# Load checkpoint into your custom class\n",
    "path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(path,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68159f82-7f70-460f-8dff-03cd5782fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-38101a57-acdb-0625-2da5-ae6faf020a3a]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0:   1%|          | 1/83 [00:00<00:18,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   2%|▏         | 2/83 [00:00<00:17,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   4%|▎         | 3/83 [00:00<00:17,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'O', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   5%|▍         | 4/83 [00:00<00:17,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'G', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'G', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   6%|▌         | 5/83 [00:01<00:17,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'À', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   7%|▋         | 6/83 [00:01<00:17,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   8%|▊         | 7/83 [00:01<00:16,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'V', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  10%|▉         | 8/83 [00:01<00:16,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'V', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  11%|█         | 9/83 [00:02<00:16,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'r', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  12%|█▏        | 10/83 [00:02<00:16,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  13%|█▎        | 11/83 [00:02<00:16,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  14%|█▍        | 12/83 [00:02<00:15,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  16%|█▌        | 13/83 [00:02<00:15,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'Q', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'À', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  17%|█▋        | 14/83 [00:03<00:15,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  18%|█▊        | 15/83 [00:03<00:15,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'G', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  19%|█▉        | 16/83 [00:03<00:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  20%|██        | 17/83 [00:03<00:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  22%|██▏       | 18/83 [00:04<00:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  23%|██▎       | 19/83 [00:04<00:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  24%|██▍       | 20/83 [00:04<00:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  25%|██▌       | 21/83 [00:04<00:13,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  27%|██▋       | 22/83 [00:04<00:13,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  28%|██▊       | 23/83 [00:05<00:13,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  29%|██▉       | 24/83 [00:05<00:13,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  30%|███       | 25/83 [00:05<00:12,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'K', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  31%|███▏      | 26/83 [00:05<00:12,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  33%|███▎      | 27/83 [00:06<00:12,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  34%|███▎      | 28/83 [00:06<00:12,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'G', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  35%|███▍      | 29/83 [00:06<00:12,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  36%|███▌      | 30/83 [00:06<00:11,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  37%|███▋      | 31/83 [00:06<00:11,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'Q', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  39%|███▊      | 32/83 [00:07<00:11,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  40%|███▉      | 33/83 [00:07<00:11,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  41%|████      | 34/83 [00:07<00:10,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  42%|████▏     | 35/83 [00:07<00:10,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  43%|████▎     | 36/83 [00:08<00:10,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  45%|████▍     | 37/83 [00:08<00:10,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'Y', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  46%|████▌     | 38/83 [00:08<00:10,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  47%|████▋     | 39/83 [00:08<00:09,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  48%|████▊     | 40/83 [00:08<00:09,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  49%|████▉     | 41/83 [00:09<00:09,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  51%|█████     | 42/83 [00:09<00:09,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'V', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  52%|█████▏    | 43/83 [00:09<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'F', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  53%|█████▎    | 44/83 [00:09<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': '1', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  54%|█████▍    | 45/83 [00:10<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'À', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  55%|█████▌    | 46/83 [00:10<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  57%|█████▋    | 47/83 [00:10<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'O', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  58%|█████▊    | 48/83 [00:10<00:07,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'À', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': '3', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  59%|█████▉    | 49/83 [00:10<00:07,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  60%|██████    | 50/83 [00:11<00:07,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  61%|██████▏   | 51/83 [00:11<00:07,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  63%|██████▎   | 52/83 [00:11<00:06,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  64%|██████▍   | 53/83 [00:11<00:06,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  65%|██████▌   | 54/83 [00:12<00:06,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  66%|██████▋   | 55/83 [00:12<00:06,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  67%|██████▋   | 56/83 [00:12<00:06,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  69%|██████▊   | 57/83 [00:12<00:05,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  70%|██████▉   | 58/83 [00:12<00:05,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  71%|███████   | 59/83 [00:13<00:05,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  72%|███████▏  | 60/83 [00:13<00:05,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'K', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  73%|███████▎  | 61/83 [00:13<00:04,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  75%|███████▍  | 62/83 [00:13<00:04,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'O', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  76%|███████▌  | 63/83 [00:14<00:04,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  77%|███████▋  | 64/83 [00:14<00:04,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  78%|███████▊  | 65/83 [00:14<00:04,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'I', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  80%|███████▉  | 66/83 [00:14<00:03,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'R', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'i', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  81%|████████  | 67/83 [00:15<00:03,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  82%|████████▏ | 68/83 [00:15<00:03,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  83%|████████▎ | 69/83 [00:15<00:03,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  84%|████████▍ | 70/83 [00:15<00:02,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  86%|████████▌ | 71/83 [00:15<00:02,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'U', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  87%|████████▋ | 72/83 [00:16<00:02,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  88%|████████▊ | 73/83 [00:16<00:02,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  89%|████████▉ | 74/83 [00:16<00:02,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  90%|█████████ | 75/83 [00:16<00:01,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  92%|█████████▏| 76/83 [00:17<00:01,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'B', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  93%|█████████▎| 77/83 [00:17<00:01,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  94%|█████████▍| 78/83 [00:17<00:01,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'T', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  95%|█████████▌| 79/83 [00:17<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  96%|█████████▋| 80/83 [00:17<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'C', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'S', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  98%|█████████▊| 81/83 [00:18<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'A', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'H', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 83/83 [00:18<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'É', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'M', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'E', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'D', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'N', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'L', 'start': 0, 'end': 1})})\n",
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 1]}}\n",
      "word:  0\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'P', 'start': 0, 'end': 1})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate and extract values from two columns\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    Postedit_id = row['Postedit_id']\n",
    "    source      = row['source']\n",
    "    translation = row['translation']\n",
    "    postedition  = row['postedition']\n",
    "    #print(\"index number of the dataset: \", index)\n",
    "    #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "    \n",
    "    src = sentence\n",
    "    mt  = translation[i]\n",
    "    ref = postedition[i]\n",
    "    data.append({\n",
    "        \"src\": src,\n",
    "        \"mt\" : mt,\n",
    "        \"ref\": ref,\n",
    "    })\n",
    "\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79245868-01e0-491c-ba7e-bcc8529dfff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4381980895996094, 0.4537394940853119, 0.45163148641586304, 0.4519088864326477, 0.45163148641586304, 0.45163148641586304, 0.4381980895996094, 0.4632798433303833, 0.45840132236480713, 0.4381980895996094, 0.4602714478969574, 0.32893961668014526, 0.44389674067497253, 0.44389674067497253, 0.4632798433303833, 0.45163148641586304, 0.4537394940853119, 0.44389674067497253, 0.44389674067497253, 0.4537610709667206, 0.44389674067497253, 0.44703835248947144, 0.4537394940853119, 0.44389674067497253, 0.35607606172561646, 0.4381980895996094, 0.44703835248947144, 0.46520599722862244, 0.4537394940853119, 0.44389674067497253, 0.33785685896873474, 0.4632798433303833, 0.4537394940853119, 0.33785685896873474, 0.44389674067497253, 0.45163148641586304, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.4689275920391083, 0.44389674067497253, 0.4519088864326477, 0.4632798433303833, 0.4537394940853119, 0.4381980895996094, 0.4537394940853119, 0.3410367965698242, 0.4519088864326477, 0.45163148641586304, 0.3427835702896118, 0.4537394940853119, 0.4381980895996094, 0.44389674067497253, 0.4632798433303833, 0.45163148641586304, 0.45163148641586304, 0.44389674067497253, 0.44389674067497253, 0.4537610709667206, 0.46520599722862244, 0.45163148641586304, 0.4509292244911194, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4509292244911194, 0.45163148641586304, 0.44389674067497253, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.4632798433303833, 0.44389674067497253, 0.44389674067497253, 0.4381980895996094, 0.45163148641586304, 0.44389674067497253, 0.45840132236480713, 0.4602714478969574, 0.46520599722862244, 0.4632798433303833, 0.4381980895996094, 0.44389674067497253, 0.44389674067497253, 0.4632798433303833, 0.45163148641586304, 0.5354592204093933, 0.4537394940853119, 0.3513483703136444, 0.45163148641586304, 0.44703835248947144, 0.35373303294181824, 0.45163148641586304, 0.4689275920391083, 0.44389674067497253, 0.45840132236480713, 0.34456098079681396, 0.45163148641586304, 0.34677255153656006, 0.45840132236480713, 0.44389674067497253, 0.4537610709667206, 0.4519088864326477, 0.4632798433303833, 0.4632798433303833, 0.3622349202632904, 0.44389674067497253, 0.33496272563934326, 0.45163148641586304, 0.4537394940853119, 0.4786553382873535, 0.4537394940853119, 0.44703835248947144, 0.44389674067497253, 0.45296284556388855, 0.4519088864326477, 0.45840132236480713, 0.4632798433303833, 0.45819544792175293, 0.45163148641586304, 0.4537394940853119, 0.4632798433303833, 0.44389674067497253, 0.45163148641586304, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.46520599722862244, 0.4632798433303833, 0.45163148641586304, 0.46520599722862244, 0.4632798433303833, 0.4632798433303833, 0.44389674067497253, 0.4537394940853119, 0.4689275920391083, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.44389674067497253, 0.4689275920391083, 0.44389674067497253, 0.33604806661605835, 0.44389674067497253, 0.4433417320251465, 0.45329511165618896, 0.44703835248947144, 0.5354592204093933, 0.4537394940853119, 0.46520599722862244, 0.4689275920391083, 0.4537394940853119, 0.44389674067497253, 0.4632798433303833, 0.42529237270355225, 0.46520599722862244, 0.4537394940853119, 0.45163148641586304, 0.44389674067497253, 0.4509292244911194, 0.3509306311607361, 0.45163148641586304, 0.44389674067497253, 0.45163148641586304, 0.45840132236480713, 0.4537394940853119, 0.4537394940853119, 0.4632798433303833, 0.3395414352416992, 0.45163148641586304, 0.44389674067497253, 0.4602714478969574, 0.46520599722862244, 0.3480850160121918, 0.4519088864326477, 0.4537394940853119, 0.45163148641586304, 0.4537394940853119, 0.46520599722862244, 0.46520599722862244, 0.45840132236480713, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4632798433303833, 0.44389674067497253, 0.4632798433303833, 0.3841099143028259, 0.46520599722862244, 0.44389674067497253, 0.3230275511741638, 0.45163148641586304, 0.46520599722862244, 0.4537394940853119, 0.4381980895996094, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.45840132236480713, 0.44389674067497253, 0.4381980895996094, 0.4632798433303833, 0.4537394940853119, 0.44389674067497253, 0.4537394940853119, 0.44389674067497253, 0.45840132236480713, 0.44389674067497253, 0.3218003213405609, 0.4632798433303833, 0.4537394940853119, 0.44389674067497253, 0.4537394940853119, 0.44389674067497253, 0.45329511165618896, 0.3216983675956726, 0.46520599722862244, 0.4632798433303833, 0.4537391662597656, 0.45819544792175293, 0.44389718770980835, 0.44389674067497253, 0.45163148641586304, 0.45163148641586304, 0.4381980895996094, 0.45329511165618896, 0.44389674067497253, 0.3551764488220215, 0.4632798433303833, 0.4537394940853119, 0.335832417011261, 0.45819544792175293, 0.4632798433303833, 0.44389674067497253, 0.4632798433303833, 0.44389674067497253, 0.4537394940853119, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.45840132236480713, 0.4689275920391083, 0.4632798433303833, 0.4323264956474304, 0.44389674067497253, 0.44703835248947144, 0.45163148641586304, 0.4537394940853119, 0.4519088864326477, 0.4537394940853119, 0.45840132236480713, 0.44389674067497253, 0.4632798433303833, 0.4632798433303833, 0.45819544792175293, 0.4689275920391083, 0.44389674067497253, 0.4519088864326477, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.44389674067497253, 0.46520599722862244, 0.3406291604042053, 0.4632798433303833, 0.4537610709667206, 0.45819544792175293, 0.45163148641586304, 0.4537394940853119, 0.4632798433303833, 0.44389674067497253, 0.45329511165618896, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4519088864326477, 0.44389674067497253, 0.4537394940853119, 0.3599588871002197, 0.45163148641586304, 0.4632798433303833, 0.4632798433303833, 0.4509292244911194, 0.45840132236480713, 0.33513712882995605, 0.44389674067497253, 0.4632798433303833, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.45163148641586304, 0.4381980895996094, 0.4537394940853119, 0.45163148641586304, 0.4537394940853119, 0.4537394940853119, 0.4632798433303833, 0.44389674067497253, 0.44389674067497253, 0.4537394940853119, 0.46520599722862244, 0.4537394940853119, 0.44389674067497253, 0.45840132236480713, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.4537394940853119, 0.4537394940853119, 0.44389674067497253, 0.45329511165618896, 0.4602714478969574, 0.45163148641586304, 0.4537394940853119, 0.45163148641586304, 0.45840132236480713, 0.44389674067497253, 0.45163148641586304, 0.4632798433303833, 0.45819544792175293, 0.44703835248947144, 0.44389674067497253, 0.44389674067497253, 0.4537610709667206, 0.4537394940853119, 0.4537394940853119, 0.4632798433303833, 0.4537394940853119, 0.44258904457092285, 0.44389674067497253, 0.45163148641586304, 0.4251839816570282, 0.4537394940853119, 0.46423596143722534, 0.46520599722862244, 0.4632798433303833, 0.44389674067497253, 0.4537610709667206, 0.4689275920391083, 0.45163148641586304, 0.4537610709667206, 0.4509292244911194, 0.45840132236480713, 0.4519088864326477, 0.4537610709667206, 0.45163148641586304, 0.4537610709667206, 0.4381980895996094, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.3277798593044281, 0.44703835248947144, 0.3427835702896118, 0.45163148641586304, 0.4537394940853119, 0.45163148641586304, 0.4381980895996094, 0.3277798593044281, 0.4537394940853119, 0.4537394940853119, 0.45819544792175293, 0.35267502069473267, 0.44389674067497253, 0.45163148641586304, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4519088864326477, 0.4537394940853119, 0.4537394940853119, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4519088864326477, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.44389674067497253, 0.4602714478969574, 0.4509292244911194, 0.4632798433303833, 0.4537394940853119, 0.45163148641586304, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.45163148641586304, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.46520599722862244, 0.4632798433303833, 0.4537394940853119, 0.45840132236480713, 0.44389674067497253, 0.46520599722862244, 0.44389674067497253, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.44258904457092285, 0.4537394940853119, 0.44258904457092285, 0.5354592204093933, 0.34300875663757324, 0.4632798433303833, 0.3216983675956726, 0.45163148641586304, 0.4519088864326477, 0.45163148641586304, 0.4433417320251465, 0.44703835248947144, 0.44389674067497253, 0.33604806661605835, 0.4689275920391083, 0.44389674067497253, 0.4602714478969574, 0.4632798433303833, 0.45163148641586304, 0.45163148641586304, 0.34284695982933044, 0.4602714478969574, 0.44389674067497253, 0.4537394940853119, 0.4519088864326477, 0.45163148641586304, 0.46520599722862244, 0.44389674067497253, 0.44703835248947144, 0.3427835702896118, 0.45163148641586304, 0.4537394940853119, 0.45163148641586304, 0.4381980895996094, 0.484835147857666, 0.39660394191741943, 0.44703835248947144, 0.45163148641586304, 0.44389674067497253, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.45163148641586304, 0.45819544792175293, 0.45163148641586304, 0.4537394940853119, 0.4632798433303833, 0.44389674067497253, 0.4509292244911194, 0.44389674067497253, 0.4632798433303833, 0.45163148641586304, 0.4537394940853119, 0.3635893762111664, 0.45840132236480713, 0.4537394940853119, 0.46520599722862244, 0.44389674067497253, 0.4537394940853119, 0.4537394940853119, 0.45840132236480713, 0.4689275920391083, 0.4632798433303833, 0.44389674067497253, 0.44703835248947144, 0.45163148641586304, 0.4537394940853119, 0.4519088864326477, 0.4537394940853119, 0.45840132236480713, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4632798433303833, 0.4509292244911194, 0.3256564140319824, 0.44389674067497253, 0.45819544792175293, 0.4381980895996094, 0.4509292244911194, 0.46520599722862244, 0.4519088864326477, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.46520599722862244, 0.45329511165618896, 0.44389674067497253, 0.45163148641586304, 0.45163148641586304, 0.4381980895996094, 0.45163148641586304, 0.4632798433303833, 0.4509292244911194, 0.4537394940853119, 0.45329511165618896, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.46520599722862244, 0.4689275920391083, 0.44389674067497253, 0.4519088864326477, 0.4632798433303833, 0.4537394940853119, 0.45329511165618896, 0.4632798433303833, 0.4519088864326477, 0.3397388160228729, 0.4519088864326477, 0.48123010993003845, 0.4537394940853119, 0.45840132236480713, 0.44389674067497253, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.46520599722862244, 0.4537394940853119, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.44389674067497253, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.46520599722862244, 0.45163148641586304, 0.4632798433303833, 0.44389674067497253, 0.46520599722862244, 0.44389674067497253, 0.4519088864326477, 0.4519088864326477, 0.45840132236480713, 0.3399368226528168, 0.4537394940853119, 0.4537394940853119, 0.45163148641586304, 0.45163148641586304, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.45163148641586304, 0.4632798433303833, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.46520599722862244, 0.4632798433303833, 0.44389674067497253, 0.45163148641586304, 0.34413009881973267, 0.45163148641586304, 0.44389674067497253, 0.4665045142173767, 0.44389674067497253, 0.44389674067497253, 0.4632798433303833, 0.46520599722862244, 0.4537394940853119, 0.44389674067497253, 0.4509292244911194, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.4509292244911194, 0.45163148641586304, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.44389674067497253, 0.44389674067497253, 0.4537610709667206, 0.4632798433303833, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.4537394940853119, 0.46520599722862244, 0.4519088864326477, 0.4519088864326477, 0.4537610709667206, 0.45163148641586304, 0.45163148641586304, 0.46423596143722534, 0.46520599722862244, 0.4632798433303833, 0.44389674067497253, 0.45163148641586304, 0.3862210214138031, 0.4537394940853119, 0.4537394940853119, 0.44389674067497253, 0.4537394940853119, 0.4509292244911194, 0.4632798433303833, 0.44389674067497253, 0.45819544792175293, 0.4381980895996094, 0.4509292244911194, 0.46520599722862244, 0.4519088864326477, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.46520599722862244, 0.45329511165618896, 0.4602714478969574, 0.45163148641586304, 0.4537394940853119, 0.45163148641586304, 0.45329511165618896, 0.4519088864326477, 0.33706900477409363, 0.4632798433303833, 0.4537394940853119, 0.4537394940853119, 0.34413009881973267, 0.4509292244911194, 0.44258904457092285, 0.44389674067497253, 0.44389674067497253, 0.44389674067497253, 0.45190906524658203]\n",
      "0.44624649370385955\n"
     ]
    }
   ],
   "source": [
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f1509b2-d37b-4b8c-b267-6ba2760134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed7a69c1-dfa6-4f55-bfda-b81554f785e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "paragraph = defaultdict()\n",
    "for index, row in df.iterrows():\n",
    "    if row['Postedit_id'] not in paragraph.keys():\n",
    "        source      = row['source']\n",
    "        translation = row['translation']\n",
    "        postedition  = row['postedition']\n",
    "        paragraph[row['Postedit_id']] = {'src': source,\n",
    "                                         'mt': translation,\n",
    "                                         'ref': postedition\n",
    "                                        }\n",
    "    else:\n",
    "        source      += row['source']\n",
    "        translation += row['translation']\n",
    "        postedition  += row['postedition']\n",
    "        paragraph[row['Postedit_id']] = {'src': source,\n",
    "                                         'mt': translation,\n",
    "                                         'ref': postedition\n",
    "                                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79e0b3b7-08ef-412d-99a0-17557274cf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {402: {'src': 'Transforming Dependency Structures to LTAG Derivation TreesWe propose a new algorithm for parsing Lexicalized Tree Adjoining Grammars (LTAGs) which uses pre-assigned bilexi-cal dependency relations as a filter.That is, given a sentence and its corresponding well-formed dependency structure, the parser assigns elementary trees to words of the sentence and return attachment sites compatible with these elementary trees and predefined dependencies.Moreover, we prove that this algorithm has a linear-time complexity in the input length.This algorithm returns all compatible derivation trees as a packed forest.This result is of practical interest to the development of efficient weighted LTAG parsers based on derivation tree decoding.',\n",
       "              'mt': \"Transformation de structures de dépendances en arbres de dérivation LTAGNous proposons un nouvel algorithme pour analyser les grammaires adjacentes à l'arbre lexicalisé (LTAG) qui utilise des relations de dépendance bilexi-cal pré-assignées comme filtre.C'est-à-dire, compte tenu d'une phrase et de sa structure de dépendance bien formée correspondante, le parseur attribue des arbres élémentaires à des mots de la phrase et retourne des sites d'attachement compatibles avec ces arbres élémentaires et des dépendances prédéfinies.Par ailleurs, on prouve que cet algorithme présente une complexité linéaire temporelle dans la longueur d'entrée.Cet algorithme retourne tous les arbres de dérivation compatibles en tant que forêt compactée.Ce résultat présente un intérêt pratique pour le développement d'analyseurs LTAG pondérés efficaces basés sur un décodage d'arbre de dérivation.\",\n",
       "              'ref': \"Transformation de structures de dépendances en arbres de dérivation LTAGNous proposons un nouvel algorithme pour analyser les grammaires d'arbres adjoints lexicalisés (LTAG) qui utilise des relations de dépendance bilexicale pré-assignées comme filtre.C'est-à-dire, étant donné une phrase et de sa structure en dépendances bien formé, l'analyseur attribue des arbres élémentaires à des mots de la phrase et retourne les sites d'attachement compatibles avec ces arbres élémentaires et ces dépendances prédéfinies.Par ailleurs, on prouve que cet algorithme a une complexité temporelle linéaire dans la longueur d'entrée.Cet algorithme retourne tous les arbres de dérivation compatibles sous forme d'une forêt compact.Ce résultat présente un intérêt pratique pour le développement d'analyseurs LTAG pondérés efficaces fondés sur un décodage de l'arbre de dérivation.\"},\n",
       "             159: {'src': 'Corpus Based Machine Translation for Scientific TextFrom many years, machine translation and computational linguistic research community has given immense attention towards the development of machine translation techniques.In order to fulfill the goal of machine translation “translation without losing meaning”, a lot of translation methods have been proposed.All of these translation methods differ in their theories and implementation strategies.Although some basic rules of translation are same but many of them vary with the selection of language pair.While concerning with the scientific text, every science domain has thousands of terminologies.Translation of these terminologies according to the domain boosts the performance of translation.Translation of scientific text is ignored in the literature, as it needs more effort and expertise of both domain and language are required.In this research, we have proposed an effective scientific text translator for English to Urdu to cope with the challenge of scientific text translation.This method tags and translate the terms according to the domain.We have introduced a term tagger for tagging terms.The system can work for any domain but for experimental purpose we have selected the domain of computer science.System is evaluated on self-generated corpus of computer science.It is also compared with the existing translators to demonstrate the dominance of proposed translator as compared to the competitor.The comparative results of proposed approach and existing are shown in the form of tables.',\n",
       "              'mt': \"Traduction automatique basée sur Corpus pour texte scientifiqueDepuis de nombreuses années, la communauté de la traduction automatique et de la recherche linguistique computationnelle accorde une grande attention au développement des techniques de traduction automatique.Afin d'atteindre l'objectif de la traduction automatique « traduction sans perdre de sens », de nombreuses méthodes de traduction ont été proposées.Toutes ces méthodes de traduction diffèrent dans leurs théories et stratégies de mise en oeuvre.Bien que certaines règles de base de la traduction soient identiques, beaucoup d'entre elles varient avec la sélection de la paire de langues.Bien que concernant le texte scientifique, chaque domaine scientifique a des milliers de terminologies.La traduction de ces terminologies en fonction du domaine améliore les performances de traduction.La traduction de textes scientifiques est ignorée dans la littérature, car elle nécessite plus d'efforts et d'expertise, tant dans le domaine que dans la langue.Dans cette recherche, nous avons proposé un traducteur de texte scientifique efficace pour l'anglais à l'ourdou pour faire face au défi de la traduction de texte scientifique.Cette méthode marque et traduit les termes en fonction du domaine.Nous avons introduit un marqueur de termes pour marquer des termes.Le système peut fonctionner pour n'importe quel domaine, mais à des fins expérimentales, nous avons choisi le domaine de l'informatique.Le système est évalué sur un corpus autogénéré d'informatique.Il est également comparé avec les traducteurs existants pour démontrer la domination du traducteur proposé par rapport au concurrent.Les résultats comparatifs de l'approche proposée et de l'approche existante sont présentés sous forme de tableaux.\",\n",
       "              'ref': \"Traduction automatique basée sur corpus pour le texte scientifiqueDepuis de nombreuses années, la communauté des chercheurs en traduction automatique et en traitement automatique des langues prête une attention aiguë au développement des techniques de traduction automatique.Afin d'atteindre l'objectif de la traduction automatique d'une « traduction sans perte de sens », de nombreuses méthodes de traduction ont été proposées.Toutes ces méthodes de traduction diffèrent au plan théorique et dans la stratégie de leur mise en œuvre.Bien que certaines règles de base de la traduction soient identiques, nombre d'entre elles varient selon la paire de langues considérée.S'agissant de textes scientifiques, chaque domaine des sciences possède une terminologie spécifique.La traduction de ces terminologies en fonction du domaine améliore les performances de traduction.La traduction de textes scientifiques est négligée dans la littérature, car elle nécessite plus d'efforts et d'expertise, tant dans le domaine que dans la langue.Dans cette recherche, nous avons proposé un traducteur de textes scientifiques efficace anglais--&gt;ourdou pour faire face au défi de la traduction scientifique.Cette méthode annote et traduit les termes en fonction du domaine.Nous avons introduit un marqueur de termes pour marquer les termes.Le système peut être appliqué à tous les domaines, mais à des fins expérimentales, nous avons choisi le domaine de l'informatique.Le système est évalué sur un corpus auto-généré d'informatique.Il est également comparé avec les traducteurs existants pour démontrer la supériorité du traducteur proposé par rapport à la concurrence.Les résultats comparés de l'approche proposée et de celles existantes sont présentés sous forme de tableaux.\"},\n",
       "             155: {'src': 'T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine TranslationWe present a new approach to perform zeroshot cross-modal transfer between speech and text for translation tasks.Multilingual speech and text are encoded in a joint fixed-size representation space.Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities.All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks.In particular, we outperform the state of the art for zero-shot speech translation on Must-C.We also introduce the first results for zero-shot direct speechto-speech and text-to-speech translation.',\n",
       "              'mt': \"Modules T : Modules de traduction pour la traduction automatique intermodale à tir nulNous présentons une nouvelle approche pour effectuer un transfert intermodal zéro-shot entre la parole et le texte pour les tâches de traduction.La parole et le texte multilingues sont codés dans un espace de représentation commun de taille fixe.Ensuite, nous comparons différentes approches pour décoder ces représentations multimodales et multilingues de taille fixe, permettant une traduction de zéro plan entre les langues et les modalités.Tous nos modèles sont formés sans avoir besoin de données de traduction étiquetées intermodales.Malgré une représentation de taille fixe, nous obtenons des résultats très compétitifs sur plusieurs tâches de traduction de texte et de parole.En particulier, nous surpassons l'état de la technique pour la traduction vocale à tir nul sur Must-C.Nous présentons également les premiers résultats pour la traduction vocale directe à tir nul et la traduction de texte à parole.\",\n",
       "              'ref': \"Modules T : Modules de traduction pour la traduction automatique intermodale zéro-shotNous présentons une nouvelle approche permettant d'effectuer un transfert intermodal zéro-shot entre parole et texte écrit pour des tâches de traduction.La parole et le texte multilingues sont encodés dans un espace de représentation commun de taille fixe.Nous comparons ensuite différentes approches visant à décoder ces représentations multimodales et multilingues de taille fixe, permettant une traduction zéro-shot entre langues et modalités.Tous nos modèles sont entraînés sans que des données de traduction intermodales annotées soient nécessaires.Malgré la taille fixe de cette représentation, nous obtenons des résultats très compétitifs sur plusieurs tâches de traduction du texte et de la parole.En particulier, nous surpassons l'état de l'art pour la traduction zéro-shot de la parole sur Must-C.Nous présentons également les premiers résultats pour la traduction directe zéro-shot de la parole et la traduction du texte à la parole.\"},\n",
       "             338: {'src': 'LIMSI-COT at SemEval-2016 task 12: Temporal relation identification using a pipeline of classifiersSemEval 2016 Task 12 addresses temporal reasoning in the clinical domain.In this paper, we present our participation for relation extraction based on gold standard entities (subtasks DR and CR).We used a supervised approach comparing plain lexical features to word embeddings for temporal relation identification, and obtained above-median scores.',\n",
       "              'mt': 'LIMSI-COT à SemEval-2016 tâche 12: Identification des relations temporelles à l’aide d’un pipeline de classificateursSemEval 2016 Task 12 aborde le raisonnement temporel dans le domaine clinique.Dans cet article, nous présentons notre participation pour l’extraction relationnelle basée sur des entités d’étalon-or (sous-tâches DR et CR).Nous avons utilisé une approche supervisée comparant les caractéristiques lexicales simples aux incorporations de mots pour l’identification des relations temporelles, et nous avons obtenu des scores supérieurs à la médiane.',\n",
       "              'ref': \"LIMSI-COT à SemEval-2016 tâche 12: Identification de relations temporelles à l’aide d’une chaîne de classifieursLa tâche 12 de SemEval 2016 aborde le raisonnement temporel dans le domaine clinique.Dans cet article, nous présentons notre participation pour l’extraction de relations à partir d'entités de référence (sous-tâches DR et CR).Nous avons utilisé une approche supervisée comparant les caractéristiques lexicales simples aux plongements de mots pour l’identification de relations temporelles, et nous avons obtenu des scores supérieurs à la médiane.\"},\n",
       "             160: {'src': 'Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive EvaluationOne of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as an image.However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations but also by the lack of specific evaluation and training data.We present a new MMT approach based on a strong text-only MT model, which uses neural adapters and a novel guided self-attention mechanism and which is jointly trained on both visual masking and MMT.We also release CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation dataset, composed of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation.Our approach obtains competitive results over strong text-only models on standard English-to-French benchmarks and outperforms these baselines and state-of-the-art MMT systems with a large margin on our contrastive test set.',\n",
       "              'mt': \"S'attaquer à l'ambiguïté avec des images : Traduction automatique multimodale améliorée et évaluation contrastiveL'un des principaux défis de la traduction automatique (TA) est l'ambiguïté, qui peut dans certains cas être résolue par l'accompagnement d'un contexte tel qu'une image.Cependant, des travaux récents dans le domaine de la traduction automatique multimodale ont montré qu'il était difficile d'obtenir des améliorations à partir d'images, limitées non seulement par la difficulté de construire des représentations multimodales efficaces, mais aussi par le manque de données d'évaluation et d'entraînement spécifiques.Nous présentons une nouvelle approche de la MT multimodale basée sur un modèle de MT textuelle solide, qui utilise des adaptateurs neuronaux et un nouveau mécanisme d'auto-attention guidée et qui est entraîné conjointement sur le masquage visuel et la MT multimodale.Nous publions également CoMMuTE, un ensemble de données d'évaluation de la traduction multimodale multilingue contrastive, composé de phrases ambiguës et de leurs traductions possibles, accompagnées d'images désambiguïsantes correspondant à chaque traduction.Notre approche obtient des résultats compétitifs par rapport à des modèles textuels forts sur des benchmarks anglais-français standards et surpasse ces baselines et les systèmes MMT de pointe avec une grande marge sur notre ensemble de tests contrastifs.\",\n",
       "              'ref': \"Lutter contre l'ambiguïté par les images : Traduction automatique multimodale améliorée et évaluation contrastiveL'un des principaux défis de la traduction automatique (TA) est l'ambiguïté, laquelle peut, dans certains cas, être résolue par l'adjonction d'un contexte telle une image.Cependant, des travaux récents dans le domaine de la traduction automatique multimodale ont montré qu'il était difficile d'obtenir des améliorations à partir d'images, en raison non seulement de la difficulté de construire des représentations multimodales efficaces, mais aussi du fait du manque de données d'évaluation et d'entraînement spécifiques.Nous présentons une nouvelle approche de la TA multimodale basée sur un modèle de TA pur texte robuste, qui utilise des adaptateurs neuronaux et un nouveau mécanisme d'auto-attention guidée, et qui est entraîné conjointement sur des images masquées et la TA multimodale.Nous publions également le jeu de données CoMMuTE (Contrastive Multilingual Multimodal Translation Evaluation), composé de phrases ambiguës et de leurs traductions possibles, accompagnées d'images visant à leur désambiguïsation et correspondant à chaque traduction.Notre approche obtient des résultats compétitifs par rapport à des modèles textuels performants sur des benchmarks anglais-français standards et surpasse ces baselines et les systèmes de TA multimodale de pointe d'une large marge sur nos données de test contrastives.\"},\n",
       "             140: {'src': 'Word Representations in Factored Neural Machine TranslationTranslation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena, which is a challenge for neural machine translation architectures.To address this issue, the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words.Within this framework, we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors.We report experiments for translation from English into two morphologically rich languages, Czech and Latvian, and show the importance of explicitly modeling target morphology.',\n",
       "              'mt': \"Représentations de mots dans la traduction automatique neuronale factoriséeLa traduction dans un langage morphologiquement riche nécessite un grand vocabulaire de sortie pour modéliser divers phénomènes morphologiques, ce qui est un défi pour les architectures de traduction automatique neuronale.Pour répondre à cette question, le présent document étudie l'impact d'avoir deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d'information morpho-syntaxique entre les deux facteurs.Nous présentons des expériences de traduction de l'anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l'importance de modéliser explicitement la morphologie cible.\",\n",
       "              'ref': \"Représentations de mots dans la traduction automatique neuronale factoriséeLa traduction dans un langage morphologiquement riche nécessite un grand vocabulaire de sortie pour modéliser divers phénomènes morphologiques, ce qui est un défi pour les architectures de traduction automatique neuronale.Pour répondre à cette problématique, le présent article étudie l'impact qu'entraîne le fait d'avoir deux facteurs de sortie au sein d'un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d'information morphosyntaxique entre les deux facteurs.Nous présentons des expériences de traduction de l'anglais vers deux langues morphologiquement riches, le tchèque et le letton, et démontrons qu'il est important de modéliser explicitement la morphologie cible.\"},\n",
       "             102: {'src': 'Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringWe present a new pre-training method, Multimodal Inverse Cloze Task, for Knowledge-based Visual Question Answering about named Entities (KVQAE).KVQAE is a recently introduced task that consists in answering questions about named entities grounded in a visual context using a Knowledge Base.Therefore, the interaction between the modalities is paramount to retrieve information and must be captured with complex fusion models.As these models require a lot of training data, we design this pre-training task from existing work in textual Question Answering.It consists in considering a sentence as a pseudo-question and its context as a pseudo-relevant passage and is extended by considering images near texts in multimodal documents.Our method is applicable to different neural network architectures and leads to a 9% relative-MRR and 15% relative-F1 gain for retrieval and reading comprehension, respectively, over a no-pre-training baseline.',\n",
       "              'mt': 'Tâche multimodale de fermeture inversée pour la réponse aux questions visuelles basées sur les connaissancesNous présentons une nouvelle méthode de préformation, Multimodal Inverse Cloze Task, pour répondre aux questions visuelles basées sur les connaissances sur les entités nommées (KVQAE).KVQAE est une tâche récemment introduite qui consiste à répondre à des questions sur des entités nommées fondées sur un contexte visuel à l’aide d’une base de connaissances.Par conséquent, l’interaction entre les modalités est primordiale pour récupérer des informations et doit être capturée avec des modèles de fusion complexes.Comme ces modèles nécessitent beaucoup de données de formation, nous concevons cette tâche de préformation à partir du travail existant en réponse aux questions textuelles.Il consiste à considérer une phrase comme une pseudo-question et son contexte comme un passage pseudo-pertinent et s’étend en considérant des images à proximité de textes dans des documents multimodals.Notre méthode est applicable à différentes architectures de réseau neuronal et conduit à un gain de 9\\xa0% relatif-MRR et 15\\xa0% relatif-F1 pour la récupération et la compréhension de lecture, respectivement, sur une base sans formation.',\n",
       "              'ref': 'Tâche de complétion inversée multimodale pour répondre aux questions visuelles basées sur les connaissancesNous présentons une nouvelle méthode de préentraînement, la tâche de complétion inversée multimodale, pour répondre aux questions visuelles à propos d’entités nommées (KVQAE).La KVQAE est une tâche récemment introduite qui consiste à répondre, à l’aide d’une base de connaissances, à des questions à propos d’entités nommées ancrées dans un contexte visuel.Par conséquent, l’interaction entre les modalités est primordiale pour rechercher des informations et doit être capturée avec des modèles de fusion complexes.Comme ces modèles nécessitent beaucoup de données d’entraînement, nous concevons cette tâche de préentraînement à partir de travaux existants en question-réponse textuel.Cette tâche consiste à considérer une phrase comme une pseudo-question et son contexte comme un passage pseudo-pertinent et s’étend en considérant des images à proximité de textes dans des documents multimodaux.Notre méthode est applicable à différentes architectures de réseaux de neurones et conduit à un gain relatif de 9\\xa0% de MRR et 15\\xa0% de F1 pour la recherche d’information et l’extraction de réponse, respectivement, par rapport à un modèle sans préentraînement.'},\n",
       "             184: {'src': 'Machine TranslationThe dream of a universal translation device goes back many decades, long before Douglas Adams’s fictional Babel fish provided this service in The Hitchhiker’s Guide to the Galaxy.Since the advent of computers, research has focused on the design of digital machine translation tools—computer programs capable of automatically translating a text from a source language to a target language.This has become one of the most fundamental tasks of artificial intelligence.This volume in the MIT Press Essential Knowledge series offers a concise, nontechnical overview of the development of machine translation, including the different approaches, evaluation issues, and market potential.The main approaches are presented from a largely historical perspective and in an intuitive manner, allowing the reader to understand the main principles without knowing the mathematical details.The book begins by discussing problems that must be solved during the development of a machine translation system and offering a brief overview of the evolution of the field.It then takes up the history of machine translation in more detail, describing its pre-digital beginnings, rule-based approaches, the 1966 ALPAC (Automatic Language Processing Advisory Committee) report and its consequences, the advent of parallel corpora, the example-based paradigm, the statistical paradigm, the segment-based approach, the introduction of more linguistic knowledge into the systems, and the latest approaches based on deep learning.Finally, it considers evaluation challenges and the commercial status of the field, including activities by such major players as Google and Systran.',\n",
       "              'mt': 'Traduction automatiqueLe rêve d’un dispositif de traduction universel remonte à de nombreuses décennies, bien avant que le poisson fictif Babel de Douglas Adams fournisse ce service dans The Hitchhiker’s Guide to the Galaxy.Depuis l’avènement des ordinateurs, la recherche s’est concentrée sur la conception d’outils numériques de traduction automatique — des programmes informatiques capables de traduire automatiquement un texte d’une langue source vers une langue cible.C’est devenu l’une des tâches les plus fondamentales de l’intelligence artificielle.Ce volume de la série MIT Press Essential Knowledge offre un aperçu concis et non technique du développement de la traduction automatique, y compris les différentes approches, les questions d’évaluation et le potentiel du marché.Les approches principales sont présentées d’un point de vue largement historique et d’une manière intuitive, permettant au lecteur de comprendre les principes principaux sans connaître les détails mathématiques.Le livre commence par discuter des problèmes qui doivent être résolus lors du développement d’un système de traduction automatique et offrir un bref aperçu de l’évolution du domaine.Il reprend ensuite plus en détail l’histoire de la traduction automatique, décrivant ses débuts prénumériques, ses approches fondées sur des règles, le rapport de 1966 ALPAC (Automatic Language Processing Advisory Committee) et ses conséquences, l’avènement de corpus parallèles, le paradigme basé sur l’exemple, le paradigme statistique, l’approche par segment, l’introduction de plus de connaissances linguistiques dans les systèmes, et les dernières approches basées sur l’apprentissage profond.Enfin, il examine les enjeux d’évaluation et le statut commercial du terrain, y compris les activités d’acteurs majeurs tels que Google et Systran.',\n",
       "              'ref': \"Traduction automatiqueLe rêve d’un dispositif de traduction universel est vieux de plusieurs décennies, bien avant que le poisson Babel fictif de Douglas Adams ne fournisse ce service dans Le Guide du voyageur galactique.Depuis l’avènement des ordinateurs, la recherche s’est concentrée sur la conception d’outils numériques de traduction automatique — des programmes informatiques capables de traduire automatiquement un texte d’une langue source vers une langue cible.C’est devenu l’une des tâches les plus fondamentales de l’intelligence artificielle.Ce volume de la série MIT Press Essential Knowledge offre un aperçu concis et non technique du développement de la traduction automatique, traitant des différentes approches, des questions d’évaluation et de son potentiel commercial.Les approches principales sont présentées d’un point de vue largement historique et d’une manière intuitive, permettant au lecteur d'en comprendre les principes majeurs sans en connaître les détails mathématiques.Le livre commence par discuter des problèmes qui doivent être résolus lors du développement d’un système de traduction automatique, offrant un bref aperçu de l’évolution du domaine.Il reprend ensuite plus en détail l’histoire de la traduction automatique, décrivant ses débuts pré-numériques, ses approches fondées sur des règles, le rapport ALPAC de 1966 (Automatic Language Processing Advisory Committee) et ses conséquences, l’avènement de corpus parallèles, le paradigme basé sur l’exemple, le paradigme statistique, l’approche par segment, l’introduction de plus de connaissances linguistiques dans les systèmes, et les dernières approches basées sur l’apprentissage profond.Enfin, il examine les enjeux d’évaluation et le statut commercial du domaine, y compris les activités d’acteurs majeurs tels que Google et Systran.\"},\n",
       "             136: {'src': 'Focused Concatenation for Context-Aware Neural Machine TranslationA straightforward approach to context-aware neural machine translation consists in feeding the standard encoder-decoder architecture with a window of consecutive sentences, formed by the current sentence and a number of sentences from its context concatenated to it.In this work, we propose an improved concatenation approach that encourages the model to focus on the translation of the current sentence, discounting the loss generated by target context.We also propose an additional improvement that strengthen the notion of sentence boundaries and of relative sentence distance, facilitating model compliance to the context-discounted objective.We evaluate our approach with both average-translation quality metrics and contrastive test sets for the translation of inter-sentential discourse phenomena, proving its superiority to the vanilla concatenation approach and other sophisticated context-aware systems.',\n",
       "              'mt': 'Concaténation ciblée pour la traduction automatique de neurones en contexteUne approche simple de la traduction automatique neuronale consciente du contexte consiste à alimenter l’architecture standard de codeur-décodeur avec une fenêtre de phrases consécutives, formée par la phrase actuelle et un certain nombre de phrases de son contexte concaténé à elle.Dans ce travail, nous proposons une approche de concaténation améliorée qui encourage le modèle à se concentrer sur la traduction de la phrase actuelle, en évitant la perte générée par le contexte cible.Nous proposons également une amélioration supplémentaire qui renforce la notion de limites de peine et de distance relative des peines, facilitant ainsi la conformité du modèle à l’objectif de réduction du contexte.Nous évaluons notre approche à la fois avec des métriques de qualité de traduction moyenne et des ensembles de tests contrastifs pour la traduction de phénomènes de discours intersentiels, prouvant sa supériorité à l’approche de la conception de la vanille et à d’autres systèmes sophistiqués conscients du contexte.',\n",
       "              'ref': \"Concaténation ciblée pour la traduction automatique neuronale en contexteUne approche simple de la traduction automatique neuronale en contexte consiste à fournir à l’architecture encodeur-décodeur standard une fenêtre de phrases consécutives, comprenant la phrase en cours de traduction et un certain nombre de phrases issues de son contexte concaténées à elle.Dans ce travail, nous proposons une approche améliorée de la concaténation qui encourage le modèle à se concentrer sur la phrase en cours de traduction, en décomptant la part d'erreur imputable au contexte cible.Nous proposons également une amélioration supplémentaire qui renforce la notion de limites de phrase et de distance relative entre les phrases, facilitant ainsi la capacité du modèle à respecter l’objectif de décompte du contexte cible.Nous évaluons notre approche à la fois sur la base des moyennes de métriques de qualité de traduction et sur des jeux de données de tests contrastifs pour la traduction de phénomènes de discours interphrastiques, prouvant sa supériorité vis-à-vis de l’approche ordinaire de la concaténation et d’autres systèmes sophistiqués tenant compte du contexte en traduction.\"},\n",
       "             290: {'src': 'A First Corpus of AZee Discourse ExpressionsThis paper presents a corpus of AZee discourse expressions, i.e. expressions which formally describe Sign Language utterances of any length using the AZee approach and language.The construction of this corpus had two main goals: a first reference corpus for AZee, and a test of its coverage on a significant sample of real-life utterances.We worked on productions from an existing corpus, namely the 40 brèves, containing an hour of French Sign Language.We wrote the corresponding AZee discourse expressions for the entire video content, i.e. expressions capturing the forms produced by the signers and their associated meaning by combining known production rules, a basic building block for these expressions.These are made available as a version 2 extension of the 40 brèves.We explain the way in which these expressions can be built, present the resulting corpus and set of production rules used, and perform first measurements on it.We also propose an evaluation of our corpus: for one hour of discourse, AZee allows to describe 94% of it, while ongoing studies are increasing this coverage.This corpus offers a lot of future prospects, for instance concerning synthesis with virtual signers, machine translation or formal grammars for Sign Language.',\n",
       "              'mt': \"Un premier corpus d'expressions de discours AZeeCet article présente un corpus d'expressions de discours AZee, c'est-à-dire des expressions qui décrivent formellement des énoncés de langage gestuel de n'importe quelle longueur en utilisant l'approche et le langage AZee.La construction de ce corpus avait deux objectifs principaux : un premier corpus de référence pour AZee, et un test de sa couverture sur un échantillon significatif d'énoncés de la vie réelle.Nous avons travaillé sur des productions issues d'un corpus existant, à savoir les 40 brèves, contenant une heure de Langue des Signes Française.Nous avons écrit les expressions de discours AZee correspondantes pour l'ensemble du contenu vidéo, c'est-à-dire des expressions capturant les formes produites par les signataires et leur signification associée en combinant des règles de production connues, un bloc de base pour ces expressions.Ceux-ci sont disponibles en version 2 extension des 40 brèves.Nous expliquons la façon dont ces expressions peuvent être construites, présentons le corpus résultant et l'ensemble des règles de production utilisées, et effectuons les premières mesures.Nous proposons également une évaluation de notre corpus : pendant une heure de discours, AZee permet d'en décrire 94%, alors que des études en cours accroissent cette couverture.Ce corpus offre de nombreuses perspectives d'avenir, par exemple en matière de synthèse avec des signataires virtuels, de traduction automatique ou de grammaires formelles pour la langue des signes.\",\n",
       "              'ref': \"Un premier corpus d'expressions de discours AZeeCet article présente un corpus d'expressions AZee de discours, c'est-à-dire des expressions qui décrivent formellement des énoncés en langue des signes de n'importe quelle longueur en utilisant l'approche et le langage AZee.La construction de ce corpus avait deux objectifs principaux : un premier corpus de référence pour AZee, et un test de sa couverture sur un échantillon significatif d'énoncés de la vie réelle.Nous avons travaillé sur des productions issues d'un corpus existant, à savoir les 40 brèves, contenant une heure de Langue des Signes Française.Nous avons écrit les expressions de discours AZee correspondantes pour l'ensemble du contenu vidéo, c'est-à-dire des expressions capturant les formes produites par les signeurs et la signification qui leur est associée, en combinant des règles de production connues, un bloc de base pour ces expressions.Celles-ci sont disponibles dans une version 2, extension des 40 brèves.Nous expliquons la façon dont ces expressions peuvent être construites, présentons le corpus résultant et l'ensemble des règles de production utilisées, et effectuons de premières mesures.Nous proposons également une évaluation de notre corpus : pour une heure de discours, AZee permet d'en décrire 94%, tandis que des études en cours accroissent cette couverture.Ce corpus offre de nombreuses perspectives pour le futur, par exemple en matière de synthèse avec des signeurs virtuels, de traduction automatique ou de grammaires formelles pour la langue des signes.\"},\n",
       "             344: {'src': 'De-Identification of French Unstructured Clinical Notes for Machine Learning TasksUnstructured textual data are at the heart of health systems: liaison letters between doctors, operating reports, coding of procedures according to the ICD-10 standard, etc.The details included in these documents make it possible to get to know the patient better, to better manage him or her, to better study the pathologies, to accurately remunerate the associated medical acts.. .All this seems to be (at least partially) within reach of today by artificial intelligence techniques.However, for obvious reasons of privacy protection, the designers of these AIs do not have the legal right to access these documents as long as they contain identifying data.Deidentifying these documents, i.e. detecting and deleting all identifying information present in them, is a legally necessary step for sharing this data between two complementary worlds.Over the last decade, several proposals have been made to de-identify documents, mainly in English.While the detection scores are often high, the substitution methods are often not very robust to attack.In French, very few methods are based on arbitrary detection and/or substitution rules.In this paper, we propose a new comprehensive de-identification method dedicated to French-language medical documents.Both the approach for the detection of identifying elements (based on deep learning) and their substitution (based on differential privacy) are based on the most proven existing approaches.The result is an approach that effectively protects the privacy of the patients at the heart of these medical documents.The whole approach has been evaluated on a French language medical dataset of a French public hospital and the results are very encouraging.',\n",
       "              'mt': \"Désidentification des notes cliniques non structurées françaises pour les tâches d'apprentissage automatiqueLes données textuelles non structurées sont au coeur des systèmes de santé : lettres de liaison entre médecins, rapports d'opération, codage des procédures selon la norme CIM-10, etc.Les détails inclus dans ces documents permettent de mieux connaître le patient, de mieux le prendre en charge, de mieux étudier les pathologies, de rémunérer avec précision les actes médicaux associés. .Tout cela semble être (au moins partiellement) à la portée d'aujourd'hui par les techniques d'intelligence artificielle.Cependant, pour des raisons évidentes de protection de la vie privée, les concepteurs de ces IA n'ont pas le droit légal d'accéder à ces documents tant qu'ils contiennent des données d'identification.La désidentification de ces documents, c'est-à-dire la détection et la suppression de toutes les informations d'identification qu'ils contiennent, est une étape juridiquement nécessaire pour partager ces données entre deux mondes complémentaires.Au cours de la dernière décennie, plusieurs propositions ont été faites pour désidentifier les documents, principalement en anglais.Bien que les scores de détection soient souvent élevés, les méthodes de substitution ne sont souvent pas très robustes à attaquer.En français, très peu de méthodes sont basées sur des règles de détection et/ou de substitution arbitraires.Dans le présent document, nous proposons une nouvelle méthode globale de désidentification dédiée aux documents médicaux de langue française.Tant l'approche de détection des éléments d'identification (basée sur l'apprentissage profond) que leur substitution (basée sur la confidentialité différentielle) sont basées sur les approches existantes les plus éprouvées.Le résultat est une approche qui protège efficacement la vie privée des patients au coeur de ces documents médicaux.L'ensemble de l'approche a été évalué sur un ensemble de données médicales de langue française d'un hôpital public français et les résultats sont très encourageants.\",\n",
       "              'ref': \"Désidentification de documents cliniques non structurés en français pour les tâches d'apprentissage machineLes données textuelles non structurées sont au cœur des systèmes de santé : courriers entre médecins, rapports d'opérations, codage des procédures selon la norme CIM-10, etc.Les détails inclus dans ces documents permettent de mieux connaître le patient, de mieux le prendre en charge, de mieux étudier les pathologies, de rémunérer avec précision les actes médicaux associés. .Tout cela semble être (au moins partiellement) à la portée d'aujourd'hui par les techniques d'intelligence artificielle.Cependant, pour des raisons évidentes de protection de la vie privée, les concepteurs de ces IA n'ont légalement pas le droit d'accéder à ces documents tant qu'ils contiennent des données identifiantes.La désidentification de ces documents, c'est-à-dire la détection et la suppression de toutes les informations directement identifiantes qu'ils contiennent, est une étape juridiquement nécessaire pour partager ces données entre deux mondes complémentaires.Au cours de la dernière décennie, plusieurs propositions ont été faites pour désidentifier les documents, principalement en anglais.Bien que les scores de détection soient souvent élevés, les méthodes de substitution ne sont souvent pas très robustes aux attaques.En français, très peu de méthodes sont basées sur des règles de détection et/ou de substitution arbitraires.Dans le présent document, nous proposons une nouvelle méthode globale de désidentification dédiée aux documents médicaux de langue française.Tant l'approche de détection des éléments identifiants (basée sur l'apprentissage profond) que leur substitution (basée sur la confidentialité différentielle) sont basées sur les approches existantes les plus éprouvées.Le résultat est une approche qui protège efficacement la vie privée des patients au cœur de ces documents médicaux.L'ensemble de l'approche a été évalué sur un corpus de données médicales de langue française d'un hôpital public français et les résultats sont très encourageants.\"},\n",
       "             162: {'src': 'Screening Gender Transfer in Neural Machine TranslationThis paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English.Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system.Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.',\n",
       "              'mt': 'Dépistage du transfert de genre dans la traduction automatique neuronaleCet article vise à identifier le flux d’informations dans les systèmes de traduction automatique de pointe, en prenant comme exemple le transfert du genre lors de la traduction du français vers l’anglais.À l’aide d’un ensemble contrôlé d’exemples, nous expérimentons plusieurs façons d’étudier comment les informations de genre circulent dans une architecture de codeur-décodeur en tenant compte à la fois des techniques de sondage et des interventions sur les représentations internes utilisées dans le système MT.Nos résultats montrent que l’information sur le genre peut être trouvée dans toutes les représentations symboliques construites par l’encodeur et le décodeur et nous amène à conclure qu’il existe de multiples voies de transfert de genre.',\n",
       "              'ref': 'Dépistage du transfert de genre dans la traduction automatique neuronaleCet article vise à identifier le flux d’informations dans les systèmes de traduction automatique de pointe, en prenant comme exemple le transfert du genre lors de la traduction du français vers l’anglais.À l’aide d’un ensemble contrôlé d’exemples, nous expérimentons plusieurs façons d’étudier comment les informations de genre circulent dans une architecture de encodeur-décodeur en nous basant à la fois sur des techniques de sondage et sur des interventions sur les représentations internes utilisées dans le système de TA.Nos résultats montrent que l’information sur le genre peut être trouvée dans toutes les représentations symboliques construites par l’encodeur et le décodeur et nous amène à conclure qu’il existe de multiples voies de transfert de genre.'},\n",
       "             302: {'src': 'Combining MEDLINE and publisher data to create parallel corpora for the automatic translation of biomedical textBackground: Most of the institutional and research information in the biomedical domain is available in the formof English text.Even in countries where English is an official language, such as the United States, language can be abarrier for accessing biomedical information for non-native speakers.Recent progress in machine translationsuggests that this technique could help make English texts accessible to speakers of other languages.However, thelack of adequate specialized corpora needed to train statistical models currently limits the quality of automatictranslations in the biomedical domain.Results: We show how a large-sized parallel corpus can automatically be obtained for the biomedical domain,using the MEDLINE database.The corpus generated in this work comprises article titles obtained from MEDLINEand abstract text automatically retrieved from journal websites, which substantially extends the corpora used inprevious work.After assessing the quality of the corpus for two language pairs (English/French and English/Spanish)we use the Moses package to train a statistical machine translation model that outperforms previous models forautomatic translation of biomedical text.Conclusions: We have built translation data sets in the biomedical domain that can easily be extended to otherlanguages available in MEDLINE.These sets can successfully be applied to train statistical machine translationmodels.While further progress should be made by incorporating out-of-domain corpora and domain-specificlexicons, we believe that this work improves the automatic translation of biomedical texts.',\n",
       "              'mt': \"Combiner MEDLINE et les données des éditeurs pour créer des corpus parallèles pour la traduction automatique de textes biomédicauxContexte : La plupart des informations institutionnelles et de recherche dans le domaine biomédical sont disponibles sous forme de texte en anglais.Même dans les pays où l'anglais est une langue officielle, comme les États-Unis, la langue peut constituer un obstacle à l'accès à l'information biomédicale pour les locuteurs non natifs.Les progrès récents de la traduction automatique laissent penser que cette technique pourrait contribuer à rendre les textes anglais accessibles aux locuteurs d'autres langues.Cependant, le manque de corpus spécialisés adéquats nécessaires à l'entraînement des modèles statistiques limite actuellement la qualité des traductions automatiques dans le domaine biomédical.Résultats : Nous montrons comment un corpus parallèle de grande taille peut être obtenu automatiquement pour le domaine biomédical, en utilisant la base de données MEDLINE.Le corpus généré dans ce travail comprend des titres d'articles obtenus à partir de MEDLINE et des textes de résumés récupérés automatiquement sur les sites web des revues, ce qui élargit considérablement les corpus utilisés dans les travaux précédents.Après avoir évalué la qualité du corpus pour deux paires de langues (anglais/français et anglais/espagnol), nous utilisons le logiciel Moses pour entraîner un modèle statistique de traduction automatique qui surpasse les modèles précédents pour la traduction automatique de textes biomédicaux :Nous avons construit des ensembles de données de traduction dans le domaine biomédical qui peuvent facilement être étendus à d'autres langues disponibles dans MEDLINE.Ces ensembles peuvent être appliqués avec succès pour former des modèles statistiques de traduction automatique.Bien qu'il faille encore progresser en incorporant des corpus hors domaine et des lexiques spécifiques au domaine, nous pensons que ce travail améliore la traduction automatique des textes biomédicaux.\",\n",
       "              'ref': \"Intégration de méta-données issues de MEDLINE et des sites d'éditeurs de revues pour la création de corpus parallèles dans le domaine biomédicalContexte : Une quantité importante d'informations institutionnelles et concernant la recherche dans le domaine biomédical sont disponibles sous forme de texte en anglais.Même dans les pays où l'anglais est une langue officielle, comme les États-Unis, la langue peut constituer un obstacle à l'information biomédicale pour les locuteurs non natifs.Grâce aux progrès récents de la traduction automatique, cette technique pourrait améliorer l'accessibilité des textes en anglais aux locuteurs d'autres langues.Cependant, le manque de corpus spécialisés adéquats nécessaires à l'entraînement des modèles statistiques limite actuellement la qualité des traductions automatiques dans le domaine biomédical.Résultats : Nous montrons comment un corpus parallèle de grande taille peut être obtenu automatiquement pour le domaine biomédical, en utilisant la base de données MEDLINE.Le corpus généré dans ce travail comprend des titres d'articles obtenus à partir de MEDLINE et des résumés récupérés automatiquement sur les sites web des revues, ce qui permet d'obtenir des corpus de plus grande taille que ceux utilisés dans les travaux précédents.Après avoir évalué la qualité du corpus pour deux paires de langues (anglais/français et anglais/espagnol), nous utilisons le logiciel Moses pour entraîner un modèle statistique de traduction automatique qui surpasse les modèles précédents pour la traduction automatique de textes biomédicauxConclusion: Nous avons proposé des corpus parallèles dans le domaine biomédical à l'aide de méthodes qui peuvent facilement être appliquées à d'autres langues utilisées dans les articles indexés dans MEDLINE.Ces corpus peuvent être utilisés avec succès pour entraîner des modèles statistiques de traduction automatique.Il serait souhaitable d'intégrer également des corpus hors domaine et des lexiques spécifiques au domaine. Néanmoins, nous pensons que ce travail offre une contribution intéressante pour la traduction automatique de textes biomédicaux.\"},\n",
       "             86: {'src': 'Aligning Bilingual Literary Works: a Pilot StudyElectronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common.It is often the case thatliterary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation.To be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difficult task.In this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems.Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach significantly outperforms two open source tools.',\n",
       "              'mt': \"Alignement des œuvres littéraires bilingues : une étude piloteLes versions électroniques d'œuvres littéraires abondent sur l'internet et la diffusion rapide des lecteurs électroniques rendra les livres électroniques de plus en plus courants.Il arrive souvent que des œuvres littéraires existent dans plus d'une langue, ce qui suggère que, si elles sont correctement alignées, elles pourraient être transformées en ressources utiles pour de nombreuses applications pratiques, telles que les aides à l'écriture et à l'apprentissage des langues, les études de traduction ou la traduction automatique basée sur des données.Pour être utiles, ces œuvres bilingues doivent être alignées aussi précisément que possible, une tâche notoirement difficile.Dans cet article, nous revisitons le problème de l'alignement des phrases pour les œuvres littéraires et explorons les performances d'une nouvelle approche, à plusieurs passages, basée sur une combinaison de systèmes.Des expériences menées sur des extraits de dix chefs-d'œuvre de la littérature française et anglaise montrent que notre approche surpasse de manière significative deux outils open source.\",\n",
       "              'ref': \"Alignement des œuvres littéraires bilingues : une étude piloteLes versions électroniques d'œuvres littéraires abondent sur Internet et la diffusion rapide des liseuses électroniques rendra les livres électroniques de plus en plus courants.Il arrive souvent que des œuvres littéraires existent dans plus d'une langue, ce qui suggère que, si elles sont correctement alignées, elles pourraient être transformées en ressources utiles pour de nombreuses applications pratiques, telles que les aides à l'écriture et à l'apprentissage des langues, les études de traduction ou la traduction automatique à base de données.Pour être utiles, ces œuvres bilingues doivent être alignées aussi précisément que possible, une tâche notoirement difficile.Dans cet article, nous revisitons le problème de l'alignement de phrases pour les œuvres littéraires et explorons les performances d'une nouvelle approche multi-passe basée sur une combinaison de systèmes.Des expériences menées sur des extraits de dix chefs-d'œuvre de la littérature française et anglaise montrent que notre approche surpasse de manière significative deux outils open source.\"},\n",
       "             329: {'src': 'You Reap What You Sow: On the Challenges of Bias Evaluation Under Multilingual SettingsEvaluating bias, fairness, and social impact in monolingual language models is a difficult task.This challenge is further compounded when language modeling occurs in a multilingual context.Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks:(1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.',\n",
       "              'mt': 'Vous récoltez ce que vous semez: Sur les défis de l’évaluation des risques dans des paramètres multilinguesL’évaluation des préjugés, de l’équité et de l’impact social dans les modèles linguistiques monolingues est une tâche difficile.Ce défi est encore aggravé lorsque la modélisation linguistique se produit dans un contexte multilingue.Compte tenu de l’implication des biais d’évaluation pour les grands modèles linguistiques multilingues, nous situons la discussion de l’évaluation des biais dans un contexte plus large de recherche scientifique sociale avec des travaux de calcul.Nous soulignons trois dimensions de l’élaboration de cadres d’évaluation des biais multilingues:1) accroître la transparence grâce à la documentation, 2) élargir les cibles de préjugés au-delà du sexe, et (3) traiter les différences culturelles qui existent entre les langues.Nous discutons en outre de la dynamique de puissance et des conséquences de la formation de grands modèles linguistiques et recommandons aux chercheurs de rester conscients des ramifications du développement de telles technologies.',\n",
       "              'ref': \"On récolte ce que l'on sème: défis de l’évaluation des biais dans un contexte multilingueL’évaluation des biais, de l’équité et de l’impact sociétal des modèles de langue monolingues est une tâche difficile.Le défi est d'autant plus ambitieux dans le contexte d'une modélisation multilingue.Compte tenu de l’enjeu que représente l'évaluation des biais dans les grands modèles de langue multilingues, nous positionnons la discussion de l’évaluation des biais dans le contexte plus large d'une recherche scientifique impactant la société par le numérique.Nous soulignons trois dimensions de l’élaboration de cadres d’évaluation multilingues des biais :1) améliorer la transparence grâce à la documentation, 2) élargir le spectre des stéréotypes étudiés au-delà du genre, et (3) prendre en compte les différences culturelles qui existent entre les langues.Nous discutons en outre de la dynamique de pouvoir et des conséquences de l'entraînement de grands modèles de langue; nous recommandons aux chercheurs de rester conscients des ramifications du développement de telles technologies.\"},\n",
       "             104: {'src': 'Continuous space translation models with neural networksThe use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models.For lack of sufficient training data, most models only consider a small amount of context.As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations.In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture.In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.',\n",
       "              'mt': \"Modèles de traduction dans l'espace continu avec des réseaux neuronauxL'utilisation d'estimations conventionnelles du maximum de vraisemblance entrave les performances des modèles de traduction basés sur les phrases existants.Faute de données d'apprentissage suffisantes, la plupart des modèles ne prennent en compte qu'une petite partie du contexte.Pour y remédier partiellement, nous explorons ici plusieurs modèles de traduction à espace continu, dans lesquels les probabilités de traduction sont estimées à l'aide d'une représentation continue des unités de traduction au lieu des représentations discrètes standard.Afin de traiter un grand nombre d'unités de traduction, ces représentations et les estimations associées sont calculées conjointement à l'aide d'un réseau neuronal multicouche doté d'une architecture SOUL.Dans des expériences à petite et grande échelle de l'anglais vers le français, nous montrons que les modèles résultants peuvent effectivement être formés et utilisés en plus d'un système de traduction par n-grammes, offrant des améliorations significatives en termes de performance.\",\n",
       "              'ref': \"Modèles de traduction dans l'espace continu avec des réseaux neuronauxL'utilisation des estimateurs conventionnels du maximum de vraisemblance limite les performances des modèles de traduction actuels basés sur les segments.Faute de données d'apprentissage suffisantes, la plupart des modèles ne prennent en compte qu'une petite partie du contexte.Pour y remédier partiellement, nous explorons ici plusieurs modèles de traduction neuronaux, dans lesquels les probabilités de traduction sont estimées à l'aide d'une représentation continue des unités de traduction, au lieu des représentations discrètes standard.Afin de traiter un grand nombre d'unités de traduction, ces représentations et les paramètres associés sont calculés conjointement à l'aide d'un réseau neuronal multicouche doté d'une architecture SOUL.Dans des expériences de traduction depuis l'anglais vers le français, à petite et à grande échelle, nous montrons que les modèles résultants peuvent effectivement être entraînés et utilisés en complément d'un système de traduction par n-grammes, et permettent d'obtenir des améliorations significatives des performances.\"},\n",
       "             145: {'src': 'LIUM Machine Translation Systems for WMT17 News Translation TaskThis paper describes LIUM submissions to WMT17 News Translation Task for English↔German, English↔Turkish, English→Czech and English→Latvian language pairs.We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.Competitive scores were obtained by en-sembling various systems and exploiting the availability of target monolingual corpora for back-translation.The impact of back-translation quantity and quality is also analyzed for English→Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.',\n",
       "              'mt': 'Systèmes de traduction automatique Lium pour WMT17 Nouvelles Tâche de TraductionCet article décrit les soumissions de LIUM à WMT17 News Translation Task pour l’anglais, l’allemand, l’anglais, l’anglais, le tchèque et l’anglais→langue latine.Nous formons des systèmes de traduction automatique neuronales attentifs basés sur BPE avec et sans sorties factorisées en utilisant le framework nmtpy open source.Des scores compétitifs ont été obtenus par l’assemblage de divers systèmes et l’exploitation de la disponibilité des corpus monolingues cibles pour la traduction en arrière.L’impact de la rétro-traduction quantitative et de la qualité est également analysé pour English→Turkish où notre soumission post-date a dépassé la meilleure entrée de +\\xa01,6 BLEU.',\n",
       "              'ref': \"Systèmes de traduction automatique du LIUM pour la tâche de traduction d'articles de presse de la conférence WMT17Cet article décrit les contributions du LIUM à la tâche de traduction d'articles de presse de la conférence WMT17 pour les paires de langues anglais↔allemand, anglais↔turc, anglais→tchèque et anglais→letton.Nous avons entraîné des systèmes de traduction automatique neuronale attentifs basés sur la tokenisation BPE, avec et sans sorties factorisées, en utilisant la suite d'outils libre nmtpy.Des scores compétitifs ont été obtenus par l’assemblage de divers systèmes et l’exploitation de la disponibilité des corpus monolingues cibles pour la rétro-traduction.L’impact de la quantité et de la qualité de la rétro-traduction a également été analysé pour la direction anglais→turc où notre contribution, soumise après la date limite, a dépassé le meilleur concurrent de +\\xa01,6 BLEU.\"},\n",
       "             178: {'src': 'Findings of the 2022 Conference on Machine Translation (WMT22)This paper presents the results of the General Machine Translation Task organised as part of the Conference on Machine Translation (WMT) 2022.In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of four different domains.We evaluate system outputs with human annotators using two different techniques: reference-based direct assessment and (DA) and a combination of DA and scalar quality metric (DA+SQM).',\n",
       "              'mt': 'Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Cet article présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans le cadre de la tâche générale de MT, les participants ont été invités à construire des systèmes de traduction automatique pour l’une des 11 paires de langues, à évaluer sur des ensembles de tests composés de quatre domaines différents.Nous évaluons les sorties du système avec des annotateurs humains en utilisant deux techniques différentes: évaluation directe basée sur les références et (DA) et combinaison de DA et de mesure de la qualité scalaire (DA+SQM).',\n",
       "              'ref': 'Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Cet article présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans le cadre de la tâche générale de TA, les participants ont été invités à construire des systèmes de traduction automatique pour l’une des 11 paires de langues, devant être évalués sur des jeux de données de tests composés de quatre domaines distincts.Nous évaluons les traductions produites par ces systèmes grâce à des annotateurs humains en utilisant deux techniques différentes: évaluation directe basée sur les références et (DA) et combinaison de DA et de mesure de la qualité scalaire (DA+SQM).'},\n",
       "             97: {'src': 'Effectiveness of French Language Models on Abstractive Dialogue Summarization TaskPre-trained language models have established the state-of-the-art on various natural language processing tasks, including dialogue summarization, which allows the reader to quickly access key information from long conversations in meetings, interviews or phone calls.However, such dialogues are still difficult to handle with current models because the spontaneity of the language involves expressions that are rarely present in the corpora used for pre-training the language models.Moreover, the vast majority of the work accomplished in this field has been focused on English.In this work, we present a study on the summarization of spontaneous oral dialogues in French using several language specific pre-trained models: BARThez, and BelGPT-2, as well as multilingual pre-trained models: mBART, mBARThez, and mT5.Experiments were performed on the DECODA (Call Center) dialogue corpus whose task is to generate abstractive synopses from call center conversations between a caller and one or several agents depending on the situation.Results show that the BARThez models offer the best performance far above the previous state-of-the-art on DECODA.We further discuss the limits of such pre-trained models and the challenges that must be addressed for summarizing spontaneous dialogues.',\n",
       "              'mt': \"Efficacité des modèles de langue française sur le dialogue abstrait Tâche de synthèseDes modèles linguistiques pré-formés ont établi l'état de l'art sur diverses tâches de traitement du langage naturel, y compris la synthèse des dialogues, ce qui permet au lecteur d'accéder rapidement à des informations clés à partir de longues conversations lors de réunions, d'entretiens ou d'appels téléphoniques.Cependant, ces dialogues sont encore difficiles à gérer avec les modèles actuels car la spontanéité du langage implique des expressions rarement présentes dans les corpus utilisés pour la pré-formation des modèles linguistiques.En outre, la grande majorité du travail accompli dans ce domaine a été axée sur l'anglais.Dans ce travail, nous présentons une étude sur la synthèse des dialogues oraux spontanés en français à l'aide de plusieurs modèles préformés spécifiques à la langue : BARThez et BelGPT-2, ainsi que des modèles multilingues préformés : mBART, mBARThez, et mT5.Des expériences ont été réalisées sur le corpus de dialogue DECODA (Call Center) dont la tâche est de générer des synopsis abstraits à partir de conversations de call center entre un appelant et un ou plusieurs agents selon la situation.Les résultats montrent que les modèles BARThez offrent les meilleures performances bien au-dessus de l'état de l'art antérieur sur DECODA.Nous examinons en outre les limites de ces modèles préformés et les défis à relever pour résumer les dialogues spontanés.\",\n",
       "              'ref': \"Efficacité des modèles de langue française sur le dialogue abstrait Tâche de synthèseDes modèles linguistiques pré-formés ont établi l'état de l'art sur diverses tâches de traitement du langage naturel, y compris la synthèse des dialogues, ce qui permet au lecteur d'accéder rapidement à des informations clés à partir de longues conversations lors de réunions, d'entretiens ou d'appels téléphoniques.Cependant, ces dialogues sont encore difficiles à gérer avec les modèles actuels car la spontanéité du langage implique des expressions rarement présentes dans les corpus utilisés pour la pré-formation des modèles linguistiques.En outre, la grande majorité du travail accompli dans ce domaine a été axée sur l'anglais.Dans ce travail, nous présentons une étude sur la synthèse des dialogues oraux spontanés en français à l'aide de plusieurs modèles préformés spécifiques à la langue : BARThez et BelGPT-2, ainsi que des modèles multilingues préformés : mBART, mBARThez, et mT5.Des expériences ont été réalisées sur le corpus de dialogue DECODA (Call Center) dont la tâche est de générer des synopsis abstraits à partir de conversations de call center entre un appelant et un ou plusieurs agents selon la situation.Les résultats montrent que les modèles BARThez offrent les meilleures performances bien au-dessus de l'état de l'art antérieur sur DECODA.Nous examinons en outre les limites de ces modèles préformés et les défis à relever pour résumer les dialogues spontanés.\"},\n",
       "             177: {'src': 'Findings of the 2022 Conference on Machine Translation (WMT22)This paper presents the results of the General Machine Translation Task organised as part of the Conference on Machine Translation (WMT) 2022.In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of four different domains.We evaluate system outputs with human annotators using two different techniques: reference-based direct assessment and (DA) and a combination of DA and scalar quality metric (DA+SQM).',\n",
       "              'mt': \"Résultats de la Conférence 2022 sur la traduction automatique (WMT22)Cet article présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la conférence sur la traduction automatique (WMT) 2022.Dans le cadre de la tâche générale de traduction automatique, il a été demandé aux participants de construire des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, afin de les évaluer sur des ensembles de tests composés de quatre domaines différents.Nous évaluons les résultats des systèmes avec des annotateurs humains en utilisant deux techniques différentes : l'évaluation directe basée sur la référence (DA) et une combinaison de DA et de métrique de qualité scalaire (DA+SQM).\",\n",
       "              'ref': \"Résultats de la Conférence 2022 sur la traduction automatique (WMT22)Cet article présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la conférence sur la traduction automatique (WMT) 2022.Dans le cadre de la tâche générale de traduction automatique, il a été demandé aux participants de construire des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, afin de les évaluer sur des jeux de données de tests composés de quatre domaines différents.Nous évaluons les résultats des systèmes avec des annotateurs humains en utilisant deux techniques différentes : l'évaluation directe basée sur la référence (DA) et une combinaison de DA et de métrique de qualité scalaire (DA+SQM).\"},\n",
       "             337: {'src': 'Can reproducibility be improved in clinical natural language processing? A study of 7 clinical NLP suitesAbstract BackgroundThe increasing complexity of data streams and computational processes in modern clinical health information systems makes reproducibility challenging.Clinical natural language processing (NLP) pipelines are routinely leveraged for the secondary use of data.Workflow management systems (WMS) have been widely used in bioinformatics to handle the reproducibility bottleneck.ObjectiveTo evaluate if WMS and other bioinformatics practices could impact the reproducibility of clinical NLP frameworks.Materials and MethodsBased on the literature across multiple researcho fields (NLP, bioinformatics and clinical informatics) we selected articles which (1) review reproducibility practices and (2) highlight a set of rules or guidelines to ensure tool or pipeline reproducibility.We aggregate insight from the literature to define reproducibility recommendations.Finally, we assess the compliance of 7 NLP frameworks to the recommendations.ResultsWe identified 40 reproducibility features from 8 selected articles.Frameworks based on WMS match more than 50% of features (26 features for LAPPS Grid, 22 features for OpenMinted) compared to 18 features for current clinical NLP framework (cTakes, CLAMP) and 17 features for GATE, ScispaCy, and Textflows.Discussion34 recommendations are endorsed by at least 2 articles from our selection.Overall, 15 features were adopted by every NLP Framework.Nevertheless, frameworks based on WMS had a better compliance with the features.ConclusionNLP frameworks could benefit from lessons learned from the bioinformatics field (eg, public repositories of curated tools and workflows or use of containers for shareability) to enhance the reproducibility in a clinical setting.',\n",
       "              'mt': \"La reproductibilité peut-elle être améliorée dans le traitement clinique du langage naturel ? Étude de 7 suites cliniques de PNLRésumé ContexteLa complexité croissante des flux de données et des processus informatiques dans les systèmes modernes d'information clinique sur la santé complique la reproductibilité.Les pipelines de traitement clinique du langage naturel (PNL) sont couramment exploités pour l'utilisation secondaire des données.Les systèmes de gestion de flux de travail (WMS) ont été largement utilisés en bioinformatique pour gérer le goulot d'étranglement de la reproductibilité.ObjectifÉvaluer si les SMM et d'autres pratiques bioinformatiques pourraient avoir une incidence sur la reproductibilité des cadres cliniques de PNL.Matériels et méthodesÀ partir de la documentation dans de multiples domaines de recherche (PNL, bioinformatique et informatique clinique), nous avons sélectionné des articles qui (1) examinent les pratiques de reproductibilité et (2) mettent en évidence un ensemble de règles ou de lignes directrices pour assurer la reproductibilité de l'outil ou du pipeline.Nous avons regroupé les connaissances tirées de la documentation pour définir les recommandations en matière de reproductibilité.Enfin, nous évaluons la conformité de 7 cadres de PNL aux recommandations.RésultatsNous avons identifié 40 caractéristiques de reproductibilité à partir de 8 articles sélectionnés.Les cadres basés sur WMS correspondent à plus de 50 % des fonctionnalités (26 fonctionnalités pour LAPPS Grid, 22 fonctionnalités pour OpenMinted), contre 18 fonctionnalités pour le framework NLP clinique actuel (ctakes, CLAMP) et 17 fonctionnalités pour GATE, ScispaCy et Textflows.Discussion34 recommandations sont approuvées par au moins 2 articles de notre sélection.En tout, 15 caractéristiques ont été adoptées par chaque cadre de PNL.Néanmoins, les cadres basés sur WMS avaient une meilleure conformité avec les fonctionnalités.ConclusionLes cadres de PNL pourraient tirer profit des leçons apprises dans le domaine de la bioinformatique (p. ex., dépôts publics d'outils et de flux de travail organisés ou utilisation de conteneurs pour le partage) pour améliorer la reproductibilité en milieu clinique.\",\n",
       "              'ref': \"La reproductibilité peut-elle être améliorée dans le traitement automatique de la langue clinique ? Étude de 7 outils de TAL cliniqueRésumé ContexteLa complexité croissante des flux de données et des processus informatiques dans les systèmes modernes d'information clinique rend la reproductibilité difficile.Des chaines de traitement automatique de la langue (TAL) clinique sont couramment exploitées pour l'utilisation secondaire des données de santé.Les systèmes de gestion des workflows (WMS) sont largement utilisés en bioinformatique pour gérer la problématique de la reproductibilité.ObjectifÉvaluer si les WMS et d'autres pratiques issues de la bioinformatique pourraient avoir une incidence sur la reproductibilité des chaînes de TAL clinique.Matériels et méthodesÀ partir de la littérature de plusieurs domaines de recherche (TAL, bioinformatique et informatique clinique), nous avons sélectionné des articles qui (1) examinent les pratiques de reproductibilité et (2) mettent en évidence un ensemble de règles ou de lignes directrices pour assurer la reproductibilité des outils ou des chaînes de traitement.Nous avons analysé les connaissances issues de la littérature pour définir des recommandations en matière de reproductibilité.Enfin, nous évaluons la conformité de 7 outils de TAL à ces recommandations.RésultatsNous avons identifié 40 critères de reproductibilité à partir de 8 articles sélectionnés.Les outils reposant sur des WMS remplissent plus de 50 % des critères (26 critères pour LAPPS Grid, 22 critères pour OpenMinted), contre 18 critères pour les outils de TAL clinique actuels (ctakes, CLAMP) et 17 critères pour GATE, ScispaCy et Textflows.Discussion34 recommandations sont attestées par au moins 2 articles de notre sélection.En tout, 15 critères ont été adoptées par chacun des outils évalués.Néanmoins, les outils reposant sur des WMS avaient une meilleure conformité avec les critères de reproductibilité.ConclusionLes outils de TAL pourraient tirer profit des leçons apprises dans le domaine de la bioinformatique (p. ex., dépôts publics d'outils et de workflows organisés ou utilisation de conteneurs pour le partage) pour améliorer la reproductibilité dans le domaine clinique.\"},\n",
       "             341: {'src': 'Use of a Citizen Science Platform for the Creation of a Language Resource to Study Bias in Language Models for French: a case studyThere is a growing interest in the evaluation of bias, fairness and social impact of Natural Language Processing models and tools.However, little resources are available for this task in languages other than English.Translation of resources originally developed for English is a promising research direction.However, there is also a need for complementing translated resources by newly sourced resources in the original languages and social contexts studied.In order to collect a language resource for the study of biases in Language Models for French, we decided to resort to citizen science.We created three tasks on the LanguageARC citizen science platform to assist with the translation of an existing resource from English into French as well as the collection of complementary resources in native French.We successfully collected data for all three tasks from a total of 102 volunteer participants.Participants from different parts of the world contributed and we noted that although calls sent to mailing lists had a positive impact on participation, some participants pointed barriers to contributions due to the collection platform.',\n",
       "              'mt': 'Utilisation d’une plateforme scientifique citoyenne pour la création d’une ressource linguistique pour étudier les bias dans les modèles linguistiques pour le français: une étude de casOn s’intéresse de plus en plus à l’évaluation des préjugés, de l’équité et de l’impact social des modèles et des outils de traitement du langage naturel.Cependant, peu de ressources sont disponibles pour cette tâche dans des langues autres que l’anglais.La traduction des ressources initialement développées pour l’anglais est une orientation de recherche prometteuse.Cependant, il est également nécessaire de compléter les ressources traduites par des ressources nouvellement obtenues dans les langues originales et les contextes sociaux étudiés.Afin de recueillir une ressource linguistique pour l’étude des préjugés dans les modèles linguistiques pour le français, nous avons décidé de recourir à la science citoyenne.Nous avons créé trois tâches sur la plateforme de science citoyenne LanguageARC pour aider à la traduction d’une ressource existante de l’anglais vers le français ainsi qu’à la collecte de ressources complémentaires en français natif.Nous avons recueilli avec succès des données pour les trois tâches auprès d’un total de 102 participants volontaires.Des participants de différentes parties du monde ont contribué et nous avons noté que, bien que les appels envoyés aux listes de diffusion aient eu un impact positif sur la participation, certains participants ont souligné les obstacles aux contributions en raison de la plate-forme de collecte.',\n",
       "              'ref': 'Utilisation d’une plateforme scientifique citoyenne pour la création d’une ressource linguistique pour étudier les bias dans les modèles de langue du français: une étude de casL’évaluation des biais, de l’équité et de l’impact sociétal des modèles et des outils de traitement automatique de la langue connaissent un intérêt croissant.Cependant, peu de ressources sont disponibles pour cette tâche dans des langues autres que l’anglais.La traduction des ressources initialement développées pour l’anglais est une piste de recherche prometteuse.Cependant, il est également nécessaire de compléter les ressources traduites par des ressources nouvellement obtenues dans les langues natives et les contextes sociétaux étudiés.Afin de recueillir une ressource linguistique pour l’étude des biais dans les modèles de langue du français, nous avons décidé de recourir à la science citoyenne.Nous avons créé trois tâches sur la plateforme de science citoyenne LanguageARC pour aider à la traduction d’une ressource existante de l’anglais vers le français ainsi qu’à la collecte de ressources complémentaires en français natif.Nous avons recueilli avec succès des données pour les trois tâches auprès d’un total de 102 participants volontaires.Des participants de différentes parties du monde ont contribué et nous avons noté que, bien que les appels envoyés aux listes de diffusion aient eu un impact positif sur la participation, certains participants ont signalé des obstacles aux contributions dues à la plate-forme de collecte.'},\n",
       "             158: {'src': 'On the use of linguistic similarities to improve Neural Machine Translation for African LanguagesIn recent years, there has been a resurgence in research on empirical methods for machine translation.Most of this research has been focused on high-resource, European languages.Despite the fact that around 30% of all languages spoken worldwide are African, the latter have been heavily under investigated and this, partly due to the lack of public parallel corpora online.Furthermore, despite their large number (more than 2,000) and the similarities between them, there is currently no publicly available study on how to use this multilingualism (and associated similarities) to improve machine translation systems performance on African languages.So as to address these issues, we propose a new dataset (from a source that allows us to use and release) for African languages that provides parallel data for vernaculars not present in commonly used dataset like JW300.To exploit multilingualism, we first use a historical approach based on migrations of population to identify similar vernaculars.We also propose a new metric to automatically evaluate similarities between languages.This new metric does not require word level parallelism like traditional methods but only paragraph level parallelism.We then show that performing Masked Language Modelling and Translation Language Modeling in addition to multi-task learning on a cluster of similar languages leads to a strong boost of performance in translating individual pairs inside this cluster.In particular, we record an improvement of 29 BLEU on the pair Bafia-Ewondo using our approaches compared to previous work methods that did not exploit multilingualism in any way.Finally, we release the dataset and code of this work to ensure reproducibility and accelerate research in this domain.',\n",
       "              'mt': \"Sur l'utilisation des similarités linguistiques pour améliorer la traduction automatique neuronale pour les langues africainesCes dernières années, la recherche sur les méthodes empiriques de traduction automatique a connu un regain d'intérêt.La plupart de ces recherches se sont concentrées sur les langues européennes à ressources élevées.Bien qu'environ 30 % de toutes les langues parlées dans le monde soient africaines, ces dernières ont été très peu étudiées, en partie à cause du manque de corpus parallèles publics en ligne.En outre, malgré leur grand nombre (plus de 2 000) et les similitudes entre elles, il n'existe actuellement aucune étude publique sur la manière d'utiliser ce multilinguisme (et les similitudes associées) pour améliorer les performances des systèmes de traduction automatique sur les langues africaines.Pour répondre à ces questions, nous proposons un nouvel ensemble de données (provenant d'une source qui nous autorise à l'utiliser et à le diffuser) pour les langues africaines qui fournit des données parallèles pour les langues vernaculaires qui ne sont pas présentes dans les ensembles de données couramment utilisés tels que JW300.Pour exploiter le multilinguisme, nous utilisons d'abord une approche historique basée sur les migrations de population pour identifier les langues vernaculaires similaires.Nous proposons également une nouvelle mesure pour évaluer automatiquement les similitudes entre les langues.Cette nouvelle métrique ne nécessite pas de parallélisme au niveau des mots comme les méthodes traditionnelles, mais seulement au niveau des paragraphes.Nous montrons ensuite que la modélisation du langage masqué et la modélisation du langage de traduction, en plus de l'apprentissage multitâche sur un groupe de langues similaires, permettent d'améliorer considérablement les performances de traduction des paires individuelles à l'intérieur de ce groupe.En particulier, nous enregistrons une amélioration de 29 BLEU sur la paire Bafia-Ewondo en utilisant nos approches par rapport aux méthodes de travail précédentes qui n'ont pas exploité le multilinguisme de quelque manière que ce soit.Enfin, nous publions l'ensemble des données et le code de ce travail afin de garantir la reproductibilité et d'accélérer la recherche dans ce domaine.\",\n",
       "              'ref': \"De l'utilisation des similarités linguistiques pour l'amélioration de la traduction automatique neuronale des langues africainesCes dernières années, la recherche sur les méthodes empiriques de traduction automatique a connu un regain d'intérêt.La plupart de ces recherches se sont concentrées sur les langues européennes bien dotées.Bien qu'environ 30 % de toutes les langues parlées dans le monde soient africaines, ces dernières ont été très peu étudiées, en partie à cause du manque de corpus parallèles libres d'accès mis en ligne.En outre, malgré leur grand nombre (plus de 2 000) et leurs similitudes relatives, il n'existe actuellement aucune étude publiée sur la manière de tirer parti de ce multilinguisme (et des similitudes qui lui sont associées) pour améliorer les performances des systèmes de traduction automatique pour les langues africaines.Afin de répondre à ces enjeux, nous proposons un nouveau jeu de données (provenant d'une source qui nous autorise à l'utiliser et à le diffuser) en langues africaines offrant des données parallèles pour des langues vernaculaires absentes des jeux de données couramment utilisés tels que JW300.Pour exploiter le multilinguisme, nous nous appuyons d'abord sur une approche historique basée sur les migrations des populations pour identifier les langues vernaculaires similaires.Nous proposons également une nouvelle mesure pour évaluer automatiquement les similitudes entre les langues.Cette nouvelle métrique ne nécessite pas de parallélisme au niveau des mots comme les méthodes traditionnelles, mais seulement au niveau des paragraphes.Nous démontrons ensuite que la modélisation du langage masqué et la modélisation du langage de traduction, en plus de l'apprentissage multitâche sur un groupe de langues similaires, permettent d'améliorer considérablement les performances de traduction pour les paires de langues du groupe considéré.En particulier, nous enregistrons une amélioration de 29 BLEU sur la paire Bafia-Ewondo grâce à nos approches par rapport aux méthodes de travail antérieures qui n'avaient pas su tirer parti du multilinguisme.Enfin, nous publions l'ensemble des données et le code informatique produits lors de cette étude afin d'en garantir la reproductibilité et d'accélérer la recherche dans ce domaine.\"},\n",
       "             163: {'src': 'Priming Neural Machine TranslationPriming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response.In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT).We evaluate the effect of using similar translations as priming cues on the NMT network.We propose a method to inject priming cues into the NMT networkand compare our framework to other mechanisms that perform micro-adaptation during inference.Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.',\n",
       "              'mt': \"Apprentissage de la traduction automatique neuronaleL'amorçage est un phénomène psychologique bien connu et étudié, basé sur la présentation préalable d'un stimulus (cue) pour influencer le traitement d'une réponse.Dans cet article, nous proposons un cadre pour imiter le processus d'amorçage dans le contexte de la traduction automatique neuronale (NMT).Nous évaluons l'effet de l'utilisation de traductions similaires comme indices d'amorçage sur le réseau NMT.Nous proposons une méthode pour injecter des indices d'amorçage dans le réseau de traduction automatique neuronale et comparons notre cadre à d'autres mécanismes qui effectuent une micro-adaptation pendant l'inférence.Dans l'ensemble, les expériences menées dans un contexte multi-domaine confirment que l'ajout d'indices d'amorçage dans le décodeur NMT peut grandement contribuer à améliorer la précision de la traduction.\",\n",
       "              'ref': \"Amorçage de la traduction automatique neuronaleL'amorçage est un phénomène psychologique bien connu et étudié, basé sur la présentation préalable d'un stimulus (signal) pour influencer la production d'une réponse.Dans cet article, nous proposons un cadre pour imiter le processus d'amorçage dans le contexte de la traduction automatique neuronale (TAN).Nous évaluons l'effet de l'utilisation de traductions similaires comme indices d'amorçage sur le réseau de TA neuronale.Nous proposons une méthode pour injecter des signaux d'amorçage dans le réseau de TA neuronale et comparons notre approche à d'autres mécanismes qui réalisent une micro-adaptation pendant l'inférence.Dans l'ensemble, les expériences menées dans un contexte multi-domaine confirment que l'ajout d'indices d'amorçage dans le décodeur TAN peut grandement contribuer à améliorer l'exactitude de la traduction. Nous démontrons par ailleurs que notre approche permet de recueillir, au départ de ressources monolingues, de l'information utile aux systèmes de TA neuronaux.\"},\n",
       "             151: {'src': 'Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of languageindependent representations, or whether a multilingual model partitions its weights among different languages.While most of such work has been conducted in a \"black-box\" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model.In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as \"variance\" or \"confidence\", and (2) systematically ranking the importance of attention heads with respect to translation quality.Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.',\n",
       "              'mt': 'Les modèles de traduction automatique neuronale multilingue contiennent-ils des têtes d’attention spécifiques aux paires de langues?Des études récentes sur l’analyse des représentations multilingues visent à déterminer s’il y a une émergence de représentations indépendantes de la langue, ou si un modèle multilingue répartit ses poids entre différentes langues.Bien que la plupart de ces travaux aient été menés de manière «boîte noire», cet article vise à analyser les composants individuels d’un modèle de traduction neuronale multilingue (NMT).En particulier, nous examinons l’attention personnelle du codeur et les têtes d’attention de codeur-décodeur (dans un modèle NMT multiple-to-one) qui sont plus spécifiques à la traduction d’une certaine paire de langues que d’autres en utilisant (1) des mesures qui quantifient certains aspects des poids de l’attention tels que «variance» ou «confiance», et (2) classer systématiquement l’importance des têtes d’attention par rapport à la qualité de la traduction.Les résultats expérimentaux montrent qu’étonnamment, l’ensemble des têtes d’attention les plus importantes sont très similaires dans les paires de langues et qu’il est possible d’enlever près d’un tiers des têtes les moins importantes sans nuire grandement à la qualité de la traduction.',\n",
       "              'ref': \"Les modèles de traduction automatique neuronale multilingues contiennent-ils des têtes d’attention spécifiques aux paires de langues?Des études récentes portant sur l’analyse des représentations multilingues ont visé à déterminer si des représentations indépendantes de la langue en émergent, ou bien si un modèle multilingue compartimente ses poids entre les différentes langues.Alors que la plupart de ces travaux ont été conduits selon l'approche de la «boîte noire», cet article vise à analyser les composants individuels d’un modèle de traduction automatique neuronale multilingue (TAN).En particulier, nous examinons l’auto-attention de l'encodeur et les têtes d’attention de l'interface encodeur-décodeur (dans un modèle TAN &quot;many-to-one&quot;) qui seraient plus spécifiques à la traduction d’une paire de langues déterminée en utilisant (1) des mesures qui quantifient certains aspects des poids de l’attention, telles la «variance» ou la «confiance», et (2) en classant systématiquement les têtes d’attention par ordre d'importance sous le rapport de la qualité des traductions.Les résultats expérimentaux démontrent que, paradoxalement, l’ensemble des têtes d’attention les plus importantes sont très similaires d'une paire de langues à l'autre et qu’il est possible d'exclure près d’un tiers des têtes les moins importantes sans nuire grandement à la qualité de la traduction.\"},\n",
       "             161: {'src': 'Screening Gender Transfer in Neural Machine TranslationThis paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English.Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system.Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.',\n",
       "              'mt': \"Screening du transfert de genre dans la traduction automatique neuronaleCet article vise à identifier le flux d'informations dans les systèmes de traduction automatique de pointe, en prenant comme exemple le transfert du genre lors de la traduction du français vers l'anglais.En utilisant un ensemble contrôlé d'exemples, nous expérimentons plusieurs façons d'étudier comment l'information sur le genre circule dans une architecture codeur-décodeur en considérant à la fois des techniques de sondage et des interventions sur les représentations internes utilisées dans le système de traduction automatique.Nos résultats montrent que l'information sur le genre peut être trouvée dans toutes les représentations de jetons construites par le codeur et le décodeur et nous amènent à conclure qu'il existe de multiples voies pour le transfert du genre.\",\n",
       "              'ref': \"Passage au crible du transfert de genre en traduction automatique neuronaleCet article vise à identifier le flux d'informations dans les systèmes de traduction automatique de pointe, en prenant comme exemple le transfert du genre lors de la traduction du français vers l'anglais.En utilisant un ensemble contrôlé d'exemples, nous expérimentons plusieurs façons d'étudier comment l'information sur le genre circule dans une architecture encodeur-décodeur en considérant à la fois des techniques de sondage et des interventions sur les représentations internes utilisées dans le système de traduction automatique.Nos résultats montrent que l'information sur le genre peut être trouvée dans toutes les représentations de tokens construites par l'encodeur et le décodeur et nous amènent à conclure qu'il existe de multiples voies pour le transfert du genre.\"},\n",
       "             501: {'src': \"Machine Translation, it’s a question of style, innit? The case of English tag questionsIn this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, isn't it?) in Machine Translation (MT).We propose a post-edition solution, formulating the problem as a multi-class classification task.We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of classifiers to predict TQ forms, which we use to post-edit state-of-the-art MT outputs.Our method provides significant improvements in English TQ translation when translating from Czech, French and German, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.\",\n",
       "              'mt': \"Traduction automatique, c'est une question de style, n'est-ce pas ? Le cas des questions de tag en anglaisDans cet article, nous abordons le problème de la génération de questions à étiquette (TQ) en anglais (par exemple, it is, isn't it ?) dans le cadre de la traduction automatique (TA).Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes.Nous présentons (i) l'annotation automatique des QT anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire les formes de QT, que nous utilisons pour post-éditer les résultats de traduction automatique les plus récents.Notre méthode apporte des améliorations significatives à la traduction des QT en anglais à partir du tchèque, du français et de l'allemand, améliorant ainsi la fluidité, le naturel, l'exactitude grammaticale et la cohérence pragmatique des résultats de la traduction assistée par ordinateur.\",\n",
       "              'ref': \"Traduction automatique, c'est une question de style, n'est-ce pas ? Le cas des questions à étiquette en anglaisDans cet article, nous abordons le problème de la génération de questions à étiquette (QT) en anglais (par exemple, it is, isn't it ?) dans le cadre de la traduction automatique (TA).Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes.Nous présentons (i) l'annotation automatique des QT anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire les formes de QT, que nous utilisons pour post-éditer les résultats de traduction automatique les plus récents.Notre méthode apporte des améliorations significatives à la traduction des QT en anglais à partir du tchèque, du français et de l'allemand, améliorant ainsi la fluidité, le naturel, l'exactitude grammaticale et la cohérence pragmatique des résultats de la traduction automatique.\"},\n",
       "             526: {'src': 'Automating Document Discovery in the Systematic Review Process: How to Use Chaff to Extract Wheat.Systematic reviews in e.g. empirical medicine address research questions by comprehensively examining the entire published literature.Conventionally, manual literature surveys decide inclusion in two steps, first based on abstracts and title, then by full text, yet currentmethods to automate the process make no distinction between gold data from these two stages.In this work we compare the impact different schemes for choosing positive and negative examples from the different screening stages have on the training of automated systems.We train a ranker using logistic regression and evaluate it on a new gold standard dataset for clinical NLP, and on an existing gold standard dataset for drug class efficacy.The classification and ranking achieves an average AUC of 0.803 and 0.768 when relying on gold standard decisions based on title and abstracts of articles, and an AUC of 0.625 and 0.839 when relying on gold standard decisions based on full text.Our results suggest that it makes little difference which screening stage the gold standard decisions are drawn from, and that the decisions need not be based on the full text.The results further suggest that common-off-the-shelf algorithms can reduce the amount of work required to retrieve relevant literature.',\n",
       "              'mt': \"Automatisation de la découverte de documents dans le processus de revue systématique : Comment utiliser l'ivraie pour extraire le blé.Les revues systématiques, par exemple en médecine empirique, abordent des questions de recherche en examinant de manière exhaustive l'ensemble de la littérature publiée.Traditionnellement, les études bibliographiques manuelles décident de l'inclusion en deux étapes, d'abord sur la base des résumés et du titre, puis sur la base du texte intégral, mais les méthodes actuelles d'automatisation du processus ne font pas de distinction entre les données de référence de ces deux étapes.Dans ce travail, nous comparons l'impact des différents schémas de sélection des exemples positifs et négatifs des différentes étapes de sélection sur l'apprentissage des systèmes automatisés.Nous formons un classificateur à l'aide de la régression logistique et l'évaluons sur un nouvel ensemble de données de référence pour le NLP clinique et sur un ensemble de données de référence existant pour l'efficacité des classes de médicaments.La classification et le classement atteignent une AUC moyenne de 0,803 et 0,768 lorsqu'ils s'appuient sur les décisions de l'étalon-or basées sur le titre et les résumés des articles, et une AUC de 0,625 et 0,839 lorsqu'ils s'appuient sur les décisions de l'étalon-or basées sur le texte intégral.Nos résultats suggèrent qu'il n'y a guère de différence entre l'étape de dépistage à partir de laquelle les décisions de référence sont prises, et qu'il n'est pas nécessaire que les décisions soient basées sur le texte intégral.Les résultats suggèrent également que les algorithmes courants peuvent réduire la quantité de travail nécessaire à la recherche de la littérature pertinente.\",\n",
       "              'ref': \"Automatisation de la découverte de documents dans le processus de revue systématique : Comment utiliser l'ivraie pour extraire le blé.Les revues systématiques, par exemple en médecine empirique, abordent des questions de recherche en examinant de manière exhaustive l'ensemble de la littérature publiée.Traditionnellement, les études bibliographiques manuelles décident de l'inclusion en deux étapes, d'abord sur la base des résumés et du titre, puis sur la base du texte intégral, mais les méthodes actuelles d'automatisation du processus ne font pas de distinction entre les données de référence de ces deux étapes.Dans ce travail, nous comparons l'impact des différents schémas de sélection des exemples positifs et négatifs des différentes étapes de sélection sur l'apprentissage des systèmes automatisés.Nous entraînons un classificateur à l'aide de la régression logistique et l'évaluons sur un nouvel ensemble de données de référence pour le TAL clinique et sur un ensemble de données de référence existant pour l'efficacité des classes de médicaments.La classification et le classement atteignent une AUC moyenne de 0,803 et 0,768 lorsqu'ils s'appuient sur les décisions de l'étalon-or basées sur le titre et les résumés des articles, et une AUC de 0,625 et 0,839 lorsqu'ils s'appuient sur les décisions de l'étalon-or basées sur le texte intégral.Nos résultats suggèrent qu'il n'y a guère de différence entre l'étape de dépistage à partir de laquelle les décisions de référence sont prises, et qu'il n'est pas nécessaire que les décisions soient basées sur le texte intégral.Les résultats suggèrent également que les algorithmes courants peuvent réduire la quantité de travail nécessaire à la recherche de la littérature pertinente.\"},\n",
       "             304: {'src': 'Automating Document Discovery in the Systematic Review Process: How to Use Chaff to Extract Wheat.Systematic reviews in e.g. empirical medicine address research questions by comprehensively examining the entire published literature.Conventionally, manual literature surveys decide inclusion in two steps, first based on abstracts and title, then by full text, yet currentmethods to automate the process make no distinction between gold data from these two stages.In this work we compare the impact different schemes for choosing positive and negative examples from the different screening stages have on the training of automated systems.We train a ranker using logistic regression and evaluate it on a new gold standard dataset for clinical NLP, and on an existing gold standard dataset for drug class efficacy.The classification and ranking achieves an average AUC of 0.803 and 0.768 when relying on gold standard decisions based on title and abstracts of articles, and an AUC of 0.625 and 0.839 when relying on gold standard decisions based on full text.Our results suggest that it makes little difference which screening stage the gold standard decisions are drawn from, and that the decisions need not be based on the full text.The results further suggest that common-off-the-shelf algorithms can reduce the amount of work required to retrieve relevant literature.',\n",
       "              'mt': \"Automatisation de la découverte de documents dans le processus de révision systématique : Comment utiliser la paillette pour extraire le blé.Les revues systématiques, par exemple en médecine empirique, abordent les questions de recherche en examinant de façon exhaustive l'ensemble de la littérature publiée.Habituellement, les études documentaires manuelles décident de l'inclusion en deux étapes, d'abord en fonction des résumés et du titre, puis par texte intégral, mais les méthodes actuelles pour automatiser le processus ne font aucune distinction entre les données d'or de ces deux étapes.Dans ce travail, nous comparons l'impact de différents schémas pour choisir des exemples positifs et négatifs à partir des différentes étapes de dépistage ont sur la formation des systèmes automatisés.Nous formons un évaluateur à l'aide d'une régression logistique et nous l'évaluons en fonction d'un nouvel ensemble de données étalon or pour les PNL cliniques et d'un ensemble de données étalon or existant pour l'efficacité de la classe de médicaments.La classification et le classement atteignent une ASC moyenne de 0,803 et 0,768 en s'appuyant sur des décisions sur l'étalon-or basées sur le titre et des résumés d'articles, et une ASC de 0,625 et 0,839 en s'appuyant sur des décisions sur l'étalon-or basées sur le texte intégral.Nos résultats suggèrent qu'il fait peu de différence à quel stade de l'examen préliminaire les décisions de référence sont tirées, et que les décisions ne doivent pas être fondées sur le texte intégral.Les résultats suggèrent en outre que les algorithmes courants peuvent réduire la quantité de travail nécessaire pour récupérer la littérature pertinente.\",\n",
       "              'ref': \"Automatisation de la découverte de documents dans le processus d'élaboration des revues systématiques : Comment utiliser l'ivraie pour extraire le bon grain.Les revues systématiques, par exemple en médecine fondée sur les preuves, abordent les questions de recherche en examinant de façon exhaustive l'ensemble de la littérature publiée.Le processus classique de sélection des articles à inclure dans une revue systématique comporte deux étapes et repose sur un examen successif des titres et résumés, puis puis du texte intégral des articles sélectionnés à la première étape. Cependant, les méthodes automatiques ne font actuellement aucune distinction entre les évaluations de référence issues de ces deux étapes.Dans ce travail, nous comparons l'impact de différents schémas pour choisir des exemples positifs et négatifs à partir des différentes étapes de sélection sur l'entrainement des systèmes automatisés.Nous entrainons un classifieur à l'aide d'une régression logistique et nous l'évaluons sur un corpus de test en TAL clinique et sur un corpus existant dans le domaine de l'efficacité des médicaments.La classification et le classement atteignent une AUC moyenne de 0,803 et 0,768 en s'appuyant sur les références issues de la sélection à partir du titre et des résumés d'articles, et une AUC de 0,625 et 0,839 en s'appuyant sur les références issues de la sélection à partir du texte intégral.Nos résultats montrent peu de différence dans les modèles en fonction du niveau de sélection des références utilisées par le classifieur, en particulier, l'utilisation du texte intégral pour le classifieur ne semble pas indispensable.Les résultats suggèrent en outre que les algorithmes actuels peuvent réduire la quantité de travail nécessaire pour sélectionner la littérature pertinente.\"},\n",
       "             527: {'src': 'Evaluating the carbon footprint of NLP methods: a survey and analysis of existing toolsModern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications.Due to the significant environmental impact of deep learning, costbenefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment.In this paper, we review the tools that are available to measure energy use and CO 2 emissions of NLP methods.We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational setups (local server vs. computing facility).Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.',\n",
       "              'mt': \"Évaluation de l'empreinte carbone des méthodes PNL : une étude et une analyse des outils existantsModern Natural Language Processing (NLP) fait un usage intensif des méthodes d'apprentissage profond en raison de la précision qu'elles offrent pour une variété d'applications.En raison de l'impact environnemental significatif de l'apprentissage profond, une analyse coûts/bénéfices incluant l'empreinte carbone ainsi que des mesures de précision ont été suggérées pour mieux documenter l'utilisation des méthodes PNL pour la recherche ou le déploiement.Dans ce document, nous examinons les outils disponibles pour mesurer la consommation d'énergie et les émissions de CO 2 des méthodes de PNL.Nous décrivons la portée des mesures fournies et comparons l'utilisation de six outils (traceur carbone, traceur d'impact d'expérience, algorithmes verts, impact CO2 ML, consommation d'énergie et cumulateur) sur des expériences de reconnaissance d'entités nommées effectuées sur différentes configurations informatiques (serveur local vs installation informatique).Sur la base de ces constatations, nous proposons des recommandations exploitables pour mesurer avec précision l'impact environnemental des expériences de PNL.\",\n",
       "              'ref': \"Évaluation de l'empreinte carbone des méthodes TAL : une étude et analyse des outils existantsLe traitement automatique du langage (TAL) moderne fait un usage intensif des méthodes d'apprentissage profond en raison de la précision qu'elles offrent pour une variété d'applications.En raison de l'impact environnemental significatif de l'apprentissage profond, une analyse coûts/bénéfices incluant l'empreinte carbone ainsi que des mesures de précision a été suggérée pour mieux documenter l'utilisation des méthodes TAL pour la recherche ou le déploiement.Dans ce document, nous examinons les outils disponibles pour mesurer la consommation d'énergie et les émissions de CO2 des méthodes de PNL.Nous décrivons la portée des mesures fournies et comparons l'utilisation de six outils (traceur carbone, traceur d'impact d'expérience, algorithmes verts, impact CO2 ML, consommation d'énergie et cumulateur) sur des expériences de reconnaissance d'entités nommées effectuées sur différentes configurations informatiques (serveur local vs installation informatique).Sur la base de ces constatations, nous proposons des recommandations exploitables pour mesurer avec précision l'impact environnemental des expériences de TAL.\"},\n",
       "             168: {'src': 'The ETAPE Speech Processing EvaluationThe ETAPE evaluation is the third evaluation in automatic speech recognition and associated technologies in a series which started with ESTER.This evaluation proposed some new challenges, by proposing TV and radio shows with prepared and spontaneous speech, annotation and evaluation of overlapping speech, a cross-show condition in speaker diarization, and new, complex but very informative named entities in the information extraction task.This paper presents the whole campaign, including the data annotated, the metrics used and the anonymized system results.All the data created in the evaluation, hopefully including system outputs, will be distributed through the ELRA catalogue in the future.',\n",
       "              'mt': \"L'évaluation du traitement de la parole ETAPEL'évaluation ETAPE est la troisième évaluation dans le domaine de la reconnaissance automatique de la parole et des technologies associées dans une série qui a commencé avec ESTER.Cette évaluation a proposé de nouveaux défis, en proposant des émissions de télévision et de radio avec de la parole préparée et spontanée, l'annotation et l'évaluation de la parole qui se chevauche, une condition d'émission croisée dans la diarisation du locuteur, et de nouvelles entités nommées complexes mais très informatives dans la tâche d'extraction d'informations.Cet article présente l'ensemble de la campagne, y compris les données annotées, les mesures utilisées et les résultats anonymes du système.Toutes les données créées dans le cadre de l'évaluation, y compris, nous l'espérons, les résultats du système, seront distribuées à l'avenir par l'intermédiaire du catalogue ELRA.\",\n",
       "              'ref': \"L'évaluation du traitement de la parole ETAPEL'évaluation ETAPE est la troisième dans une série d'évaluations qui a commencé avec ESTER, dans le domaine de la reconnaissance automatique de la parole et des technologies associées.Cette évaluation a soulevé de nouveaux défis, en proposant des émissions de télévision et de radio avec de la parole préparée et spontanée, l'annotation et l'évaluation de la parole superposée, la diarisation du locuteur à travers des types différents d'émission, et de nouvelles entités nommées complexes mais très informatives dans la tâche d'extraction d'informations.Cet article présente l'ensemble de la campagne, y compris les données annotées, les mesures utilisées et les résultats anonymes du système.Toutes les données créées dans le cadre de l'évaluation, y compris, nous l'espérons, les résultats du système, seront distribuées à l'avenir par l'intermédiaire du catalogue ELRA.\"},\n",
       "             305: {'src': 'Evaluating the carbon footprint of NLP methods: a survey and analysis of existing toolsModern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications.Due to the significant environmental impact of deep learning, costbenefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment.In this paper, we review the tools that are available to measure energy use and CO 2 emissions of NLP methods.We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational setups (local server vs. computing facility).Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.',\n",
       "              'mt': \"Évaluation de l'empreinte carbone des méthodes de TAL : une enquête et une analyse des outils existantsLe traitement moderne du langage naturel (NLP) fait un usage intensif des méthodes d'apprentissage profond en raison de la précision qu'elles offrent pour une variété d'applications.En raison de l'impact environnemental significatif de l'apprentissage profond, une analyse coût-bénéfice incluant l'empreinte carbone ainsi que des mesures de précision a été suggérée pour mieux documenter l'utilisation des méthodes de TAL à des fins de recherche ou de déploiement.Dans cet article, nous passons en revue les outils disponibles pour mesurer la consommation d'énergie et les émissions de CO 2 des méthodes de NLP.Nous décrivons la portée des mesures fournies et comparons l'utilisation de six outils (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) sur des expériences de reconnaissance d'entités nommées réalisées sur différentes configurations informatiques (serveur local vs. centre de calcul).Sur la base de ces résultats, nous proposons des recommandations concrètes pour mesurer avec précision l'impact environnemental des expériences de NLP.\",\n",
       "              'ref': \"Évaluation de l'empreinte carbone des méthodes de TAL : revue et analyse des outils existantsLe traitement automatique des langues (TAL) moderne fait un usage intensif des méthodes d'apprentissage profond en raison des performances qu'elles offrent pour une variété d'applications.L'ampleur de l'impact environnemental de l'apprentissage profond motive le besoin d'analyses coût-bénéfice incluant l'empreinte carbone en plus des métriques de performance afin de mieux documenter l'utilisation des méthodes de TAL dans un contexte de recherche ou de déploiement.Dans cet article, nous passons en revue les outils disponibles pour mesurer la consommation d'énergie et les émissions de CO 2 des méthodes de TAL.Nous décrivons les mesures fournies et comparons l'utilisation de six outils (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage et cumulator) sur des expériences de reconnaissance d'entités nommées réalisées sur différentes configurations informatiques (serveur local et centre de calcul).Nous nous appuyons sur ces résultats pour proposer des recommandations concrètes pour mesurer avec précision l'impact environnemental des expériences de TAL.\"},\n",
       "             152: {'src': 'Automatic Selection of Parallel Data for Machine TranslationNowadays machine translation is widely used, but the required data for training, tuning and testing a machine translation engine is often not sufficient or not useful.The automatic selection of data that are qualitatively appropriate for building translation models can help improve translation accuracy.In this paper, we used a large parallel corpus of educational video lecture subtitles as well as text posted by students and lecturers on the course fora.The text is quite challenging to translate due to the scientific domains involved and its informal genre.We applied a random forest classification schema on the output of three machine translation models (one based on statistical machine translation and two on neural machine translation) in order to automatically identify the best output.The unorthodox language phenomena observed as well as the rich-in-terminology scientific domains addressed in the educational video lectures, the language-independent nature of the approach, and the tackled three-class classification problem constitute innovative challenges of the work described herein.',\n",
       "              'mt': \"Sélection automatique de données parallèles pour la traduction automatiqueDe nos jours, la traduction automatique est largement utilisée, mais les données requises pour la formation, le réglage et l'essai d'un moteur de traduction automatique sont souvent insuffisantes ou non utiles.La sélection automatique de données qualitativement appropriées pour construire des modèles de traduction peut aider à améliorer la précision de la traduction.Dans cet article, nous avons utilisé un large corpus parallèle de sous-titres de conférences vidéo éducatives ainsi que du texte publié par les étudiants et les conférenciers sur les forums de cours.Le texte est assez difficile à traduire en raison des domaines scientifiques impliqués et de son genre informel.Nous avons appliqué un schéma de classification de forêt aléatoire sur la sortie de trois modèles de traduction automatique (un basé sur la traduction automatique statistique et deux sur la traduction automatique neuronale) afin d'identifier automatiquement la meilleure sortie.Les phénomènes langagiers non orthodoxes observés, ainsi que les domaines scientifiques riches en terminologie abordés dans les conférences vidéo éducatives, la nature de l'approche indépendante de la langue et le problème de classification à trois classes abordé constituent des défis innovants de l'oeuvre décrite ici.\",\n",
       "              'ref': \"Sélection automatique de données parallèles pour la traduction automatiqueDe nos jours, la traduction automatique est largement utilisée, mais les données requises pour l'entraînement, l'adaptation et l'évaluation d'un moteur de traduction automatique sont souvent insuffisantes ou de piètre qualité lorsqu'elles existent.La sélection automatique de données qualitativement adéquates pour l'entraînement de tels modèles peut aider à améliorer l'exactitude des traductions.Dans cet article, nous avons utilisé un vaste corpus parallèle de sous-titres de conférences éducatives filmées, ainsi que du texte posté par les étudiants et les conférenciers sur les forums de cours.Le texte est assez difficile à traduire en raison des domaines scientifiques mis en jeu et de son caractère informel.Nous avons appliqué un schéma de classification de forêt aléatoire sur les données de sortie de trois modèles de traduction automatique (l'un basé sur la traduction automatique statistique et les deux autres sur la traduction automatique neuronale) afin d'identifier automatiquement la meilleure sortie.Les phénomènes langagiers singuliers observés, ainsi que les domaines scientifiques riches en terminologie abordés dans les conférences éducatives filmées, la nature de notre approche, indépendante de la langue, et le problème de classification en trois catégories abordé constituent les défis innovants de la présente étude.\"},\n",
       "             117: {'src': 'The MultiTal NLP tool infrastructureThis paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions.The goal is to make NLP tools more accessible and usable to end-users of different disciplines.The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation.The model is conceptualised using an OWL ontology.The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.',\n",
       "              'mt': 'L’infrastructure multitale de l’outil PNLCet article donne un aperçu du projet multital, qui vise à créer une infrastructure de recherche qui assure la distribution à long terme des descriptions des outils de PNL.L’objectif est de rendre les outils de PNL plus accessibles et utilisables pour les utilisateurs finaux de différentes disciplines.L’infrastructure est construite sur un schéma de méta-données modélisant et standardisant la documentation multilingue des outils de PNL.Le modèle est conceptualisé à l’aide d’une ontologie OWL.La représentation formelle de l’ontologie nous permet de générer automatiquement une documentation organisée et structurée dans différentes langues pour chaque outil représenté.',\n",
       "              'ref': \"L’infrastructure d’outils de TAL :  MultiTalCet article donne un aperçu du projet MultiTal, qui vise à créer une infrastructure de recherche qui assure la distribution à long terme des descriptions d'outils de TAL.L’objectif est de rendre les outils de TAL plus accessibles et utilisables pour les utilisateurs finaux de différentes disciplines.L’infrastructure est construite sur un schéma de méta-données, modélisant et standardisant la documentation multilingue des outils de TAL.Le modèle est conceptualisé à l’aide d’une ontologie OWL.La représentation formelle de l’ontologie, permet de générer automatiquement une documentation organisée et structurée dans différentes langues pour chaque outil représenté.\"},\n",
       "             340: {'src': 'LIMSI-COT at SemEval-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical NarrativesIn this paper we present our participation to SemEval 2017 Task 12.We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies.We achieved competitive performance for both tasks',\n",
       "              'mt': \"LIMSI-COT à SemEval-2017 Tâche 12 : Architecture neuronale pour l'extraction d'informations temporelles à partir de récits cliniques.Dans cet article, nous présentons notre participation à la tâche 12 de SemEval 2017.Nous avons utilisé une approche basée sur les réseaux de neurones pour l'extraction d'entités et de relations temporelles, et nous avons expérimenté deux stratégies d'adaptation au domaine.Nous avons obtenu des performances compétitives pour les deux tâches\",\n",
       "              'ref': \"LIMSI-COT à SemEval-2017 Tâche 12 : Architecture neuronale pour l'extraction d'informations temporelles à partir de documents cliniques.Dans cet article, nous présentons notre participation à la tâche 12 de SemEval 2017.Nous avons utilisé une approche basée sur les réseaux de neurones pour l'extraction d'entités et de relations temporelles, et nous avons expérimenté deux stratégies d'adaptation en domaine.Nous avons obtenu des performances compétitives pour les deux tâches.\"},\n",
       "             499: {'src': 'Amazon Mechanical Turk: Gold Mine or Coal Mine ?Recently heard at a tutorial in our field: “It cost me less than one hundred bucks to annotate this using Amazon Mechanical Turk!”Assertions like this are increasingly common, but we believe they should not be stated so proudly;they ignore the ethical consequences of using MTurk (Amazon Mechanical Turk) as a source of labour.Manually annotating corpora or manually developing any other linguistic resource, such as a set of judgments about system outputs, represents such a high cost that many researchers are looking for alternative solutions to the standard approach.MTurk is becoming a popular one.However, as in any scientific endeavor involving humans, there is an unspoken ethical dimension involved in resource construction and system evaluation, and this is especially true of MTurk.We would like here to raise some questions about the use of MTurk.To do so, we will define precisely what MTurk is and what it is not, highlighting the issues raised by the system.We hope that this will point out opportunities for our community to deliberately value ethics above cost savings.',\n",
       "              'mt': \"Amazon Mechanical Turk : Mine d'or ou mine de charbon ?Récemment entendu à un tutoriel dans notre domaine : « Cela m'a coûté moins de cent dollars pour l'annoter avec Amazon Mechanical Turk ! »Des affirmations comme celle-ci sont de plus en plus fréquentes, mais nous pensons qu'elles ne devraient pas être énoncées aussi fièrement.ils ignorent les conséquences éthiques de l'utilisation de MTurk (Amazon Mechanical Turk) comme source de travail.L'annotation manuelle des corpus ou le développement manuel de toute autre ressource linguistique, comme un ensemble de jugements sur les extrants du système, représentent un coût si élevé que de nombreux chercheurs cherchent des solutions de rechange à l'approche standard.MTurk est en train de devenir populaire.Cependant, comme dans toute entreprise scientifique impliquant des humains, il y a une dimension éthique tacite impliquée dans la construction des ressources et l'évaluation des systèmes, et c'est particulièrement vrai pour MTurk.Nous voudrions ici soulever quelques questions concernant l'utilisation de MTurk.Pour ce faire, nous définirons précisément ce qu'est MTurk et ce qu'il n'est pas, en mettant en évidence les questions soulevées par le système.Nous espérons que cela permettra à notre communauté de valoriser délibérément l'éthique au-delà des économies.\",\n",
       "              'ref': \"Amazon Mechanical Turk : mine d'or ou mine de charbon ?Récemment entendu dans un tutoriel dans notre domaine : « Cela m'a coûté moins de cent dollars pour l'annoter avec Amazon Mechanical Turk ! »Des affirmations comme celle-ci sont de plus en plus fréquentes, mais nous pensons qu'elles ne devraient pas être énoncées aussi fièrement.Ils ignorent les conséquences éthiques de l'utilisation de MTurk (Amazon Mechanical Turk) comme source de travail.L'annotation manuelle des corpus ou le développement manuel de toute autre ressource linguistique, comme un ensemble de jugements sur les résultats du système, représentent un coût si élevé que de nombreux chercheurs cherchent des solutions de rechange par rapport à l'approche standard.MTurk est en train de devenir populaire.Cependant, comme dans toute entreprise scientifique impliquant des humains, il y a une dimension éthique tacite impliquée dans la construction des ressources et l'évaluation des systèmes, et c'est particulièrement vrai pour MTurk.Nous voudrions ici soulever quelques questions concernant l'utilisation de MTurk.Pour ce faire, nous définirons précisément ce qu'est MTurk et ce qu'il n'est pas, en mettant en évidence les questions soulevées par le système.Nous espérons que cela permettra à notre communauté de valoriser délibérément l'éthique au-delà des économies.\"},\n",
       "             299: {'src': 'Multi-Domain Adaptation in Neural Machine Translation with Dynamic Sampling StrategiesBuilding effective Neural Machine Translation models often implies accommodating diverse sets of heterogeneous data so as to optimize performance for the domain(s) of interest.Such multi-source / multi-domain adaptation problems are typically approached through instance selection or reweighting strategies, based on a static assessment of the relevance of training instances with respect to the task at hand.In this paper, we study dynamic data selection strategies that are able to automatically re-evaluate the usefulness of data samples in the course of training.Based on the results of multiple experiments, we show that our method offer a generic framework to automatically handle several real-world situations, from multi-source or unsupervised domain adaptation to multidomain learning.',\n",
       "              'mt': \"Adaptation multi-domaine en traduction automatique neuronale avec stratégies d'échantillonnage dynamiqueLa construction de modèles efficaces de traduction automatique neuronale implique souvent de prendre en compte divers ensembles de données hétérogènes afin d'optimiser les performances pour le ou les domaines d'intérêt.De tels problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection ou de repondération d'instances, basées sur une évaluation statique de la pertinence des instances d'apprentissage par rapport à la tâche en cours.Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l'utilité des échantillons de données au cours de la formation.Sur la base des résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l'adaptation de domaine multi-source ou non supervisée à l'apprentissage multidomaine.\",\n",
       "              'ref': \"Adaptation multi-domaine en traduction automatique neuronale avec stratégies d'échantillonnage dynamiqueLa construction de modèles efficaces de traduction automatique neuronale implique souvent de prendre en compte divers ensembles de données hétérogènes afin d'optimiser les performances pour le ou les domaines d'intérêt.De tels problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection ou de repondération d'instances, basées sur une évaluation statique de la pertinence des instances d'apprentissage par rapport à la tâche en cours.Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l'utilité des échantillons de données au cours de l'entrainement.Sur la base des résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l'adaptation de domaine multi-source ou non supervisée à l'apprentissage multidomaine.\"},\n",
       "             141: {'src': 'Word Representations in Factored Neural Machine TranslationTranslation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena, which is a challenge for neural machine translation architectures.To address this issue, the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words.Within this framework, we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors.We report experiments for translation from English into two morphologically rich languages, Czech and Latvian, and show the importance of explicitly modeling target morphology.',\n",
       "              'mt': \"Représentations des mots dans la traduction automatique neuronale factoriséeLa traduction dans une langue morphologiquement riche nécessite un large vocabulaire de sortie pour modéliser les différents phénomènes morphologiques, ce qui constitue un défi pour les architectures neuronales de traduction automatique.Pour résoudre ce problème, le présent article étudie l'impact de l'existence de deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d'informations morpho-syntaxiques dans les deux facteurs.Nous présentons des expériences de traduction de l'anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l'importance de la modélisation explicite de la morphologie de la cible.\",\n",
       "              'ref': \"Représentations des mots dans la traduction automatique neuronale factoriséeLa traduction dans une langue morphologiquement riche nécessite un large vocabulaire de sortie pour modéliser les différents phénomènes morphologiques, ce qui constitue un défi pour les architectures neuronales de traduction automatique.Pour résoudre ce problème, le présent article étudie l'impact de l'existence de deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d'informations morpho-syntaxiques dans les deux facteurs.Nous présentons des expériences de traduction de l'anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l'importance de la modélisation explicite de la morphologie de la cible.\"},\n",
       "             82: {'src': 'Novel elicitation and annotation schemes for sentential and sub-sentential alignments of bitextsResources for evaluating sentence-level and word-level alignment algorithms are unsatisfactory.Regarding sentence alignments, the existing data is too scarce, especially when it comes to difficult bitexts, containing instances of non-literal translations.Regarding word-level alignments, most available hand-aligned data provide a complete annotation at the level of words that is difficult to exploit, for lack of a clear semantics for alignment links.In this study, we propose new methodologies for collecting human judgements on alignment links, which have been used to annotate 4 new data sets, at the sentence and at the word level.These will be released online, with the hope that they will prove useful to evaluate alignment software and quality estimation tools for automatic alignment.',\n",
       "              'mt': \"Nouveaux schémas d'élicitation et d'annotation pour l'alignement des phrases et des sous-phrases de bitsLes ressources pour évaluer les algorithmes d'alignement au niveau de la phrase et au niveau des mots sont insatisfaisantes.En ce qui concerne les alignements de phrases, les données existantes sont trop rares, en particulier lorsqu'il s'agit de textes difficiles, contenant des exemples de traductions non littérales.En ce qui concerne les alignements au niveau des mots, la plupart des données disponibles alignées à la main fournissent une annotation complète au niveau des mots qui est difficile à exploiter, faute d'une sémantique claire pour les liens d'alignement.Dans cette étude, nous proposons de nouvelles méthodologies pour recueillir des jugements humains sur les liens d'alignement, qui ont été utilisés pour annoter 4 nouveaux ensembles de données, au niveau de la phrase et au niveau du mot.Ils seront publiés en ligne, dans l'espoir qu'ils s'avéreront utiles pour évaluer les logiciels d'alignement et les outils d'estimation de la qualité pour l'alignement automatique.\",\n",
       "              'ref': \"Nouveaux schémas d'élicitation et d'annotation pour l'alignement des phrases et des sous-phrases de bitsLes ressources qui existent pour évaluer les algorithmes d'alignement au niveau des phrases et au niveau des mots sont insatisfaisantes.En ce qui concerne les alignements de phrases, les données existantes sont trop rares, en particulier pour ce qui concerne des textes difficiles, contenant des exemples de traductions non littérales.Pour les alignements de mots, la plupart des données disponibles alignées à la main fournissent une annotation complète au niveau des mots qui est difficile à exploiter, faute de disposer d'une sémantique claire pour les liens d'alignement.Dans cette étude, nous proposons de nouvelles méthodes pour recueillir des jugements humains sur les liens d'alignement. Ces méthodes ont été utilisées pour annoter 4 nouveaux jeux de données, au niveau de la phrase et au niveau des mots.Ils seront publiés en ligne, en espérant qu'ils seront utiles pour évaluer les logiciels d'alignement et les outils d'estimation de la qualité de l'alignement automatique.\"},\n",
       "             143: {'src': 'LIUM Machine Translation Systems for WMT17 News Translation TaskThis paper describes LIUM submissions to WMT17 News Translation Task for English↔German, English↔Turkish, English→Czech and English→Latvian language pairs.We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.Competitive scores were obtained by en-sembling various systems and exploiting the availability of target monolingual corpora for back-translation.The impact of back-translation quantity and quality is also analyzed for English→Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.',\n",
       "              'mt': \"Systèmes de traduction automatique LIUM pour la tâche de traduction de nouvelles WMT17Cet article décrit les soumissions de LIUM à la tâche de traduction de nouvelles WMT17 pour les paires de langues anglais↔allemand, anglais↔turc, anglais→tchèque et anglais→latvien.Nous entraînons des systèmes de traduction automatique neuronale attentive basés sur les BPE avec et sans sorties factorisées en utilisant le cadre nmtpy à source ouverte.Des scores compétitifs ont été obtenus en assemblant différents systèmes et en exploitant la disponibilité de corpus monolingues cibles pour la rétro-traduction.L'impact de la quantité et de la qualité de la rétro-traduction est également analysé pour l'anglais→ turc, où notre soumission après la date limite a dépassé la meilleure entrée de +1,6 BLEU.\",\n",
       "              'ref': \"Systèmes de traduction automatique du LIUM pour la tâche de traduction d'articles de presse WMT17Cet article décrit les contributions du LIUM à la tâche de traduction d'articles de presse WMT17 pour les paires de langues anglais↔allemand, anglais↔turc, anglais→tchèque et anglais→letton.Nous avons entraîné des systèmes de traduction automatique neuronale attentifs s'appuyant sur une approche BPE de la tokénisation, avec et sans sorties factorisées, en utilisant la suite d'outils libre nmtpy.Des scores compétitifs ont été obtenus par l'assemblage de multiples systèmes et en mettant à profit l'existence de corpus monolingues cibles pour la rétro-traduction.L'impact de la quantité et de la qualité des données rétro-traduites est également analysé pour l'anglais→ turc, où notre contribution, soumise après la date limite, a dépassé le meilleur concurrent de +1,6 BLEU.\"},\n",
       "             372: {'src': \"Evaluating Subtitle Segmentation for End-to-end Generation SystemsSubtitles appear on screen as short pieces of text, segmented based on formal constraints (length) and syntactic/semantic criteria.Subtitle segmentation can be evaluated with sequence segmentation metrics against a human reference.However, standard segmentation metrics cannot be applied when systems generate outputs different than the reference, e.g. with end-to-end subtitling systems.In this paper, we study ways to conduct reference-based evaluations of segmentation accuracy irrespective of the textual content.We first conduct a systematic analysis of existing metrics for evaluating subtitle segmentation.We then introduce Sigma, a new Subtitle Segmentation Score derived from an approximate upper-bound of BLEU on segmentation boundaries, which allows us to disentangle the effect of good segmentation from text quality.To compare Sigma with existing metrics, we further propose a boundary projection method from imperfect hypotheses to the true reference.Results show that all metrics are able to reward high quality output but for similar outputs system ranking depends on each metric's sensitivity to error type.Our thorough analyses suggest Sigma is a promising segmentation candidate but its reliability over other segmentation metrics remains to be validated through correlations with human judgements.\",\n",
       "              'mt': 'Évaluation de la segmentation des sous-titres pour les systèmes de génération de bout en boutLes sous-titres apparaissent à l’écran sous forme de courts morceaux de texte, segmentés en fonction de contraintes formelles (longueur) et de critères syntaxiques/sémantiques.La segmentation des sous-titres peut être évaluée avec des métriques de segmentation de séquence par rapport à une référence humaine.Cependant, les métriques de segmentation standard ne peuvent pas être appliquées lorsque les systèmes génèrent des sorties différentes de la référence, par exemple avec des systèmes de sous-titrage de bout en bout.Dans cet article, nous étudions les moyens de mener des évaluations fondées sur des références de l’exactitude de la segmentation, quel que soit le contenu textuel.Nous effectuons d’abord une analyse systématique des métriques existantes pour évaluer la segmentation des sous-titres.Nous introduisons ensuite Sigma, un nouveau score de segmentation de sous-titres dérivé d’une limite supérieure approximative de BLEU sur les limites de segmentation, ce qui nous permet de démêler l’effet d’une bonne segmentation de la qualité du texte.Pour comparer Sigma avec les métriques existantes, nous proposons en outre une méthode de projection de limites allant d’hypothèses imparfaites à la vraie référence.Les résultats montrent que toutes les métriques sont en mesure de récompenser une production de haute qualité, mais pour des résultats similaires, le classement du système dépend de la sensibilité de chaque métrique au type d’erreur.Nos analyses approfondies suggèrent que Sigma est un candidat prometteur de segmentation, mais sa fiabilité par rapport à d’autres métriques de segmentation reste à valider par le biais de corrélations avec les jugements humains.',\n",
       "              'ref': \"Évaluation de la segmentation des sous-titres pour les systèmes de génération de bout en boutLes sous-titres apparaissent à l’écran sous forme de courts morceaux de texte, segmentés en fonction de contraintes formelles (p. ex. sur la longueur) et de critères syntaxiques/sémantiques.La segmentation des sous-titres peut être évaluée avec des métriques de segmentation de séquence par rapport à une référence humaine.Cependant, les métriques de segmentation génériques ne peuvent pas être appliquées lorsque les systèmes engendrent des sorties dont le texte diffère de celui de la référence, comme c'est le cas par exemple pour des systèmes de sous-titrage de bout en bout.Dans cet article, nous étudions les moyens de mener des évaluations de l’exactitude de la segmentation fondées sur des références, sans incidence du contenu textuel.Nous effectuons d’abord une analyse systématique des métriques existantes pour évaluer la segmentation des sous-titres.Nous introduisons ensuite Sigma, un nouveau score de segmentation de sous-titres dérivé d’une borne supérieure approximative de BLEU calculée sur les limites de segmentation, qui nous permet de dissocier l’effet d’une bonne segmentation de la qualité du texte.Pour comparer Sigma avec les métriques existantes, nous proposons en outre une méthode de projection de limites depuis les hypothèses imparfaites vers la vraie référence.Les résultats montrent que toutes les métriques sont en mesure de récompenser une production de haute qualité, mais que pour des sorties similaires, le classement relatif du système dépend de la sensibilité de chaque métrique au type d’erreur.Nos analyses approfondies suggèrent que Sigma est un candidat prometteur pour l'évaluation de la segmentation, mais sa fiabilité par rapport à d’autres métriques reste à valider par une mise en corrélation avec des jugements humains.\"},\n",
       "             343: {'src': 'GDPR Transparency Requirements and Data Privacy VocabulariesThis tutorial introduced participants to the transparency requirements of the General Data Protection Regulation (GDPR) [35].Therein, it was explored together with the attendees whether technical specifications can be valuable to support transparency in favour of a data subject whose personal information is being processed.In the context of the discussions, past and present international efforts were examined that focus on data privacy vocabularies and taxonomies as basis work to enable effective enforcement of data handling policies.One example of a current undertaking in this area is the W3C Data Privacy Vocabularies and Controls Community Group (DPVCG) which aims at developing a taxonomy of privacy terms aligned to the GDPR, which encompasses personal data categories, processing purposes, events of disclosures, consent, and processing operations.During the tutorial session, the potential of such efforts was discussed among the participants, allowing for conclusions about the need to re-align and update past research in this area to the General Data Protection Regulation.',\n",
       "              'mt': \"Exigences de transparence et Vocabulaires de confidentialité des données GDPRCe tutoriel a présenté aux participants les exigences de transparence du Règlement général sur la protection des données (RGPD) [35].Dans ce contexte, il a été examiné avec les participants si les spécifications techniques peuvent être utiles pour soutenir la transparence en faveur d'une personne concernée dont les informations personnelles sont traitées.Dans le contexte des discussions, on a examiné les efforts internationaux passés et présents qui mettent l'accent sur les vocabulaires et les taxonomies de confidentialité des données comme base de travail pour permettre l'application efficace des politiques de traitement des données.Un exemple d'une entreprise actuelle dans ce domaine est le W3C Data Privacy Vocabularies and Controls Community Group (DPVCG) qui vise à développer une taxonomie des termes de confidentialité alignés sur le GDPR, qui englobe les catégories de données personnelles, les finalités de traitement, les événements de divulgation, le consentement et les opérations de traitement.Au cours de la session didactique, le potentiel de ces efforts a été discuté entre les participants, permettant de tirer des conclusions sur la nécessité de réaligner et de mettre à jour les recherches passées dans ce domaine avec le règlement général sur la protection des données.\",\n",
       "              'ref': \"RGPD: Exigences de transparence et vocabulaire sur la confidentialité des donnéesCe tutoriel a présenté aux participants les exigences de transparence du Règlement général sur la protection des données (RGPD) [35].Dans ce contexte, les participants ont pu examiner l'utilité des spécifications techniques pour soutenir la transparence en faveur d'une personne dont les informations personnelles sont traitées.Dans le contexte des discussions, les efforts internationaux passés et présents qui mettent l'accent sur les vocabulaires et les taxonomies de confidentialité des données comme base de travail pour permettre l'application efficace des politiques de traitement des données ont été passés en revue.Un exemple d'initiative actuelle dans ce domaine est le W3C Data Privacy Vocabularies and Controls Community Group (DPVCG) qui a pour but de développer une taxonomie des termes de confidentialité conforme au RGPD, qui englobe les catégories de données personnelles, les finalités de traitement, les événements de divulgation, le consentement et les opérations de traitement.Au cours de la session didactique, le potentiel de ces efforts a été discuté entre les participants, permettant de tirer des conclusions sur la nécessité de réaligner et de mettre à jour les recherches passées dans ce domaine avec le règlement général sur la protection des données.\"},\n",
       "             62: {'src': 'Neural Baselines for Word AlignmentsWord alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation.In most areas of natural language processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word alignment models.In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models.We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts.We also analyze typical alignment errors of the baselines that our models overcome to illustrate the benefits --- and the limitations --- of these new models for morphologically rich languages.',\n",
       "              'mt': \"Lignes de base neuronales pour les alignements de motsLes alignements de mots identifient les correspondances traductionnelles entre les mots d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour effectuer une estimation de la qualité.Dans la plupart des domaines du traitement du langage naturel, les modèles de réseaux neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux pour l'alignement de mots non supervisé pour quatre paires de langues, en comparant plusieurs variantes de modèles neuronaux.Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalents discrets.Nous analysons également les erreurs d'alignement typiques des lignes de base que nos modèles surmontent pour illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\",\n",
       "              'ref': \"Modèles neuronaux de base pour l'alignement de motsLes alignements de mots identifient les correspondances traductionnelles entre mots au sein d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour estimer la qualité d'une traduction.Dans la plupart des domaines du traitement des langues, les modèles neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux d'alignement de mots non supervisés pour quatre paires de langues, en comparant plusieurs variantes de ces modèles.Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalentes discrètes.Nous analysons également les erreurs d'alignement typiques des modèles de base que les versions neuronales surmontent afin d'illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\"},\n",
       "             411: {'src': 'Building a bilingual dictionary from movie subtitles based on inter-lingual triggersThis paper focuses on two aspects of Machine Translation: parallel corpora and translation model.First, we present a method to automatically build parallel corpora from subtitle files.We use subtitle files gathered from the Internet.This leads to useful data for Subtitling Machine Translation.Our method is based on Dynamic Time Warping.We evaluated this alignment method by comparing it with a sample aligned by hand and we obtained a precision of alignment equal to $0.92$.Second, we use the notion of inter-lingual triggers in order to build from the subtitle parallel corpora multilingual dictionaries and translation tables for machine translation.Inter-lingual triggers allow to detect couple of source and target words from parallel corpora.The Mutual Information measure used to determine inter-lingual triggers allows to hypothesize that a word in the source language is a translation of another word in the target language.We evaluate the obtained dictionary by comparing it to two existing dictionaries.Then, we integrated the obtained translation tables into an entire translation decoding process supplied by Pharaoh.We compared the translation performance using our translation tables with the performance obtained by the Giza++ tool.The results showed that the system tuned for our tables improves the Bleu value by 2.2% compared to the ones obtained by Giza++.',\n",
       "              'mt': 'Création d’un dictionnaire bilingue à partir de sous-titres de films basés sur des déclencheurs interlinguesCet article se concentre sur deux aspects de la traduction automatique: corpus parallèles et modèle de traduction.Tout d’abord, nous présentons une méthode pour construire automatiquement des corpus parallèles à partir de fichiers de sous-titres.Nous utilisons des fichiers de sous-titres recueillis à partir d’Internet.Cela conduit à des données utiles pour le sous-titrage de la traduction automatique.Notre méthode est basée sur le Warpage Dynamique du Temps.Nous avons évalué cette méthode d’alignement en la comparant avec un échantillon aligné à la main et nous avons obtenu une précision d’alignement égale à 0,92 $.Deuxièmement, nous utilisons la notion de déclencheurs interlingues afin de construire à partir des dictionnaires multilingues et des tableaux de traduction multilingues des corpus parallèles de sous-titres pour la traduction automatique.Les déclencheurs interlingues permettent de détecter quelques mots sources et cibles provenant de corpus parallèles.La mesure de l’information mutuelle utilisée pour déterminer les déclencheurs interlingues permet de supposer qu’un mot dans la langue source est une traduction d’un autre mot dans la langue cible.Nous évaluons le dictionnaire obtenu en le comparant à deux dictionnaires existants.Ensuite, nous avons intégré les tables de traduction obtenues dans un processus complet de décodage de traduction fourni par Pharaon.Nous avons comparé les performances de traduction à l’aide de nos tableaux de traduction avec les performances obtenues par l’outil Giza++.Les résultats ont montré que le système réglé pour nos tables améliore la valeur Bleue de 2,2\\xa0% par rapport à celles obtenues par Giza++.',\n",
       "              'ref': \"Création d’un dictionnaire bilingue à partir de sous-titres de films fondé sur des déclencheurs interlinguesCet article se concentre sur deux aspects de la traduction automatique: corpus parallèles et modèle de traduction.Tout d’abord, nous présentons une méthode pour construire automatiquement des corpus parallèles à partir de fichiers de sous-titres.Nous utilisons des fichiers de sous-titres recueillis sur Internet.Cela conduit à des données utiles pour la traduction automatique de sous-titres.Notre méthode est basée sur la déformation temporelle dynamique.Nous avons évalué cette méthode d’alignement en la comparant avec un échantillon aligné à la main et avons obtenu une précision d’alignement égale à $0,92$.Deuxièmement, nous utilisons la notion de déclencheurs interlingues afin de construire des dictionnaires multilingues et des tables de traduction à partir des corpus parallèles de sous-titres.Les déclencheurs interlingues permettent de détecter des couples mots sources et cibles provenant des corpus parallèles.La mesure de l’information mutuelle utilisée pour déterminer les déclencheurs interlingues permet de supposer qu’un mot dans la langue source est une traduction d’un autre mot dans la langue cible.Nous évaluons le dictionnaire obtenu en le comparant à deux dictionnaires existants.Ensuite, nous avons intégré les tables de traduction obtenues dans un processus complet de décodage de traduction fourni par Pharaoh.Nous avons comparé les performances obtenues par l'usage de nos tables de traduction avec les performances obtenues par l’outil Giza++.Les résultats ont montré que le système ajusté pour nos tables améliore la valeur Bleu de 2,2\\xa0% par rapport à celles obtenues par Giza++.\"},\n",
       "             263: {'src': 'Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic ProgrammingWe treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision.Our approach relies on Gumbel perturbations and differentiable dynamic programming.Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable.We illustrate its effectiveness on sentiment analysis and natural language inference tasks.We also study its properties on a synthetic structure induction task.Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees.',\n",
       "              'mt': \"Apprentissage d'arbres latents avec perturbations stochastiques et programmation dynamique différentiableNous traitons les arbres de dépendance projectifs comme des variables latentes dans notre modèle probabiliste et les induisons de manière à ce qu'ils soient bénéfiques pour une tâche en aval, sans dépendre d'une supervision directe de l'arbre.Notre approche repose sur les perturbations de Gumbel et la programmation dynamique différentiable.Contrairement aux approches précédentes de l'apprentissage des arbres latents, nous échantillonnons stochastiquement les structures globales et notre analyseur syntaxique est entièrement différentiable.Nous illustrons son efficacité sur des tâches d'analyse de sentiments et d'inférence du langage naturel.Nous étudions également ses propriétés dans le cadre d'une tâche d'induction de structures synthétiques.Les études d'ablation soulignent l'importance de la stochasticité et de l'obligation pour les structures latentes d'être des arbres projectifs.\",\n",
       "              'ref': \"Apprentissage d'arbres latents avec perturbations aléatoires et programmation dynamique différentiableNous traitons les arbres de dépendance projectifs comme des variables latentes dans notre modèle probabiliste et les induisons de manière à ce qu'ils soient bénéfiques pour une tâche cible, sans dépendre d'une supervision directe de l'arbre.Notre approche repose sur les perturbations de Gumbel et la programmation dynamique différentiable.Contrairement aux approches précédentes pour l'apprentissage des arbres latents, nous échantillonnons aléatoirement des structures globales et notre analyseur syntaxique est entièrement différentiable.Nous illustrons son efficacité sur des tâches d'analyse en sentiments et d'inférence en langage naturel.Nous étudions également ses propriétés dans le cadre d'une tâche d'induction sur des données synthétiques.Les études d'ablation soulignent l'importance des perturbations et de l'importance pour les structures latentes d'être des arbres projectifs.\"},\n",
       "             60: {'src': \"Evaluating the morphological competence of Machine Translation SystemsWhile recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models.This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena.Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast.This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.\",\n",
       "              'mt': \"Évaluation de la compétence morphologique des systèmes de traduction automatiqueBien que les récents changements apportés à la traduction automatique aient permis d'améliorer la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d'informations pour mesurer pleinement l'impact des modèles neuronaux.Cet article propose un nouveau type d'évaluation axée spécifiquement sur la compétence morphologique d'un système vis-à-vis de divers phénomènes grammaticaux.Notre approche utilise des paires de phrases sources générées automatiquement, où chaque paire teste un contraste morphologique.Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT'17 pour l'anglais en tchèque et letton.\",\n",
       "              'ref': \"Évaluation de la compétence morphologique des systèmes de traduction automatiqueBien que les récentes évolutions de l'état de l'art en matière de traduction automatique aient permis d'améliorer la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d'informations pour mesurer pleinement l'impact des modèles neuronaux.Cet article propose un nouveau type d'évaluation axé spécifiquement sur la compétence morphologique d'un système vis-à-vis de divers phénomènes grammaticaux.Notre approche utilise des paires de phrases sources engendrées automatiquement, où chaque paire de phrase teste un contraste morphologique particulier.Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT'17 pour la traduction de l'anglais vers le tchèque et le letton.\"},\n",
       "             164: {'src': 'Multi-Domain Adaptation in Neural Machine Translation with Dynamic Sampling StrategiesBuilding effective Neural Machine Translation models often implies accommodating diverse sets of heterogeneous data so as to optimize performance for the domain(s) of interest.Such multi-source / multi-domain adaptation problems are typically approached through instance selection or reweighting strategies, based on a static assessment of the relevance of training instances with respect to the task at hand.In this paper, we study dynamic data selection strategies that are able to automatically re-evaluate the usefulness of data samples in the course of training.Based on the results of multiple experiments, we show that our method offer a generic framework to automatically handle several real-world situations, from multi-source or unsupervised domain adaptation to multidomain learning.',\n",
       "              'mt': \"Adaptation multi-domaine dans la traduction automatique neuronale avec des stratégies d'échantillonnage dynamiqueL'élaboration de modèles de traduction automatique neuronale efficaces implique souvent l'adaptation de divers ensembles de données hétérogènes afin d'optimiser les performances pour le(s) domaine(s) concerné(s).Ces problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection d'instances ou de repondération, basées sur une évaluation statique de la pertinence des instances d'entraînement par rapport à la tâche à accomplir.Dans cet article, nous étudions des stratégies de sélection de données dynamiques capables de réévaluer automatiquement l'utilité des échantillons de données au cours de la formation.Sur la base des résultats de plusieurs expériences, nous montrons que notre méthode offre un cadre générique pour traiter automatiquement plusieurs situations réelles, de l'adaptation multi-sources ou non supervisée à l'apprentissage multi-domaine.\",\n",
       "              'ref': \"Adaptation multi-domaine en traduction automatique neuronale par échantillonnage dynamiqueL'élaboration de modèles de traduction automatique neuronale efficaces implique souvent de recourir à de multiples ensembles de données hétérogènes afin d'optimiser les performances pour le(s) domaine(s) concerné(s).Ces problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection d'instances ou de repondération, basées sur une évaluation statique de la pertinence des instances d'entraînement par rapport à la tâche à accomplir.Dans cet article, nous étudions des stratégies de sélection de données dynamiques capables de réévaluer automatiquement l'utilité des échantillons de données au cours de l'entraînement.Sur la base des résultats de plusieurs expériences, nous montrons que notre méthode offre un cadre générique pour traiter automatiquement plusieurs situations réelles, de l'adaptation multi-sources ou non supervisée à l'apprentissage multi-domaine.\"},\n",
       "             149: {'src': 'Efficient Wait-k Models for Simultaneous Machine TranslationSimultaneous machine translation consists in starting output generation before the entire input sequence is available.Wait-k decoders offer a simple but efficient approach for this problem.They first read k source tokens, after which they alternate between producing a target token and reading another source token.We investigate the behavior of wait-k decoding in low resource settings for spoken corpora using IWSLT datasets.We improve training of these models using unidirectional encoders, and training across multiple values of k.Experiments with Transformer and 2D-convolutional architectures show that our wait-k models generalize well across a wide range of latency levels.We also show that the 2D-convolution architecture is competitive with Transformers for simultaneous translation of spoken language.',\n",
       "              'mt': \"Modèles Wait-k efficaces pour la traduction automatique simultanéeLa traduction automatique simultanée consiste à démarrer la génération de sortie avant que la séquence d'entrée entière soit disponible.Les décodeurs Wait-k offrent une approche simple mais efficace pour ce problème.Ils lisent d'abord k jetons sources, après quoi ils alternent entre la production d'un jeton cible et la lecture d'un autre jeton source.Nous étudions le comportement du décodage wait-k dans les paramètres de ressources faibles pour les corps parlés à l'aide d'ensembles de données IWSLT.Nous améliorons l'apprentissage de ces modèles en utilisant des codeurs unidirectionnels, et l'apprentissage sur plusieurs valeurs de k.Des expériences avec des transformateurs et des architectures 2D-convolutionnelles montrent que nos modèles wait-k se généralisent bien sur une large gamme de niveaux de latence.Nous montrons également que l'architecture de convolution 2D est en concurrence avec Transformers pour la traduction simultanée de la langue parlée.\",\n",
       "              'ref': \"Modèles Wait-k performants pour la traduction automatique simultanéeLa traduction automatique simultanée consiste à démarrer la génération de sortie avant que la séquence d'entrée ne soit disponible en intégralité.Les décodeurs Wait-k offrent une approche simple mais efficace de ce problème.Ils lisent d'abord k tokens sources, après quoi ils alternent entre la production d'un token cible et la lecture d'un autre token source.Nous étudions le comportement du décodage wait-k appliqué à des langues peu dotées en corpus audio, à l'aide des jeux de données de l'IWSLT.Nous améliorons l'entraînement de ces modèles en utilisant des encodeurs unidirectionnels, et en variant les valeurs de k.Des expériences avec des modèles Transformers et des architectures convolutives à deux dimensions montrent que nos modèles wait-k sont performants sur une vaste gamme de niveaux de latence.Nous montrons également que l'architecture convolutive à deux dimensions offre des résultats similaires à ceux des Transformers pour la traduction simultanée de la langue parlée.\"},\n",
       "             84: {'src': 'Keyphrase Generation for Scientific Document RetrievalSequence-to-sequence models have lead to significant progress in keyphrase generation, butit remains unknown whether they are reli-able enough to be beneficial for document re-trieval.This study provides empirical evi-dence that such models can significantly improve retrieval performance, and introducesa new extrinsic evaluation framework that al-lows for a better understanding of the limi-tations of keyphrase generation models.Using this framework, we point out and dis-cuss the difficulties encountered with supplementing documents with –not present in text– keyphrases, and generalizing models acrossdomains.Our code is available at https://github.com/boudinfl/ir-using-kg',\n",
       "              'mt': \"Génération d'expressions clés pour la récupération de documents scientifiquesLes modèles séquence à séquence ont conduit à des progrès significatifs dans la génération de phrase clé, mais il reste à savoir s'ils sont suffisamment fiables pour être bénéfiques pour la récupération de documents.Cette étude fournit des preuves empiriques que de tels modèles peuvent améliorer significativement la performance de récupération, et introduit un nouveau cadre d'évaluation extrinsèque qui permet de mieux comprendre les limites des modèles de génération de phrase clé.À l'aide de ce cadre, nous soulignons et discutons les difficultés rencontrées pour compléter des documents avec des phrases-clés - absentes dans le texte - et pour généraliser des modèles entre les domaines.Notre code est disponible sur https://github.com/boudinfl/ir-using-kg\",\n",
       "              'ref': \"Génération de mots-clés pour la recherche de documents scientifiquesLes modèles séquence à séquence ont permis des progrès significatifs dans la génération de mots-clés, mais il reste à savoir s'ils sont suffisamment fiables pour améliorer la retrouvabilité des documents.Cette étude fournit des preuves empiriques que de tels modèles peuvent améliorer significativement l'efficacité de recherche, et propose un nouveau cadre d'évaluation extrinsèque permettant de mieux comprendre les limites des modèles actuels de génération de mots-clé.Avec ce cadre, nous étudions et discutons des difficultés à enrichir des documents avec des mots-clés absents dans le texte, mais aussi à généraliser les modèles entre les domaines.Notre code est disponible à l'adresse https://github.com/boudinfl/ir-using-kg\"},\n",
       "             497: {'src': 'Some Reflections on the Interface between Professional Machine Translation Literacy and Data LiteracyDue to the widespread use of data-driven neural machine translation, both by professional translators and layperson users, an adequate machine translation literacy on the part of the users of this technology is becoming more and more important.At the same time, the increasing datafication of both the private and the business sphere requires an adequate data literacy in modern society.The present article takes a closer look at machine translation literacy and data literacy and investigates the interface between the two concepts.This is done to lay the preliminary theoretical foundations for a didactic project aiming to develop didactic resources for teaching data literacy in its machine translation-specific form to students of BA programmes in translation/specialised communication.',\n",
       "              'mt': \"Quelques réflexions sur l'interface entre la maîtrise de la traduction automatique professionnelle et la maîtrise des donnéesEn raison de l'utilisation généralisée de la traduction automatique neuronale pilotée par les données, tant par les traducteurs professionnels que par les utilisateurs non professionnels, il est de plus en plus important que les utilisateurs de cette technologie aient une bonne connaissance de la traduction automatique.Dans le même temps, la datafication croissante de la sphère privée et de la sphère professionnelle exige une connaissance adéquate des données dans la société moderne.Le présent article examine de plus près la maîtrise de la traduction automatique et la maîtrise des données et étudie l'interface entre ces deux concepts.Il s'agit de jeter les bases théoriques préliminaires d'un projet didactique visant à développer des ressources didactiques pour enseigner la maîtrise des données sous sa forme spécifique à la traduction automatique aux étudiants des programmes de licence en traduction/communication spécialisée.\",\n",
       "              'ref': \"Quelques réflexions sur l'interface entre la littératie de la traduction automatique professionnelle et la littératie des donnéesEn raison de l'utilisation généralisée de la traduction automatique neuronale axées sur les données, tant par les traducteurs professionnels que par les utilisateurs non professionnels, il est de plus en plus important que les utilisateurs de cette technologie aient une bonne connaissance de la traduction automatique.Dans le même temps, la datafication croissante de la sphère privée et de la sphère professionnelle exige une connaissance adéquate des données dans la société moderne.Le présent article examine de plus près la littératie de la traduction automatique et la littératie des données et étudie l'interface entre ces deux concepts.Il s'agit de jeter les bases théoriques préliminaires d'un projet didactique visant à développer des ressources didactiques pour enseigner la littératie des données sous sa forme spécifique à la traduction automatique aux étudiants des programmes de licence en traduction/communication spécialisée.\"},\n",
       "             291: {'src': 'Rosetta-LSF: an Aligned Corpus of French Sign Language and French for Text-to-Sign TranslationThis article presents a new French Sign Language (LSF) corpus called Rosetta-LSF.It was created to support future studies on the automatic translation of written French into LSF, rendered through the animation of a virtual signer.An overview of the field highlights the importance of a quality representation of LSF.In order to obtain quality animations understandable by signers, it must surpass the simple \"gloss transcription\" of the LSF lexical units to use in the discourse.To achieve this, we designed a corpus composed of four types of aligned data, and evaluated its usability.These are: news headlines in French, translations of these headlines into LSF in the form of videos showing animations of a virtual signer, gloss annotations of the \"traditional\" type-although including additional information on the context in which each gestural unit is performed as well as their potential for adaptation to another context-and AZee representations of the videos, i.e. formal expressions capturing the necessary and sufficient linguistic information.This article describes this data, exhibiting an example from the corpus.It is available online for public research.',\n",
       "              'mt': 'Rosetta-LSF : un corpus aligné de langue des signes française et de français pour la traduction texte-signeCet article présente un nouveau corpus en langue des signes française (LSF) appelé Rosetta-LSF.Il a été créé pour soutenir de futures études sur la traduction automatique du français écrit en LSF, rendue par l\\'animation d\\'un signeur virtuel.Un aperçu du domaine met en évidence l\\'importance d\\'une représentation de qualité de la LSF.Afin d\\'obtenir des animations de qualité compréhensibles par les signeur, il faut dépasser la simple \"transcription glossaire\" des unités lexicales en LSF à utiliser dans le discours.Pour ce faire, nous avons conçu un corpus composé de quatre types de données alignées, et évalué son utilisabilité.Il s\\'agit de titres d\\'actualité en français, de traductions de ces titres en LSF sous forme de vidéos montrant des animations d\\'un signeur virtuel, d\\'annotations de gloses de type \"traditionnel\" - bien qu\\'incluant des informations supplémentaires sur le contexte dans lequel chaque unité gestuelle est exécutée ainsi que leur potentiel d\\'adaptation à un autre contexte - et de représentations AZee des vidéos, c\\'est-à-dire d\\'expressions formelles capturant les informations linguistiques nécessaires et suffisantes.Cet article décrit ces données et présente un exemple du corpus.Il est disponible en ligne pour la recherche publique.',\n",
       "              'ref': \"Rosetta-LSF : un corpus aligné de langue des signes française et de français pour la traduction texte-signeCet article présente un nouveau corpus en langue des signes française (LSF) appelé Rosetta-LSF.Il a été créé pour soutenir de futures études sur la traduction automatique du français écrit vers la LSF, rendue par l'animation d'un signeur virtuel.Un aperçu du domaine met en évidence l'importance d'une représentation de qualité de la LSF.Afin d'obtenir des animations de qualité compréhensibles par les signeurs, il faut dépasser la simple &quot;transcription par gloses&quot; des unités lexicales en LSF à utiliser dans le discours.Pour ce faire, nous avons conçu un corpus composé de quatre types de données alignées, et évalué son utilisabilité.Il s'agit de titres d'actualité en français, de traductions de ces titres en LSF sous forme de vidéos montrant des animations d'un signeur virtuel, d'annotations en gloses de type &quot;traditionnel&quot; - bien qu'incluant des informations supplémentaires sur le contexte dans lequel chaque unité gestuelle est exécutée ainsi que leur potentiel d'adaptation à un autre contexte - et de représentations AZee des vidéos, c'est-à-dire d'expressions formelles capturant les informations linguistiques nécessaires et suffisantes.Cet article décrit ces données et présente un exemple du corpus.Il est disponible en ligne pour la recherche publique.\"},\n",
       "             165: {'src': \"Machine Translation, it’s a question of style, innit? The case of English tag questionsIn this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, isn't it?) in Machine Translation (MT).We propose a post-edition solution, formulating the problem as a multi-class classification task.We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of classifiers to predict TQ forms, which we use to post-edit state-of-the-art MT outputs.Our method provides significant improvements in English TQ translation when translating from Czech, French and German, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.\",\n",
       "              'mt': \"Traduction automatique, c'est une question de style, n'est-ce pas ? Le cas des questions de tag en anglaisDans cet article, nous abordons le problème de la génération de questions à étiquette (TQ) en anglais (par exemple, it is, isn't it ?) dans le cadre de la traduction automatique (TA).Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes.Nous présentons (i) l'annotation automatique des QT anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire les formes de QT, que nous utilisons pour post-éditer les résultats de traduction automatique les plus récents.Notre méthode apporte des améliorations significatives à la traduction des QT en anglais à partir du tchèque, du français et de l'allemand, améliorant ainsi la fluidité, le naturel, l'exactitude grammaticale et la cohérence pragmatique des résultats de la traduction assistée par ordinateur.\",\n",
       "              'ref': \"Traduction automatique, c'est une question de style, isn't it ? Le cas des question tags en anglaisDans cet article, nous abordons le problème de la génération de question tags (QT) en anglais (par exemple, it is, isn't it ?) dans le cadre de la traduction automatique (TA).Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes.Nous présentons (i) l'annotation automatique des QT anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire la forme des QT, que nous utilisons pour post-éditer les résultats produits par un système de traduction automatique de pointe.Notre méthode apporte des améliorations significatives à la traduction des QT en anglais, à partir du tchèque, du français et de l'allemand, améliorant ainsi la fluidité, le naturel, la grammaticalité et la cohérence pragmatique des résultats de la traduction automatique.\"},\n",
       "             303: {'src': 'Three Dimensions of Reproducibility in Natural Language ProcessingDespite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about even the definition of the term.That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing.This paper proposes an ontology of reproducibility in that field.We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports.These dimensions are reproducibility of a conclusion, of a finding, and of a value.Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.',\n",
       "              'mt': \"Les trois dimensions de la reproductibilité dans le traitement du langage naturelMalgré une attention considérable accordée récemment aux problèmes de reproductibilité de la recherche scientifique, il existe un manque frappant d'accord sur la définition même du terme.Il s'agit d'un problème, car l'absence d'une définition consensuelle rend difficile la comparaison des études de reproductibilité, et donc d'avoir même un large aperçu de l'état de la question dans le traitement du langage naturel.Cet article propose une ontologie de la reproductibilité dans ce domaine.Nous montrons que trois dimensions de reproductibilité, correspondant à trois types d'allégations dans les documents de traitement du langage naturel, peuvent expliquer une variété de types de rapports de recherche.Ces dimensions sont la reproductibilité d'une conclusion, d'une constatation, et d'une valeur.Trois articles biomédicaux traitant du langage naturel, rédigés par les auteurs de cet article, sont analysés en fonction de ces dimensions.\",\n",
       "              'ref': \"Trois dimensions de la reproductibilité en traitement automatique des languesMalgré une attention considérable accordée récemment aux problèmes de reproductibilité de la recherche scientifique, il existe un manque frappant d'accord sur la définition même du terme.Cela pose problème, car l'absence de définition consensuelle fait qu'il est difficile de comparer les études de reproductibilité, et donc d'avoir une vue globale de la problématique en traitement automatique des langues.Cet article propose une ontologie de la reproductibilité dans ce domaine.Nous montrons que trois dimensions de la reproductibilité, correspondant à trois types d'assertion dans les articles en traitement automatique des langues, peuvent expliquer la variété des rapports de recherche.Ces dimensions sont la reproductibilité d'une conclusion, d'une observation, et d'une valeur.Trois articles en traitement automatique de la langue biomédicale, rédigés par les auteurs de cet article, sont analysés en fonction de ces dimensions.\"},\n",
       "             80: {'src': 'Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary TasksWe introduce a constituency parser based on a bi-LSTM encoder adapted from re- cent work (Cross and Huang, 2016b; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character bi- LSTM (Ballesteros et al., 2015; Plank et al., 2016).We model two important in- terfaces of constituency parsing with aux- iliary tasks supervised at the word level: (i) part-of-speech (POS) and morpholog- ical tagging, (ii) functional label predic- tion.On the SPMRL dataset, our parser obtains above state-of-the-art results on constituency parsing without requiring ei- ther predicted POS or morphological tags, and outputs labelled dependency trees.',\n",
       "              'mt': 'Lexicalized Constituency Parsing multilingue avec des tâches auxiliaires de niveau WordNous introduisons un analyseur de circonscription basé sur un encodeur bi-LSTM adapté du travail de re-cent (Cross et Huang, 2016b; Kiperwasser et Goldberg, 2016), qui peuvent intégrer un caractère de niveau inférieur bi- LSTM (Ballesteros et al., 2015; Plank et al., 2016).Nous modélisons deux aspects importants de l’analyse des circonscriptions avec des tâches aux iliaires supervisées au niveau des mots: (I) la partie de la parole (POS) et l’étiquetage morphologique, (ii) la prédic-tion fonctionnelle de l’étiquette.Sur l’ensemble de données SPMRL, notre analyseur obtient ci-dessus des résultats de pointe sur l’analyse des circonscriptions sans nécessiter une prévision de POS ou d’étiquettes morphologiques, et des sorties marquées d’arbres de dépendance.',\n",
       "              'ref': \"Tâches auxiliaires au niveau des mots pour l'analyse syntaxique en constituants lexicalisés multilingueNous présentons un analyseur syntaxique en constituants basé sur un encodeur bi-LSTM adapté de travaux récents (Cross et Huang, 2016b; Kiperwasser et Goldberg, 2016), qui peut intégrer un bi-LSTM de caractères (Ballesteros et al., 2015; Plank et al., 2016).Nous modélisons deux aspects importants de l’analyse syntaxique avec des tâches auxiliaires supervisées au niveau des mots: (i) l'analyse en parties du discours et en traits morphologiques, (ii) la prédiction des fonctions syntaxiques.Sur l’ensemble de données SPMRL, notre analyseur obtient des résultats supérieurs à l'état de l'art en analyse syntaxique en constituents sans nécessiter de parties du discours prédites ni d’étiquettes morphologiques prédites, et permet de construire des arbres syntaxiques en dépendances étiquetées.\"},\n",
       "             176: {'src': 'The effect of domain and diacritics in Yorùbá-English neural machine translationMassively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs.However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones.The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets.In this paper, we present MENYO-20k, the first multi-domain parallel corpus with a special focus on clean orthography for Yorùbá-English with standardized train-test splits for benchmarking.We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains.Since these pre-trained models use huge amounts of data with uncertain quality, we also analyze the effect of diacritics, a major characteristic of Yorùbá, in the training data.We investigate how and when this training condition affects the final quality and intelligibility of a translation.Our models outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2M (+9.1 BLEU) when translating to Yorùbá, setting a high quality benchmark for future research.',\n",
       "              'mt': \"L'effet du domaine et des diacritiques dans la traduction automatique neuronale yorùbá-anglaiseLa traduction automatique multilingue massive (MT) a montré des capacités impressionnantes, y compris la traduction automatique entre paires de langues à faibles ressources.Cependant, ces modèles sont souvent évalués sur des langages à ressources élevées en supposant qu'ils se généralisent à des langages à faibles ressources.La difficulté d'évaluer les modèles MT sur des paires à faibles ressources est souvent due à l'absence d'ensembles de données d'évaluation normalisés.Dans cet article, nous présentons MENYO-20k, le premier corpus parallèle multi-domaines avec un accent particulier sur l'orthographe propre pour le yorùbá-anglais avec des divisions de test de train standardisées pour l'étalonnage.Nous fournissons plusieurs repères MT neuronaux et les comparons aux performances des modèles MT populaires pré-formés (massivement multilingues), à la fois pour l'ensemble de tests hétérogènes et ses sous-domaines.Puisque ces modèles pré-entraînés utilisent d'énormes quantités de données avec une qualité incertaine, nous analysons également l'effet des diacritiques, une caractéristique majeure du yorùbá, dans les données d'entraînement.Nous examinons comment et quand cette condition de formation affecte la qualité finale et l'intelligibilité d'une traduction.Nos modèles surpassent les modèles massivement multilingues tels que Google (+8.7 BLEU) et Facebook M2M (+9.1 BLEU) en traduisant en yorùbá, établissant ainsi une référence de haute qualité pour la recherche future.\",\n",
       "              'ref': \"Impact du domaine et des diacritiques dans la traduction automatique neuronale yorùbá-anglaisLa traduction automatique (TA) massivement multilingue a montré des capacités impressionnantes, y compris en traduction automatique zero- et few-shot de paires de langues peu dotées.Cependant, ces modèles sont souvent évalués sur des langues bien dotées, en présumant qu'ils seraient généralisables à des langues peu dotées.La difficulté d'évaluer les modèles de TA sur des paires de langues peu dotées est souvent due à l'absence de jeux de données d'évaluation standardisés.Dans cet article, nous présentons MENYO-20k, le premier corpus parallèle multi-domaines, compilé avec un soin particulier quant à l'orthographe, pour la paire de langues yorùbá-anglais, avec des partitions d'évaluation et d'entraînement standardisées pour le benchmarking.Nous fournissons plusieurs benchmarks en TA neuronale et les comparons aux performances des modèles de TA populaires pré-entraînés (massivement multilingues), à la fois pour les données hétérogènes d'évaluation et leurs sous-domaines.Puisque ces modèles pré-entraînés utilisent d'énormes quantités de données d'une qualité incertaine, nous analysons également l'effet des diacritiques, une caractéristique majeure du yorùbá, dans les données d'entraînement.Nous examinons comment et quand cette condition d'entraînement affecte la qualité finale et l'intelligibilité d'une traduction.Nos modèles surpassent les modèles massivement multilingues tels que Google (+8.7 BLEU) et Facebook M2M (+9.1 BLEU) en traduisant vers le yorùbá, établissant ainsi un nouveau benchmark de haute qualité pour la recherche future.\"},\n",
       "             496: {'src': 'The effect of domain and diacritics in Yorùbá-English neural machine translationMassively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs.However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones.The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets.In this paper, we present MENYO-20k, the first multi-domain parallel corpus with a special focus on clean orthography for Yorùbá-English with standardized train-test splits for benchmarking.We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains.Since these pre-trained models use huge amounts of data with uncertain quality, we also analyze the effect of diacritics, a major characteristic of Yorùbá, in the training data.We investigate how and when this training condition affects the final quality and intelligibility of a translation.Our models outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2M (+9.1 BLEU) when translating to Yorùbá, setting a high quality benchmark for future research.',\n",
       "              'mt': \"L'effet du domaine et des diacritiques dans la traduction automatique neuronale yorùbá-anglaiseLa traduction automatique multilingue massive (MT) a montré des capacités impressionnantes, y compris la traduction automatique entre paires de langues à faibles ressources.Cependant, ces modèles sont souvent évalués sur des langages à ressources élevées en supposant qu'ils se généralisent à des langages à faibles ressources.La difficulté d'évaluer les modèles MT sur des paires à faibles ressources est souvent due à l'absence d'ensembles de données d'évaluation normalisés.Dans cet article, nous présentons MENYO-20k, le premier corpus parallèle multi-domaines avec un accent particulier sur l'orthographe propre pour le yorùbá-anglais avec des divisions de test de train standardisées pour l'étalonnage.Nous fournissons plusieurs repères MT neuronaux et les comparons aux performances des modèles MT populaires pré-formés (massivement multilingues), à la fois pour l'ensemble de tests hétérogènes et ses sous-domaines.Puisque ces modèles pré-entraînés utilisent d'énormes quantités de données avec une qualité incertaine, nous analysons également l'effet des diacritiques, une caractéristique majeure du yorùbá, dans les données d'entraînement.Nous examinons comment et quand cette condition de formation affecte la qualité finale et l'intelligibilité d'une traduction.Nos modèles surpassent les modèles massivement multilingues tels que Google (+8.7 BLEU) et Facebook M2M (+9.1 BLEU) en traduisant en yorùbá, établissant ainsi une référence de haute qualité pour la recherche future.\",\n",
       "              'ref': \"L'effet du domaine et des diacritiques dans la traduction automatique neuronale yorouba-anglaisLa traduction automatique multilingue massive (MT) a montré des capacités impressionnantes, y compris la traduction automatique à zéro et peu d'exemples entre paires de langues à faibles ressources.Cependant, ces modèles sont souvent évalués sur des langues à ressources élevées en supposant qu'ils se généralisent à des langues à faibles ressources.La difficulté d'évaluer les modèles MT sur des paires à faibles ressources est souvent due à l'absence d'ensembles de données d'évaluation normalisés.Dans cet article, nous présentons MENYO-20k, le premier corpus parallèle multi-domaine avec un accent particulier sur l'orthographe propre pour yorouba-anglais avec des divisions train-test standardisées pour l'étalonnage.Nous fournissons plusieurs références MT neuronaux et les comparons aux performances des modèles MT populaires pré-entraînés (massivement multilingues), à la fois pour l'ensemble de tests hétérogènes et ses sous-domaines.Puisque ces modèles pré-entraînés utilisent d'énormes quantités de données avec une qualité incertaine, nous analysons également l'effet des diacritiques, une caractéristique majeure du yorouba, dans les données d'entraînement.Nous examinons comment et quand cette condition d'entraînement affecte la qualité finale et l'intelligibilité d'une traduction.Nos modèles surpassent les modèles massivement multilingues tels que Google (+8.7 BLEU) et Facebook M2M (+9.1 BLEU) en traduisant en yorouba, établissant ainsi une référence de haute qualité pour la recherche future.\"},\n",
       "             520: {'src': 'Controlling Utterance Length in NMT-based Word Segmentation with AttentionOne of the basic tasks of computational language documentation (CLD) is to identifyword boundaries in an unsegmented phonemic stream.While several unsupervisedmonolingual word segmentation algorithms exist in the literature,they are challenged in real-world CLD settings by the small amount of availabledata.A possible remedy is to take advantage of glosses or translation in a foreign,well-resourced, language, which often exist for such data.In this paper, we explore and compareways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation.We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.',\n",
       "              'mt': 'Contrôle de la longueur d’Utterance dans la segmentation des mots basée sur NMT avec attentionL’une des tâches de base de la documentation du langage informatique (CLD) est d’identifier les limites des mots dans un flux phonétique non segmenté.Bien que plusieurs algorithmes de segmentation de mots non supervisés existent dans la littérature, ils sont contestés dans des paramètres de CLD réels par la petite quantité de données disponibles.Un remède possible consiste à tirer parti des glosses ou de la traduction dans une langue étrangère, bien dotée de ressources, qui existe souvent pour de telles données.Dans cet article, nous explorons et comparons les moyens d’exploiter les modèles de traduction automatique neuronale pour effectuer une détection des limites non supervisée avec des informations bilingues, notamment en introduisant une nouvelle fonction de perte pour l’alignement et la segmentation de l’apprentissage conjoint.Nous expérimentons avec un langage réel sous-ressource, Mboshi, et montrons que ces techniques peuvent contrôler efficacement la longueur de segmentation de sortie.',\n",
       "              'ref': \"Contrôle de la longueur des segments dans une segmentation en mots basée sur la traduction neuronale avec attentionL’une des tâches de base de la documentation outillée des langues est d’identifier les limites des mots dans un flux phonétique non-segmenté.Bien que plusieurs algorithmes de segmentation en mots non supervisés existent dans la littérature, leur efficacité est limitée dans des conditions réelles par la petite quantité de données disponibles.Un remède possible consiste à tirer parti des gloses ou de la traduction dans une langue étrangère, bien dotée en ressources, qui existent souvent pour de telles données.Dans cet article, nous explorons et comparons divers moyens d’exploiter les modèles de traduction automatique neuronale pour effectuer une détection non supervisée des frontières de mots en présence d'informations bilingues, notamment en introduisant une nouvelle fonction de perte pour apprendre conjointement l’alignement et la segmentation.Nous expérimentons avec une langue réellement sous-dotée, le Mboshi, et montrons que nos techniques permettent de contrôler efficacement la longueur des segmentations de sortie.\"},\n",
       "             179: {'src': 'Findings of the 2022 Conference on Machine Translation (WMT22)This paper presents the results of the General Machine Translation Task organised as part of the Conference on Machine Translation (WMT) 2022.In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of four different domains.We evaluate system outputs with human annotators using two different techniques: reference-based direct assessment and (DA) and a combination of DA and scalar quality metric (DA+SQM).',\n",
       "              'mt': \"Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Ce document présente les résultats de la Tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans la tâche générale MT, les participants ont été invités à construire des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, à évaluer sur des ensembles de tests composés de quatre domaines différents.Nous évaluons les sorties du système avec des annotateurs humains en utilisant deux techniques différentes : évaluation directe et (DA) basée sur des références et combinaison de DA et de mesure de qualité scalaire (DA+SQM).\",\n",
       "              'ref': \"Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Ce document présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans le cadre de la tâche générale MT, les participants ont été invités à bâtir des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, ces systèmes devant être évalués sur des jeux de données de test subdivisés en quatre domaines distincts.Nous évaluons les traductions produites par ces systèmes grâce à des annotateurs humains en recourant à deux techniques différentes : l'évaluation directe (DA, pour direct assessment en anglais) basée sur des références et la combinaison de la DA et d'une mesure de qualité scalaire (DA+SQM).\"},\n",
       "             494: {'src': 'Findings of the 2022 Conference on Machine Translation (WMT22)This paper presents the results of the General Machine Translation Task organised as part of the Conference on Machine Translation (WMT) 2022.In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of four different domains.We evaluate system outputs with human annotators using two different techniques: reference-based direct assessment and (DA) and a combination of DA and scalar quality metric (DA+SQM).',\n",
       "              'mt': \"Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Ce document présente les résultats de la Tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans la tâche générale MT, les participants ont été invités à construire des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, à évaluer sur des ensembles de tests composés de quatre domaines différents.Nous évaluons les sorties du système avec des annotateurs humains en utilisant deux techniques différentes : évaluation directe et (DA) basée sur des références et combinaison de DA et de mesure de qualité scalaire (DA+SQM).\",\n",
       "              'ref': \"Conclusions de la conférence de 2022 sur la traduction automatique (WMT22)Ce document présente les résultats de la tâche générale de traduction automatique organisée dans le cadre de la Conférence sur la traduction automatique (WMT) 2022.Dans la tâche générale TA, les participants ont été invités à construire des systèmes de traduction automatique pour n'importe laquelle des 11 paires de langues, à évaluer sur des ensembles de tests composés de quatre domaines différents.Nous évaluons les sorties du système avec des annotateurs humains en utilisant deux techniques différentes : évaluation directe basée sur des références (DA) et une combinaison de DA et de mesure de qualité scalaire (DA+SQM).\"},\n",
       "             334: {'src': 'Temporal information extraction from clinical textIn this paper, we present a method for temporal relation extraction from clinical narratives in French and in English.We experiment on two comparable corpora, the MERLOT corpus for French and the THYME corpus for English, and show that a common approach can be used for both languages.',\n",
       "              'mt': \"Extraction d'informations temporelles à partir de textes cliniquesDans cet article, nous présentons une méthode d'extraction de relations temporelles à partir de récits cliniques en français et en anglais.Nous expérimentons sur deux corpus comparables, le corpus MERLOT pour le français et le corpus THYME pour l'anglais, et montrons qu'une approche commune peut être utilisée pour les deux langues.\",\n",
       "              'ref': \"Extraction d'informations temporelles à partir de textes cliniquesDans cet article, nous présentons une méthode d'extraction de relations temporelles à partir de documents cliniques en français et en anglais.Nous présentons des expériences sur deux corpus comparables, le corpus MERLOT pour le français et le corpus THYME pour l'anglais, et montrons qu'une approche commune peut être utilisée pour les deux langues.\"},\n",
       "             412: {'src': 'Aligning Subtitles in Sign Language VideosThe goal of this work is to temporally align asynchronous subtitles in sign language videos.In particular, we focus on sign-language interpreted TV broadcast data comprising (i) a video of continuous signing, and (ii) subtitles corresponding to the audio content.Previous work exploiting such weakly-aligned data only considered finding keyword-sign correspondences, whereas we aim to localise a complete subtitle text in continuous signing.We propose a Transformer architecture tailored for this task, which we train on manually annotated alignments covering over 15K subtitles that span 17.7 hours of video.We use BERT subtitle embeddings and CNN video representations learned for sign recognition to encode the two signals, which interact through a series of attention layers.Our model outputs frame-level predictions, i.e., for each video frame, whether it belongs to the queried subtitle or not.Through extensive evaluations, we show substantial improvements over existing alignment baselines that do not make use of subtitle text embeddings for learning.Our automatic alignment model opens up possibilities for advancing machine translation of sign languages via providing continuously synchronized video-text data.',\n",
       "              'mt': \"Alignement des sous-titres dans les vidéos en langue des signesL'objectif de ce travail est d'aligner temporellement des sous-titres asynchrones dans des vidéos en langue des signes.En particulier, nous nous concentrons sur les données d'émissions télévisées interprétées en langue des signes comprenant (i) une vidéo de signes en continu et (ii) des sous-titres correspondant au contenu audio.Les travaux antérieurs exploitant ces données faiblement alignées n'ont porté que sur la recherche de correspondances mot-clé-signe, alors que notre objectif est de localiser un texte de sous-titre complet dans une signature continue.Nous proposons une architecture Transformer adaptée à cette tâche, que nous entraînons sur des alignements annotés manuellement couvrant plus de 15 000 sous-titres sur 17,7 heures de vidéo.Nous utilisons les encastrements de sous-titres BERT et les représentations vidéo CNN apprises pour la reconnaissance des signes afin d'encoder les deux signaux, qui interagissent par le biais d'une série de couches d'attention.Notre modèle produit des prédictions au niveau de l'image, c'est-à-dire qu'il indique pour chaque image vidéo si elle appartient ou non au sous-titre demandé.Grâce à des évaluations approfondies, nous montrons des améliorations substantielles par rapport aux lignes de base d'alignement existantes qui n'utilisent pas les enchâssements de textes de sous-titres pour l'apprentissage.Notre modèle d'alignement automatique ouvre des possibilités d'amélioration de la traduction automatique des langues des signes en fournissant des données vidéo et textuelles synchronisées en continu.\",\n",
       "              'ref': \"Alignement de sous-titres dans des vidéos en langue des signesL'objectif de ce travail est d'aligner temporellement des sous-titres asynchrones dans des vidéos en langue des signes.En particulier, nous nous concentrons sur des données d'émissions télévisées interprétées en langue des signes comprenant (i) une vidéo de signes en continu et (ii) des sous-titres correspondant au contenu audio.Les travaux antérieurs exploitant ces données faiblement alignées n'ont porté que sur la recherche de correspondances entre mot-clefs et signes, alors que notre objectif est de localiser un texte de sous-titre complet dans un flux de signes continu.Nous proposons une architecture Transformer adaptée à cette tâche, que nous entraînons sur des alignements annotés manuellement couvrant plus de 15 000 sous-titres sur 17,7 heures de vidéo.Nous utilisons les plongements de sous-titres BERT et les représentations vidéo CNN apprises pour la reconnaissance des signes afin d'encoder les deux signaux, qui interagissent par le biais d'une série de couches d'attention.Notre modèle produit des prédictions au niveau des trames, c'est-à-dire qu'il indique pour chaque trame de la vidéo si elle appartient ou non au sous-titre considéré.Grâce à des évaluations approfondies, nous montrons des améliorations substantielles par rapport aux systèmes d'alignement existants qui n'utilisent pas les plongements de texte de sous-titres pour l'apprentissage.Notre modèle d'alignement automatique ouvre des possibilités pour l'amélioration de la traduction automatique des langues des signes en fournissant des données vidéo et textuelles synchronisées en continu.\"},\n",
       "             373: {'src': 'Joint Generation of Captions and Subtitles with Dual DecodingAs the amount of audiovisual content increases, the need to develop automatic captioning and subtitling solutions to match the expectations of a growing international audience appears as the only viable way to boost throughput and lower the related postproduction costs.Automatic captioning and subtitling often need to be tightly intertwined to achieve an appropriate level of consistency and synchronization with each other and with the video signal.In this work, we assess a dual decoding scheme to achieve a strong coupling between these two tasks and show how adequacy and consistency are increased, with virtually no additional cost in terms of model size and training complexity.',\n",
       "              'mt': \"Génération conjointe de sous-titres et de légendes avec double décodageÀ mesure que la quantité de contenu audiovisuel augmente, la nécessité de développer des solutions de sous-titrage automatique pour répondre aux attentes d'un public international de plus en plus nombreux apparaît comme le seul moyen viable d'augmenter le débit et de réduire les coûts de postproduction qui y sont liés.Le sous-titrage automatique et le sous-titrage doivent souvent être étroitement liés pour atteindre un niveau approprié de cohérence et de synchronisation entre eux et avec le signal vidéo.Dans ce travail, nous évaluons un schéma de décodage double pour réaliser un couplage fort entre ces deux tâches et nous montrons comment l'adéquation et la cohérence sont améliorées, avec pratiquement aucun coût supplémentaire en termes de taille du modèle et de complexité de l'apprentissage.\",\n",
       "              'ref': \"Génération conjointe de sous-titres intralinguistiques et interlinguistiques par le biais du décodage doubleÀ mesure que la quantité de contenu audiovisuel augmente, le développement de solutions de sous-titrage automatique qui répondent aux attentes d'un public international de plus en plus nombreux apparaît comme le seul moyen viable d'augmenter le débit de production et de réduire les coûts de postproduction qui y sont liés.Sous-titrages intralinguistique et interlinguistique doivent souvent être étroitement liés pour atteindre un niveau approprié de cohérence et de synchronisation, aussi bien entre eux qu'avec le signal vidéo.Dans ce travail, nous évaluons un schéma de décodage double pour réaliser un couplage fort entre ces deux tâches, et nous montrons comment l'adéquation et la cohérence sont améliorées, avec pratiquement aucun coût supplémentaire en termes de taille du modèle et de complexité de l'apprentissage.\"},\n",
       "             335: {'src': \"MEDLINE as a parallel corpus: a survey to gain insight on French-, Spanish- and Portuguese-speaking authors' abstract writing practiceBackground: Parallel corpora are used to train and evaluate machine translation systems.To alleviate the cost of producing parallelresources for evaluation campaigns, existing corpora are leveraged.However, little information may be available about the methodsused for producing the corpus, including translation direction.Objective: To gain insight on MEDLINE parallel corpus used in thebiomedical task at the Workshop on Machine Translation in 2019 (WMT 2019).Material and Methods:Contact information for theauthors of MEDLINE articles included in the English/Spanish (EN/ES), English/French (EN/FR), and English/Portuguese (EN/PT)WMT 2019 test sets was obtained from PubMed and publisher websites.The authors were asked about their abstract writing practicesin a survey.Results: The response rate was above 20%.Authors reported that they are mainly native speakers of languages other thanEnglish.Although manual translation, sometimes via professional translation services, was commonly used for abstract translation,authors of articles in the EN/ES and EN/PT sets also relied on post-edited machine translation.Discussion: This study provides acharacterization of MEDLINE authorsâ language skills and abstract writing practices.Conclusion: The information collected in thisstudy will be used to inform test set design for the next WMT biomedical task.\",\n",
       "              'mt': 'MEDLINE en tant que corpus parallèle: une enquête pour mieux comprendre la pratique de l’écriture abstraite des auteurs francophones, hispanophones et portugaisContexte: Des corpus parallèles sont utilisés pour former et évaluer les systèmes de traduction automatique.Pour alléger le coût de production de ressources parallèles pour les campagnes d’évaluation, les corpus existants sont exploités.Cependant, peu d’informations peuvent être disponibles sur les méthodes utilisées pour produire le corpus, y compris la direction de la traduction.Objectif: Pour obtenir un aperçu du corpus parallèle MEDLINE utilisé dans la tâche biomédicale lors de l’atelier sur la traduction automatique en 2019 (WMT 2019).Matériel et méthodes:Les coordonnées des auteurs d’articles MEDLINE inclus dans les tests anglais/espagnol (EN/ES), anglais/français (EN/FR) et anglais/portugais (EN/PT)WMT 2019 ont été obtenues auprès des sites Web de PubMed et d’éditeurs.Les auteurs ont été interrogés sur leurs pratiques d’écriture abstraite dans une enquête.Résultats: Le taux de réponse était supérieur à 20\\xa0%.Les auteurs ont indiqué qu’ils étaient principalement des locuteurs natifs de langues autres que l’anglais.Bien que la traduction manuelle, parfois par l’intermédiaire de services de traduction professionnelle, soit couramment utilisée pour la traduction abstraite, les auteurs d’articles dans les ensembles EN/ES et EN/PT se sont également appuyés sur la traduction automatique post-éditée.Discussion: Cette étude fournit une caractérisation des compétences linguistiques des auteurs de MEDLINE et des pratiques d’écriture abstraite.Conclusion: Les informations recueillies dans cette étude seront utilisées pour éclairer la conception des ensembles de tests pour la prochaine tâche biomédicale WMT.',\n",
       "              'ref': \"Le corpus parallèle MEDLINE : une enquête sur la pratique de rédaction de résumés des auteurs de langue française, espagnole et portugaiseContexte: Les corpus parallèles sont utilisés pour entrainer et évaluer les systèmes de traduction automatique.Pour alléger le coût de production de ressources parallèles pour les campagnes d’évaluation, des corpus existants sont exploités.Cependant, peu d’informations sont parfois disponibles sur les méthodes utilisées pour produire ces corpus, y compris la direction de la traduction.Objectif: Caractériser le corpus parallèle MEDLINE utilisé dans la tâche biomédicale lors du Workshop on Machine Translation en 2019 (WMT 2019).Matériel et méthodes:Les coordonnées des auteurs d’articles MEDLINE inclus dans les corpus de test anglais/espagnol (EN/ES), anglais/français (EN/FR) et anglais/portugais (EN/PT)de WMT 2019 ont été collectés sur PubMed et sur les sites des éditeurs de revues.Dans une enquête, les auteurs ont été interrogés sur leurs pratiques de rédaction des résumés.Résultats: Le taux de réponse était supérieur à 20\\xa0%.Les auteurs ont indiqué qu’ils étaient principalement des locuteurs natifs de langues autres que l’anglais.Bien que la traduction manuelle, parfois par l’intermédiaire de services de traduction professionnelle, soit couramment utilisée pour la traduction des résumés, les auteurs d’articles des corpus EN/ES et EN/PT indiquent également avoir recours à la traduction automatique post-éditée.Discussion: Cette étude permet de caractériser les compétences linguistiques des auteurs d'articles indexés dans MEDLINE ainsi que les pratiques de rédaction des résumés.Conclusion: Les informations recueillies dans cette étude seront utilisées pour guider la conception des corpus de tests pour la tâche biomédicale des prochaines éditions de WMT.\"},\n",
       "             307: {'src': 'Parallel Corpora for the Biomedical DomainA vast amount of biomedical information is available in the form of scientific literature and government-authored patient information documents.While English is the most widely used language in many of these sources, there is a need to provide access to health information in languages other than English.Parallel corpora can be leveraged to implement cross-lingual information retrievalor machine translation tools.Herein, we review the extent of parallel corpus coverage in the biomedical domain.Specifically, we perform a scoping review of existing resources and we describe the recent development of new datasets for scientific literature (the EDP dataset and an extension of the Scielo corpus) and clinical trials (the ReBEC corpus).These corpora are currently beingused in the biomedical task in the Conference on Machine Translation (WMT16 and WMT17), which illustrates their potential for improving and evaluating biomedical machine translation systems.Furthermore, we suggest additional applications for multilingual natural language processing using these resources, and plan to extend resource coverage to additional text genres and language pairs.',\n",
       "              'mt': \"Corpora parallèles pour le domaine biomédicalUne grande quantité d'informations biomédicales est disponible sous la forme de littérature scientifique et de documents d'information pour les patients rédigés par les pouvoirs publics.Bien que l'anglais soit la langue la plus utilisée dans bon nombre de ces sources, il est nécessaire de fournir un accès à l'information sur la santé dans des langues autres que l'anglais.Les corpus parallèles peuvent être utilisés pour mettre en œuvre des outils de recherche d'informations ou de traduction automatique multilingues.Nous examinons ici l'étendue de la couverture des corpus parallèles dans le domaine biomédical.Plus précisément, nous procédons à une analyse des ressources existantes et nous décrivons le développement récent de nouveaux ensembles de données pour la littérature scientifique (l'ensemble de données EDP et une extension du corpus Scielo) et les essais cliniques (le corpus ReBEC).Ces corpus sont actuellement utilisés dans la tâche biomédicale de la Conférence sur la traduction automatique (WMT16 et WMT17), ce qui illustre leur potentiel pour l'amélioration et l'évaluation des systèmes de traduction automatique biomédicale.En outre, nous suggérons d'autres applications pour le traitement multilingue du langage naturel en utilisant ces ressources, et nous prévoyons d'étendre la couverture des ressources à d'autres genres de textes et paires de langues.\",\n",
       "              'ref': \"Corpus parallèles pour le domaine biomédicalDans le domaine biomédical, une grande quantité d'informations est disponible sous la forme d'articles de la littérature et de documents d'information pour les patients rédigés par les pouvoirs publics.Bien que l'anglais soit la langue la plus utilisée dans bon nombre de ces sources, il est nécessaire de fournir un accès à l'information sur la santé dans des langues autres que l'anglais.Les corpus parallèles peuvent être utilisés pour mettre en œuvre des outils de recherche d'information ou de traduction automatique multilingue.Nous examinons ici l'étendue de la couverture des corpus parallèles dans le domaine biomédical.Plus précisément, nous procédons à une analyse des ressources existantes et nous décrivons le développement récent de nouveaux corpus pour la littérature scientifique (le corpus EDP et une extension du corpus Scielo) et les essais cliniques (le corpus ReBEC).Ces corpus sont actuellement utilisés dans la tâche biomédicale de la Conference on Machine Translation (WMT16 et WMT17), ce qui illustre leur potentiel pour l'amélioration et l'évaluation des systèmes de traduction automatique dans le domaine biomédical.En outre, nous suggérons d'autres applications pour le traitement multilingue des langues en utilisant ces ressources, et nous prévoyons d'étendre la couverture des ressources à d'autres genres de textes et paires de langues.\"},\n",
       "             525: {'src': 'Three Dimensions of Reproducibility in Natural Language ProcessingDespite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about even the definition of the term.That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing.This paper proposes an ontology of reproducibility in that field.We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports.These dimensions are reproducibility of a conclusion, of a finding, and of a value.Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.',\n",
       "              'mt': \"Les trois dimensions de la reproductibilité dans le traitement du langage naturelMalgré une attention considérable accordée récemment aux problèmes de reproductibilité de la recherche scientifique, il existe un manque frappant d'accord sur la définition même du terme.Il s'agit d'un problème, car l'absence d'une définition consensuelle rend difficile la comparaison des études de reproductibilité, et donc d'avoir même un large aperçu de l'état de la question dans le traitement du langage naturel.Cet article propose une ontologie de la reproductibilité dans ce domaine.Nous montrons que trois dimensions de reproductibilité, correspondant à trois types d'allégations dans les documents de traitement du langage naturel, peuvent expliquer une variété de types de rapports de recherche.Ces dimensions sont la reproductibilité d'une conclusion, d'une constatation, et d'une valeur.Trois articles biomédicaux traitant du langage naturel, rédigés par les auteurs de cet article, sont analysés en fonction de ces dimensions.\",\n",
       "              'ref': \"Les trois dimensions de la reproductibilité dans le traitement du langage naturelMalgré une attention considérable accordée récemment aux problèmes de reproductibilité de la recherche scientifique, il existe un manque d'accord frappant sur la définition même du terme.C'est un problème, car l'absence d'une définition consensuelle rend difficile la comparaison des études de reproductibilité, et donc d'avoir même un large aperçu de l'état de la question dans le traitement du langage naturel.Cet article propose une ontologie de la reproductibilité dans ce domaine.Nous montrons que trois dimensions de reproductibilité, correspondant à trois types de réclamations dans les documents du traitement du langage naturel, peuvent expliquer une variété de types de rapports de recherche.Ces dimensions sont la reproductibilité d'une conclusion, d'une constatation, et d'une valeur.Trois articles biomédicaux traitant du langage naturel, rédigés par les auteurs de cet article, sont analysés en fonction de ces dimensions.\"},\n",
       "             85: {'src': 'Towards contextual adaptation for any-text translationabstract',\n",
       "              'mt': \"Vers l'adaptation contextuelle pour la traduction de n'importe quel texterésumé\",\n",
       "              'ref': \"Vers l'adaptation contextuelle pour la traduction de textes arbitrairesrésumé\"},\n",
       "             407: {'src': 'MEDIAPI-SKEL -A 2D-Skeleton Video Database of French Sign Language With Aligned French SubtitlesThis paper presents MEDIAPI-SKEL, a 2D-skeleton database of French Sign Language videos aligned with French subtitles.The corpus contains 27 hours of video of body, face and hand keypoints, aligned to subtitles with a vocabulary size of 17k tokens.In contrast to existing sign language corpora such as videos produced under laboratory conditions or translations of TV programs into sign language, this database is constructed using original sign language content largely produced by deaf journalists at the media company Média-Pi.Moreover, the videos are accurately synchronized with French subtitles.We propose three challenges appropriate for this corpus that are related to processing units of signs in context: automatic alignment of text and video, semantic segmentation of sign language, and production of video-text embeddings for cross-modal retrieval.These challenges deviate from the classic task of identifying a limited number of lexical signs in a video stream.',\n",
       "              'mt': \"MEDIAPI-SKEL - Une base de données vidéo 2D-Skeleton de la langue des signes française avec des sous-titres français alignésCet article présente MEDIAPI-SKEL, une base de données en 2D de vidéos en Langue des Signes Française alignées avec des sous-titres en Français.Le corpus contient 27 heures de vidéo du corps, du visage et des mains, alignées sur des sous-titres avec une taille de vocabulaire de 17k jetons.Contrairement aux corpus existants en langue des signes, tels que les vidéos produites en laboratoire ou les traductions d'émissions télévisées en langue des signes, cette base de données est construite à partir de contenus originaux en langue des signes, largement produits par des journalistes sourds de la société Média-Pi.De plus, les vidéos sont synchronisées avec précision avec les sous-titres français.Nous proposons trois défis appropriés pour ce corpus qui sont liés aux unités de traitement des signes dans le contexte : alignement automatique du texte et de la vidéo, segmentation sémantique du langage des signes, et production d'intégrations de texte vidéo pour la récupération intermodale.Ces défis s'écartent de la tâche classique d'identifier un nombre limité de signes lexicaux dans un flux vidéo.\",\n",
       "              'ref': \"MEDIAPI-SKEL - Une base d'annotations de squelettes 2D pour des vidéos en langue des signes française avec des sous-titres français alignésCet article présente MEDIAPI-SKEL, une base d'annotations de squelettes 2D pour des vidéos en Langue des Signes Française, alignées avec des sous-titres en Français.Le corpus contient 27 heures de vidéo de point-clefs du corps, du visage et des mains, alignées sur des sous-titres avec une taille de vocabulaire de 17k unités.Contrairement aux corpus en langue des signes existants, tels que les vidéos produites en laboratoire ou les traductions d'émissions télévisées en langue des signes, cette base de données est construite à partir de contenus signés originaux, produits en grande partie par des journalistes sourds de la société Média-Pi.De plus, les vidéos sont synchronisées précisément avec les sous-titres français.Nous proposons trois défis appropriés pour ce corpus, qui sont liés au traitement d'unités de signes en contexte : alignement automatique du texte et de la vidéo, segmentation sémantique du langage des signes, et production de plongements conjoints vidéo-texte pour la recherche intermodale.Ces défis s'écartent de la tâche classique d'identifier un nombre limité de signes lexicaux dans un flux vidéo.\"},\n",
       "             144: {'src': 'LIUM Machine Translation Systems for WMT17 News Translation TaskThis paper describes LIUM submissions to WMT17 News Translation Task for English↔German, English↔Turkish, English→Czech and English→Latvian language pairs.We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.Competitive scores were obtained by en-sembling various systems and exploiting the availability of target monolingual corpora for back-translation.The impact of back-translation quantity and quality is also analyzed for English→Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.',\n",
       "              'mt': \"Systèmes de Traduction Automatique LIUM pour WMT17 News Translation TaskCet article décrit les soumissions LIUM à la tâche de traduction d'actualités WMT17 pour l'anglais.Nous formons des systèmes de traduction automatique neuronale attentive basés sur BPE avec et sans sorties factorisées en utilisant le framework nmtpy open source.Des scores compétitifs ont été obtenus en assemblant divers systèmes et en exploitant la disponibilité de corpus monolingues cibles pour la rétrotraduction.L'impact de la quantité et de la qualité de la rétrotraduction est également analysé pour English→Turkish, où notre soumission après la date limite a dépassé la meilleure entrée de +1,6 BLEU.\",\n",
       "              'ref': \"Systèmes de Traduction Automatique du LIUM pour la tâche de traduction d'articles de presse de la conférence WMT17Cet article décrit les contributions du LIUM à la tâche de traduction de textes de presse WMT17 pour l'anglais.Nous avons entraîné des systèmes de traduction automatique neuronale attentifs basés sur la tokénisation BPE, avec et sans sorties factorisées, en utilisant la suite d'outils libre nmtpy.Des scores compétitifs ont été obtenus en assemblant divers systèmes et en tirant parti de la disponibilité de corpus monolingues cibles pour la rétro-traduction.L'impact de la quantité et de la qualité de la rétro-traduction a également été analysé pour la direction anglais→turc, où notre contribution, soumise après la date limite, a dépassé le meilleur concurrent de +1,6 BLEU.\"},\n",
       "             61: {'src': 'One Source, Two Targets: Challenges and Rewards of Dual DecodingMachine translation is generally understood as generating one target text from an input source document.In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other.As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text.We present an analysis of possible implementations of dual decoding, and experiment with four applications.Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.',\n",
       "              'mt': 'Une source, deux cibles: Défis et récompenses de la double décorationLa traduction automatique est généralement comprise comme la génération d’un texte cible à partir d’un document source d’entrée.Dans ce document, nous considérons une exigence plus forte: générer conjointement deux textes de sorte que chaque côté de sortie dépende efficacement de l’autre.Comme nous en discutons, un tel dispositif sert plusieurs fins pratiques, de la traduction automatique multi-cibles à la génération de variations contrôlées du texte cible.Nous présentons une analyse des implémentations possibles du double décodage et expérimentons quatre applications.Voir le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du double décodage et d’analyser en profondeur les avantages de générer des traductions appariées, plutôt que indépendantes.',\n",
       "              'ref': \"Une source, deux cibles: Défis et bénéfices d'un double décodageLa traduction automatique est généralement comprise comme la génération d’un texte cible à partir d’un document source en entrée.Dans cet article, nous considérons une exigence plus forte: engendrer conjointement deux textes, de sorte que chacune des sorties dépende effectivement de l’autre.Comme nous le discutons, un tel dispositif a plusieurs applications pratiques, de la traduction automatique multi-cible à la génération de variations contrôlées du texte cible.Nous présentons une analyse des implantations possibles du double décodage et expérimentons avec quatre applications.Considérer le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du double décodage et d’analyser en profondeur les avantages d'engendrer des traductions appariées, plutôt qu'indépendantes.\"},\n",
       "             264: {'src': 'Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n^6) down to O(n^3)We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures.In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(n^6) down to O(n^3).The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers.We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting.We also experiment with pre-trained word embeddings and Bertbased neural networks.',\n",
       "              'mt': \"Analyse de constituants discontinus basée sur l'étendue : une famille d'algorithmes exacts basés sur des diagrammes avec des complexités temporelles de O(n^6) à O(n^3)Nous présentons un nouvel algorithme basé sur les diagrammes pour l'analyse syntaxique basée sur l'étendue des arbres de circonscription discontinus de degré de bloc deux, y compris les structures mal imbriquées.En particulier, nous montrons que nous pouvons construire des variantes de notre analyseur syntaxique avec des espaces de recherche plus petits et des complexités temporelles allant de O(n^6) à O(n^3).La variante à temps cubique couvre 98% des constituants observés dans les banques de données linguistiques tout en ayant la même complexité que les analyseurs de constituants continus.Nous évaluons notre approche sur des banques de données allemandes et anglaises (Negra, Tiger et DPTB) et rapportons des résultats de pointe dans un cadre entièrement supervisé.Nous expérimentons également avec des word embeddings pré-entraînés et des réseaux neuronaux basés sur Bert.\",\n",
       "              'ref': \"Analyse de constituants discontinus fondée sur les empans : une famille d'algorithmes tabulaires exacts avec des complexités temporelles de O(n^6) à O(n^3)Nous présentons un nouvel algorithme tabulaire pour l'analyse syntaxique fondées sur les empans des arbres en constituants discontinus de degré de bloc deux, y compris les structures mal imbriquées.En particulier, nous montrons que nous pouvons construire des variantes de notre analyseur syntaxique avec des espaces de recherche restreints et des complexités temporelles allant de O(n^6) à O(n^3).La variante en temps cubique couvre 98% des constituants observés dans les jeux de données linguistiques tout en ayant la même complexité que les analyseurs de constituants continus.Nous évaluons notre approche sur des jeux de données en allemand et en anglais (Negra, Tiger et DPTB) et rapportons des résultats à l'état de l'art dans un cadre entièrement supervisé.Nous expérimentons également avec des plongements lexicaux pré-entraînés et des réseaux neuronaux fondés sur Bert.\"},\n",
       "             169: {'src': 'Amazon Mechanical Turk: Gold Mine or Coal Mine ?Recently heard at a tutorial in our field: \"It cost me less than one hundred bucks to annotate this using Amazon Mechanical Turk!\"Assertions like this are increasingly common, but we believe they should not be stated so proudly; they ignore the ethical consequences of using MTurk (Amazon Mechanical Turk) as a source of labour.Manually annotating corpora or manually developing any other linguistic resource, such as a set of judgments about system outputs, represents such a high cost that many researchers are looking for alternative solutions to the standard approach.MTurk is becoming a popular one.However, as in any scientific endeavor involving humans, there is an unspoken ethical dimension involved in resource construction and system evaluation, and this is especially true of MTurk.We would like here to raise some questions about the use of MTurk.To do so, we will define precisely what MTurk is and what it is not, highlighting the issues raised by the system.We hope that this will point out opportunities for our community to deliberately value ethics above cost savings.',\n",
       "              'mt': 'Amazon Mechanical Turk : Mine d\\'or ou mine de charbon ?Récemment entendu lors d\\'un tutoriel dans notre domaine : \"Cela m\\'a coûté moins de cent dollars d\\'annoter ceci en utilisant Amazon Mechanical Turk !\"Des affirmations comme celle-ci sont de plus en plus courantes, mais nous pensons qu\\'elles ne devraient pas être énoncées avec autant de fierté ; elles ignorent les conséquences éthiques de l\\'utilisation de MTurk (Amazon Mechanical Turk) comme source de main-d\\'œuvre.L\\'annotation manuelle de corpus ou le développement manuel de toute autre ressource linguistique, telle qu\\'un ensemble de jugements sur les résultats d\\'un système, représente un coût si élevé que de nombreux chercheurs recherchent des solutions alternatives à l\\'approche standard.MTurk est en train de devenir une solution populaire.Cependant, comme dans toute entreprise scientifique impliquant des êtres humains, il existe une dimension éthique tacite dans la construction des ressources et l\\'évaluation des systèmes, et c\\'est particulièrement vrai pour MTurk.Nous voudrions ici soulever quelques questions sur l\\'utilisation de MTurk.Pour ce faire, nous définirons précisément ce qu\\'est MTurk et ce qu\\'il n\\'est pas, en soulignant les questions soulevées par le système.Nous espérons ainsi mettre en évidence les possibilités pour notre communauté de privilégier délibérément l\\'éthique par rapport aux économies de coûts.',\n",
       "              'ref': 'Amazon Mechanical Turk : Mine d\\'or ou mine de charbon ?Récemment entendu lors d\\'un tutoriel dans notre domaine : \"Cela m\\'a coûté moins de cent dollars d\\'annoter ceci en utilisant Amazon Mechanical Turk !\"Des affirmations comme celle-ci sont de plus en plus courantes, mais nous pensons qu\\'elles ne devraient pas être énoncées avec autant de fierté ; elles ignorent les conséquences éthiques de l\\'utilisation de MTurk (Amazon Mechanical Turk) comme source de main-d\\'œuvre.L\\'annotation manuelle de corpus ou le développement manuel de toute autre ressource linguistique, telle qu\\'un ensemble de jugements sur les résultats d\\'un système, représente un coût si élevé que de nombreux chercheurs recherchent des solutions alternatives à l\\'approche standard.MTurk est en train de devenir une solution populaire.Cependant, comme dans toute entreprise scientifique impliquant des êtres humains, il existe une dimension éthique tacite dans la construction des ressources et l\\'évaluation des systèmes, et c\\'est particulièrement vrai pour MTurk.Nous voudrions ici soulever quelques questions sur l\\'utilisation de MTurk.Pour ce faire, nous définirons précisément ce qu\\'est MTurk et ce qu\\'il n\\'est pas, en soulignant les questions soulevées par le système.Nous espérons ainsi mettre en évidence les possibilités pour notre communauté de privilégier délibérément l\\'éthique par rapport aux économies de coûts.'},\n",
       "             519: {'src': 'Priming Neural Machine TranslationPriming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response.In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT).We evaluate the effect of using similar translations as priming cues on the NMT network.We propose a method to inject priming cues into the NMT networkand compare our framework to other mechanisms that perform micro-adaptation during inference.Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy.Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.',\n",
       "              'mt': 'Amorçage de la traduction automatique de neuronesL’amorçage est un phénomène psychologique bien connu et étudié basé sur la présentation préalable d’un stimulus (cue) pour influencer le traitement d’une réponse.Dans cet article, nous proposons un cadre pour imiter le processus d’amorçage dans le contexte de la traduction automatique neuronale (NMT).Nous évaluons l’effet de l’utilisation de traductions similaires comme indices d’amorçage sur le réseau NMT.Nous proposons une méthode pour injecter des signaux d’amorçage dans le réseau NMT et comparer notre framework à d’autres mécanismes qui effectuent la micro-adaptation pendant l’inférence.Dans l’ensemble, les expériences menées dans un contexte multi-domaines confirment que l’ajout d’indicateurs d’amorçage dans le décodeur NMT peut contribuer grandement à améliorer la précision de la traduction.',\n",
       "              'ref': 'Amorçage de la traduction automatique de neuronesL’amorçage est un phénomène psychologique bien connu et étudié basé sur la présentation préalable d’un stimulus (indice) pour influencer la production d’une réponse.Dans cet article, nous proposons un cadre pour imiter le processus d’amorçage dans un contexte de traduction automatique neuronale (NMT).Nous évaluons l’effet de l’utilisation de traductions similaires comme indices d’amorçage dans un système de traduction neuronale.Nous proposons une méthode pour injecter des signaux d’amorçage dans le réseau et nous comparons notre framework à d’autres mécanismes qui effectuent une micro-adaptation pendant l’inférence.Dans l’ensemble, les expériences conduites dans un contexte multi-domaines confirment que l’ajout d’indices d’amorçage dans le décodeur du système peut contribuer grandement à améliorer la précision de la traduction.'},\n",
       "             306: {'src': 'A French clinical corpus with comprehensive semantic annotations: development of the Medical Entity and Relation LIMSI annOtated Text corpus (MERLOT)Quality annotated resources are essential for Natural Language Processing.The objective of this work is to present a corpus of clinical narratives in French annotated for linguistic, semantic and structural information, aimed at clinical information extraction.Six annotators contributed to the corpus annotation, using a comprehensive annotation scheme covering 21 entities, 11 attributes and 37 relations.All annotators trained on a small, common portion of the corpus before proceeding independently.An automatic tool was used to produce entity and attribute pre-annotations.About a tenth of the corpus was doubly annotated and annotation differences were resolved in consensus meetings.To ensure annotation consistency throughout the corpus, we devised harmonization tools to automatically identify annotation differences to be addressed to improve the overall corpus quality.The annotation project spanned over 24 months and resulted in a corpus comprising 500 documents (148,476 tokens) annotated with 44,740 entities and 26,478 relations.The average inter-annotator agreement is 0.793 F-measure for entities and 0.789 for relations.The performance of the pre-annotation tool for entities reached 0.814 F-measure when sufficient training data was available.The performance of our entity pre-annotation tool shows the value of the corpus to build and evaluate information extraction methods.In addition, we introduced harmonization methods that further improved the quality of annotations in the corpus.',\n",
       "              'mt': \"Un corpus clinique français avec des annotations sémantiques complètes : développement de l'entité médicale et de la relation LIMSI annOtated Text corpus (MERLOT)Des ressources annotées de qualité sont essentielles au traitement du langage naturel.L'objectif de ce travail est de présenter un corpus de récits cliniques en français annoté pour l'information linguistique, sémantique et structurelle, visant l'extraction d'information clinique.Six annotateurs ont contribué à l'annotation du corpus, en utilisant un schéma d'annotation complet couvrant 21 entités, 11 attributs et 37 relations.Tous les annotateurs se sont entraînés sur une petite partie commune du corpus avant de procéder indépendamment.Un outil automatique a été utilisé pour produire des pré-annotations d'entité et d'attribut.Environ un dixième du corpus a été doublement annoté et les différences d'annotation ont été résolues lors des réunions de consensus.Pour assurer la cohérence des annotations dans l'ensemble du corpus, nous avons conçu des outils d'harmonisation pour identifier automatiquement les différences d'annotation à traiter afin d'améliorer la qualité globale du corpus.Le projet d'annotation s'étendait sur 24 mois et aboutissait à un corpus comprenant 500 documents (148 476 jetons) annotés avec 44 740 entités et 26 478 relations.L'accord inter-annotateurs moyen est de 0,793 F-mesure pour les entités et de 0,789 pour les relations.Le rendement de l'outil de pré-annotation pour les entités a atteint 0,814 F-mesure lorsque suffisamment de données de formation étaient disponibles.La performance de notre outil de pré-annotation d'entité montre la valeur du corpus pour construire et évaluer des méthodes d'extraction d'informations.En outre, nous avons introduit des méthodes d'harmonisation qui ont amélioré la qualité des annotations dans le corpus.\",\n",
       "              'ref': \"Un corpus clinique du français annoté sémantiquement : développement du corpus MERLOT (Medical Entity and Relation LIMSI annOtated Text)La disponibilité de ressources annotées de qualité est essentielle pour le traitement automatique des langues.L'objectif de ce travail est de présenter un corpus de documents cliniques en français annoté avec des informations linguistique, sémantique et structurelle, pour l'extraction d'information clinique.Six annotateurs ont contribué à l'annotation du corpus, en utilisant un schéma d'annotation complet couvrant 21 entités, 11 attributs et 37 relations.Tous les annotateurs ont bénéficié d'une formation sur une petite partie commune du corpus avant de travailler indépendamment.Un outil automatique a été utilisé pour produire des pré-annotations en entités et attributs.Environ un dixième du corpus a été annoté en double et les divergences d'annotation ont été résolues lors de réunions de consensus.Pour assurer la cohérence des annotations dans l'ensemble du corpus, nous avons conçu des outils d'harmonisation pour identifier automatiquement les divergences d'annotation à traiter afin d'améliorer la qualité globale du corpus.Le projet d'annotation s'est déroulé sur 24 mois et a permis d'obtenir un corpus de 500 documents (148 476 tokens) annotés avec 44 740 entités et 26 478 relations.L'accord inter-annotateur moyen est de 0,793 F-mesure pour les entités et de 0,789 pour les relations.La performance de l'outil de pré-annotation pour les entités a atteint 0,814 de F-mesure lorsque suffisamment de données d'entrainement ont été disponibles.La performance de notre outil de pré-annotation en entités montre l'interêt du corpus pour le développement et l'évaluation de méthodes d'extraction d'information.En outre, nous avons introduit des méthodes d'harmonisation qui ont amélioré la qualité des annotations dans le corpus.\"},\n",
       "             500: {'src': 'The ETAPE Speech Processing EvaluationThe ETAPE evaluation is the third evaluation in automatic speech recognition and associated technologies in a series which started with ESTER.This evaluation proposed some new challenges, by proposing TV and radio shows with prepared and spontaneous speech, annotation and evaluation of overlapping speech, a cross-show condition in speaker diarization, and new, complex but very informative named entities in the information extraction task.This paper presents the whole campaign, including the data annotated, the metrics used and the anonymized system results.All the data created in the evaluation, hopefully including system outputs, will be distributed through the ELRA catalogue in the future.',\n",
       "              'mt': \"Évaluation du traitement de la parole ETAPEL'évaluation ETAPE est la troisième évaluation en reconnaissance vocale automatique et technologies associées dans une série qui a commencé avec ESTER.Cette évaluation a proposé de nouveaux défis, en proposant des émissions de télévision et de radio avec une parole préparée et spontanée, l'annotation et l'évaluation de la parole se chevauchant, une condition de cross-show dans la diarisation des locuteurs, et de nouvelles entités nommées complexes mais très informatives dans la tâche d'extraction d'informations.Cet article présente l'ensemble de la campagne, y compris les données annotées, les métriques utilisées et les résultats anonymisés du système.Toutes les données créées dans le cadre de l'évaluation, y compris, espérons-le, les résultats du système, seront distribuées dans le catalogue ELRA à l'avenir.\",\n",
       "              'ref': \"Évaluation du traitement de la parole ETAPEL'évaluation ETAPE est la troisième évaluation en reconnaissance vocale automatique et technologies associées en série qui a commencé avec ESTER.Cette évaluation a proposé de nouveaux défis, en proposant des émissions de télévision et de radio avec une parole préparée et spontanée, l'annotation et l'évaluation de la parole se chevauchant, une condition de cross-show dans la diarisation des locuteurs, et de nouvelles entités nommées complexes mais très informatives dans la tâche d'extraction d'informations.Cet article présente l'ensemble de la campagne, y compris les données annotées, les métriques utilisées et les résultats anonymisés du système.Toutes les données créées dans le cadre de l'évaluation, y compris, espérons-le, les résultats du système, seront distribuées via le catalogue ELRA à l'avenir.\"},\n",
       "             156: {'src': 'Creating a Corpus for Russian Data-to-Text Generation Using Neural Machine Translation and Post-EditingIn this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model.An error analysis of the output of an English-to-Russian neural machine translation system shows that 80% of the automatically translated sentences contain an error and that 53% of all translation errors bear on named entities (NE).We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs.',\n",
       "              'mt': 'Création d’un Corpus pour la génération russe de données à texte à l’aide de la traduction automatique neuronale et de la post-éditionDans cet article, nous proposons une approche pour la création semi-automatique d’un corpus data-to-text (D2T) pour le russe qui peut être utilisé pour apprendre un modèle de génération de langage naturel D2T.Une analyse d’erreur de la sortie d’un système de traduction automatique de neurones vers l’anglais montre que 80\\xa0% des phrases traduites automatiquement contiennent une erreur et que 53\\xa0% de toutes les erreurs de traduction portent sur des entités nommées (NE).Nous nous concentrons donc sur les entités nommées et introduisons deux techniques de post-édition pour corriger les NE mal traduites.',\n",
       "              'ref': \"Création d’un Corpus pour la génération data-to-text en russe par traduction automatique neuronale et post-éditionDans cet article, nous proposons une approche pour la création semi-automatique d’un corpus data-to-text (D2T) pour le russe qui peut servir à l'entrainement d'un modèle de génération de langage naturel D2T.Une analyse d’erreur de la sortie d’un système de traduction automatique neuronale anglais--&gt;russe montre que 80\\xa0% des phrases traduites automatiquement contiennent une erreur et que 53\\xa0% de toutes les erreurs de traduction portent sur des entités nommées (EN).Nous nous concentrons donc sur les entités nommées et introduisons deux techniques de post-édition pour corriger les EN mal traduites.\"},\n",
       "             385: {'src': 'Evaluate on-the-job learning dialogue systems and a case study for natural language understandingOn-the-job learning consists in continuously learning while being used in production, in an open environment, meaning that the system has to deal on its own with situations and elements never seen before.The kind of systems that seem to be especially adapted to on-the-job learning are dialogue systems, since they can take advantage of their interactions with users to collect feedback to adapt and improve their components over time.Some dialogue systems performing on-the-job learning have been built and evaluated but no general methodology has yet been defined.Thus in this paper, we propose a first general methodology for evaluating on-the-job learning dialogue systems.We also describe a task-oriented dialogue system which improves on-the-job its natural language component through its user interactions.We finally evaluate our system with the described methodology.',\n",
       "              'mt': \"Évaluer les systèmes de dialogue pour l'apprentissage en cours d'emploi et une étude de cas pour la compréhension du langage naturelL'apprentissage sur le tas consiste à apprendre en permanence tout en étant utilisé en production, dans un environnement ouvert, ce qui signifie que le système doit faire face de lui-même à des situations et à des éléments qu'il n'a jamais vus auparavant.Les systèmes qui semblent particulièrement adaptés à l'apprentissage sur le tas sont les systèmes de dialogue, car ils peuvent tirer parti de leurs interactions avec les utilisateurs pour recueillir des informations en retour afin d'adapter et d'améliorer leurs composants au fil du temps.Certains systèmes de dialogue permettant l'apprentissage en cours d'emploi ont été construits et évalués, mais aucune méthodologie générale n'a encore été définie.Dans cet article, nous proposons donc une première méthodologie générale pour l'évaluation des systèmes de dialogue permettant l'apprentissage en cours d'emploi.Nous décrivons également un système de dialogue axé sur les tâches qui améliore en cours d'emploi sa composante de langage naturel par le biais des interactions avec l'utilisateur.Enfin, nous évaluons notre système à l'aide de la méthodologie décrite.\",\n",
       "              'ref': \"Évaluer les systèmes de dialogue apprenant sur le terrain et une étude de cas pour la compréhension de la langueL'apprentissage sur le terrain consiste à apprendre en permanence tout en étant utilisé en production, dans un environnement ouvert, ce qui signifie que le système doit faire face de lui-même à des situations et à des éléments qu'il n'a jamais vus auparavant.Les systèmes qui semblent particulièrement adaptés à l'apprentissage sur le terrain sont les systèmes de dialogue, car ils peuvent tirer parti de leurs interactions avec les utilisateurs pour recueillir des informations en retour afin d'adapter et d'améliorer leurs composants au fil du temps.Certains systèmes de dialogue permettant l'apprentissage en cours d'emploi ont été construits et évalués, mais aucune méthodologie générale n'a encore été définie.Dans cet article, nous proposons donc une première méthodologie générale pour l'évaluation des systèmes de dialogue apprenant sur le terrain.Nous décrivons également un système de dialogue orienté tâches qui améliore son composant compréhension de la langue au fil du temps par le biais des interactions avec l'utilisateur.Enfin, nous évaluons notre système à l'aide de la méthodologie décrite.\"},\n",
       "             142: {'src': 'Word Representations in Factored Neural Machine TranslationTranslation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena, which is a challenge for neural machine translation architectures.To address this issue, the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words.Within this framework, we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors.We report experiments for translation from English into two morphologically rich languages, Czech and Latvian, and show the importance of explicitly modeling target morphology.',\n",
       "              'mt': 'Représentations de mots dans la traduction automatique neuronale factoriséeLa traduction dans un langage riche en morphologie nécessite un vocabulaire de sortie important pour modéliser divers phénomènes morphologiques, ce qui est un défi pour les architectures de traduction automatique neuronale.Pour résoudre ce problème, le présent document examine l’impact d’avoir deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d’informations morpho-syntactiques sur les deux facteurs.Nous rapportons des expériences pour la traduction de l’anglais en deux langues morphologiquement riches, le tchèque et le letton, et montrons l’importance de la modélisation explicite de la morphologie cible.',\n",
       "              'ref': \"Représentations de mots dans la traduction automatique neuronale factoriséeLa traduction dans un langage riche au plan morphologique nécessite un vocabulaire de sortie étendu pour modéliser la diversité des phénomènes morphologiques, ce qui est un défi pour les architectures de traduction automatique neuronale.Pour résoudre ce problème, le présent document examine l’impact qu'entraîne le fait d’avoir deux facteurs de sortie au sein d'un système capable de générer séparément deux représentations distinctes des mots cibles.Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d’informations morphosyntaxiques sur les deux facteurs.Nous rapportons les résultats d'expériences en traduction de l’anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l’importance de la modélisation explicite de la morphologie cible.\"},\n",
       "             308: {'src': 'Evaluation of a Sequence Tagging Tool for Biomedical TextsMany applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis.To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging.Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French.Tofurther characterize performance, we report distributions over 30 runs and different sizes of training datasets.YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81).We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.',\n",
       "              'mt': \"Évaluation d'un outil de marquage de séquences pour les textes biomédicauxDe nombreuses applications dans le traitement du langage naturel biomédical s'appuient sur l'étiquetage des séquences comme étape initiale pour effectuer des analyses plus complexes.Pour soutenir l'analyse de texte dans le domaine biomédical, nous présentons Yet Another SEquence Tagger (YASET), un étiqueteur de séquences polyvalent open-source qui met en œuvre des algorithmes d'apprentissage profond de pointe pour l'étiquetage de séquences.Ici, nous évaluons YASET sur l'étiquetage de la partie du discours et la reconnaissance des entités nommées dans une variété de genres de textes, y compris des articles de la littérature biomédicale en anglais et des récits cliniques en français.Pour mieux caractériser les performances, nous rapportons les distributions sur 30 exécutions et différentes tailles d'ensembles de données d'entraînement.YASET fournit des performances de pointe sur l'ensemble de données NER CoNLL 2003 (F1=0,87), le corpus MEDPOST (F1=0,97), le corpus MERLoT (F1=0,99) et le corpus de maladies NCBI (F1=0,81).Nous pensons que YASET est un outil polyvalent et efficace qui peut être utilisé pour le marquage de séquences dans les textes biomédicaux et cliniques.\",\n",
       "              'ref': \"Évaluation d'un outil d'étiquetage de séquences pour les textes biomédicauxDe nombreuses applications dans le traitement automatique de la langue biomédicale s'appuient sur l'étiquetage de séquences comme étape initiale pour effectuer des analyses plus complexes.Pour soutenir l'analyse de textes dans le domaine biomédical, nous présentons Yet Another SEquence Tagger (YASET), un étiqueteur de séquences polyvalent open-source qui met en œuvre des algorithmes d'apprentissage profond de pointe pour l'étiquetage de séquences.Dans cet article, nous évaluons YASET sur l'étiquetage morphosyntaxique et la reconnaissance d'entités nommées dans une variété de corpus, dont des articles de la littérature biomédicale en anglais et des documents cliniques en français.Pour mieux caractériser les performances de l'outil, nous rapportons les distributions sur 30 itérations et différentes tailles de corpus d'entraînement.YASET offre des performances à l'état de l'art sur le corpus CoNLL 2003 (F1=0,87), le corpus MEDPOST (F1=0,97), le corpus MERLoT (F1=0,99) et le corpus NCBI diseases (F1=0,81).Nous pensons que YASET est un outil polyvalent et efficace qui peut être utilisé pour l'étiquetage de séquences dans les textes biomédicaux et cliniques.\"},\n",
       "             138: {'src': 'Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languagesOut-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist.Our work adapts variants of seq2seq models to perform transduction of such words from Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs built from a bilingual dictionary of Hindi - Bhojpuri words.We demonstrate that our models can be effectively used for language pairs that have limited parallel corpora; our models work at the character level to grasp phonetic and orthographic similarities across multiple types of word adaptations, whether synchronic or diachronic, loan words or cognates.We describe the training aspects of several character level NMT systems that we adapted to this task and characterize their typical errors.Our method improves BLEU score by 6.3 on the Hindi-to-Bhojpuri translation task.Further, we show that such transductions can generalize well to other languages by applying it successfully to Hindi - Bangla cognate pairs.Our work can be seen as an important step in the process of: (i) resolving the OOV words problem arising in MT tasks; (ii) creating effective parallel corpora for resource constrained languages; and (iii) leveraging the enhanced semantic knowledge captured by word-level embeddings to perform character-level tasks.',\n",
       "              'mt': \"Apprentissage des adaptations phonologiques et orthographiques interlangues : une étude de cas pour améliorer la traduction automatique neuronale entre des langues à faibles ressourcesLes mots hors vocabulaire (OOV) peuvent poser de sérieux problèmes pour les tâches de traduction automatique (TA), et en particulier pour les paires de langues à faibles ressources (LRL), c'est-à-dire les paires de langues pour lesquelles il n'existe que peu ou pas de corpus parallèles.Notre travail adapte des variantes de modèles seq2seq pour effectuer la transduction de tels mots de l'hindi au bhojpuri (une instance LRL), en apprenant à partir d'un ensemble de paires cognées construites à partir d'un dictionnaire bilingue de mots hindi - bhojpuri.Nous démontrons que nos modèles peuvent être utilisés efficacement pour les paires de langues qui ont des corpus parallèles limités ; nos modèles fonctionnent au niveau des caractères pour saisir les similitudes phonétiques et orthographiques à travers de multiples types d'adaptations de mots, qu'elles soient synchroniques ou diachroniques, des mots d'emprunt ou des cognats.Nous décrivons les aspects de formation de plusieurs systèmes de NMT au niveau du caractère que nous avons adaptés à cette tâche et caractérisons leurs erreurs typiques.Notre méthode améliore le score BLEU de 6,3 sur la tâche de traduction Hindi-Bhojpuri.En outre, nous montrons que de telles transductions peuvent être généralisées à d'autres langues en l'appliquant avec succès à des paires de cognats hindi-bangla.Notre travail peut être considéré comme une étape importante dans le processus de : (i) la résolution du problème des mots OOV dans les tâches de traduction automatique ; (ii) la création de corpus parallèles efficaces pour les langues à ressources limitées ; et (iii) l'exploitation de la connaissance sémantique améliorée capturée par les enchâssements au niveau des mots pour effectuer des tâches au niveau des caractères.\",\n",
       "              'ref': \"Apprentissage des adaptations phonologiques et orthographiques interlingues : une étude de cas visant à l'amélioration de la traduction automatique neuronale entre langues peu dotéesLes mots hors vocabulaire (HV) peuvent poser de sérieux problèmes pour les tâches de traduction automatique (TA), et en particulier pour les paires de langues peu dotées (LPD), c'est-à-dire les paires de langues pour lesquelles il n'existe que peu ou pas de corpus parallèles.Notre travail adapte des variantes de modèles seq2seq pour effectuer la transduction de tels mots de l'hindi vers le bhodjpouri (un exemple de LPD), en apprenant à partir d'un ensemble de paires de cognats constituées au départ d'un dictionnaire bilingue hindi - bhodjpouri.Nous démontrons que nos modèles peuvent être utilisés efficacement pour les paires de langues ayant des corpus parallèles limités ; nos modèles fonctionnent à l'échelle des caractères pour saisir les similitudes phonétiques et orthographiques au travers de multiples types d'adaptations de mots, qu'elles soient synchroniques ou diachroniques, portent sur des mots d'emprunt ou des cognats.Nous décrivons les modalités d'entraînement de plusieurs systèmes de TA neuronale à l'échelle du caractère que nous avons adaptés à cette tâche et caractérisons leurs erreurs typiques.Notre méthode améliore le score BLEU de 6,3 sur la tâche de traduction hindi-bhodjpouri.En outre, nous montrons que de telles transductions peuvent être généralisées à d'autres langues en les appliquant avec succès à des paires de cognats hindi-bengali.Notre travail peut être considéré comme une étape importante dans le processus de : (i) résolution du problème des mots HV dans les tâches de traduction automatique ; (ii) création de corpus parallèles utiles pour des langues peu dotées ; et (iii) d'exploitation améliorée des propriétés sémantiques capturées par les plongements de mots pour effectuer des tâches à l'échelle des caractères.\"},\n",
       "             330: {'src': \"Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word.It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations.Previous work has applied machine translation (MT) techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters.In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT.We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup.We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality.Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task.In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.\",\n",
       "              'mt': \"La prévision apparentée peut-elle être modélisée en tant que tâche de traduction automatique à faibles ressources ?La prédiction des mots apparentés est la tâche de générer, dans une langue donnée, les mots apparentés probables dans une langue apparentée, où les mots apparentés sont des mots dans des langues apparentées qui ont évolué à partir d'un mot ancêtre commun.C'est une tâche pour laquelle peu de données existent et qui peut aider les linguistes dans la découverte de relations précédemment non découvertes.Les travaux précédents ont appliqué des techniques de traduction automatique (MT) à cette tâche, en fonction des similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyper-paramètres.Dans cet article, nous examinons si la prédiction congénitale peut bénéficier des informations provenant de MT à faibles ressources.Nous comparons d'abord les architectures MT statistiques (SMT) et MT neuronales (NMT) dans une configuration bilingue.Nous étudions ensuite l'impact de l'utilisation de techniques d'augmentation des données communément vues pour donner des gains en MT à faibles ressources : préformation monolingue, rétrotraduction et multilinguisme.Nos expériences sur plusieurs langages romains montrent que la prédiction apparentée ne se comporte que dans une certaine mesure comme une tâche MT standard à faibles ressources.En particulier, les architectures MT, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l'utilisation de données monolingues supplémentaires n'est pas toujours aussi bénéfique que l'utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour MT.\",\n",
       "              'ref': \"La prédiction de cognats peut-elle être modélisée comme une tâche de traduction automatique peu dotée?La prédiction de cognats consiste à générer, dans une langue donnée, les cognats probables dans une langue apparentée - nous désignons par cognats les mots dans des langues proches qui ont évolué à partir d'un mot ancêtre commun.C'est une tâche pour laquelle peu de données existent et qui peut aider les linguistes dans la découverte de relations précédemment non découvertes.Les travaux précédents ont appliqué des techniques de traduction automatique (TA) à cette tâche, en s'appuyant sur la similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyper-paramètres.Dans cet article, nous évaluons dans quelle mesure la prédiction de cognats peut bénéficier des méthodes de traduction automatique dans un contexte peu doté.Nous comparons d'abord les architectures de TA statistiques (SMT) et neuronales (NMT) dans une configuration bilingue.Nous étudions ensuite l'impact de techniques d'augmentation de données connues pour apporter une contribution en traduction automatique peu dotée : pré-entrainement monolingue, rétrotraduction et multilinguisme.Nos expériences sur plusieurs langues romanes montrent que la prédiction de cognats ne se comporte que dans une certaine mesure comme une tâche standard de TA peu dotée.En particulier, les architectures de TA, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l'utilisation de données monolingues supplémentaires n'est pas toujours aussi bénéfique que l'utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la TA.\"},\n",
       "             336: {'src': 'Findings of the WMT 2022 Biomedical Translation Shared Task: Monolingual Clinical Case ReportsIn the seventh edition of the WMT Biomedical Task, we addressed a total of seven language pairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian.This year’s test sets covered three types of biomedical text genre.In addition to scientific abstracts and terminology items used in previ- ous editions, we released test sets of clinical cases.The evaluation of clinical cases translations were given special attention by involving clinicians in the preparation of reference translations and manual evaluation.For the main MEDLINE test sets, we received a total of 609 submissions from 37 teams.For the ClinSpEn sub-task, we had the participation of five teams.',\n",
       "              'mt': 'Conclusions de la Tâche partagée de traduction biomédicale WMT 2022: Rapports de cas cliniques monolinguesDans la septième édition de la tâche biomédicale WMT, nous avons abordé un total de sept paires de langues, à savoir l’anglais/allemand, l’anglais/français, l’anglais/espagnol, l’anglais/portugais, l’anglais/chinois, l’anglais/russe, l’anglais/italien.Les ensembles d’essais de cette année couvraient trois types de textes biomédicaux.En plus des résumés scientifiques et des éléments terminologiques utilisés dans les éditions previ- ous, nous avons publié des séries de tests de cas cliniques.L’évaluation des traductions de cas cliniques a fait l’objet d’une attention particulière grâce à la participation de cliniciens à la préparation des traductions de référence et à l’évaluation manuelle.Pour les principaux ensembles de tests MEDLINE, nous avons reçu un total de 609 soumissions de 37 équipes.Pour la sous-tâche ClinSpEn, nous avons eu la participation de cinq équipes.',\n",
       "              'ref': \"Conclusions de la tâche partagée de traduction biomédicale WMT 2022: Rapports de cas cliniques monolinguesDans la septième édition de la tâche biomédicale WMT, nous avons abordé un total de sept paires de langues, à savoir anglais/allemand, anglais/français, anglais/espagnol, anglais/portugais, anglais/chinois, anglais/russe, anglais/italien.Les corpus proposés cette année couvraient trois types de textes biomédicaux.En plus des résumés scientifiques et des terminologies utilisés dans les éditions précédentes, nous avons également proposé des corpus de tests de cas cliniques.L’évaluation des traductions de cas cliniques a fait l’objet d’une attention particulière grâce à la participation de cliniciens à l'élaboration des traductions de référence et à l’évaluation manuelle.Pour les corpus de tests MEDLINE (tâche principale), nous avons reçu un total de 609 soumissions de 37 équipes.Pour la sous-tâche ClinSpEn, nous avons eu la participation de cinq équipes.\"},\n",
       "             339: {'src': 'Neural architecture for temporal relation extraction: A Bi-LSTM approach for detecting narrative containersWe present a neural architecture for containment relation identification between medical events and/or temporal expressions.We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus.Our model achieves an F-measure of 0.613 and outperforms the best result reported on this corpus to date.',\n",
       "              'mt': \"Architecture neuronale pour l'extraction de relations temporelles : Approche bi-LSTM pour détecter des conteneurs narratifsL'invention concerne une architecture neurale permettant d'identifier une relation de confinement entre des événements médicaux et/ou des expressions temporelles.Nous expérimentons sur un corpus de notes cliniques anonymisées en anglais de la Clinique Mayo, à savoir le corpus THYME.Notre modèle atteint une mesure F de 0,613 et surpasse le meilleur résultat rapporté sur ce corpus à ce jour.\",\n",
       "              'ref': \"Architecture neuronale pour l'extraction de relations temporelles : Approche bi-LSTM pour détecter des conteneurs narratifsNous présentons une architecture neurale permettant d'identifier des relations de containement entre des événements médicaux et/ou des expressions temporelles.Nous présentons des expériences sur un corpus de documents cliniques anonymisés en anglais issus de la Mayo Clinic , à savoir le corpus THYME.Notre modèle offre une F-mesure de 0,613 et surpasse le meilleur résultat rapporté sur ce corpus à ce jour.\"},\n",
       "             333: {'src': 'Correcting and Validating Syntactic Dependency in the Spoken French Treebank RhapsodieThis article presents the methods, results, and precision of the syntactic annotation process of the Rhapsodie Treebank of spoken French.The Rhapsodie Treebank is an 33,000 word corpus annotated for prosody and syntax, licensed in its entirety under Creative Commons.The syntactic annotation contains two levels: a macro-syntactic level, containing a segmentation into illocutionary units (including discourse markers, parentheses ...) and a micro-syntactic level including dependency relations and various paradigmatic structures, called pile constructions, the latter being particularly frequent and diverse in spoken language.The micro-syntactic annotation process, presented in this paper, includes a semi-automatic preparation of the transcription, the application of a syntactic dependency parser, transcoding of the parsing results to the Rhapsodie annotation scheme, manual correction by multiple annotators followed by a validation process, and finally the application of coherence rules that check common errors.The good inter-annotator agreement scores are presented and analyzed in greater detail.The article also includes the list of functions used in the dependency annotation and for the distinction of various pile constructions and presents the ideas underlying these choices.',\n",
       "              'mt': 'Correction et validation de la dépendance syntaxique dans le Rhapsodie de la banque d’arbres française parléeCet article présente les méthodes, les résultats et la précision du processus d’annotation syntaxique de la Rhapsodie Treebank de français parlé.Le Rhapsodie Treebank est un corpus de 33\\xa0000 mots annoté pour la prosodie et la syntaxe, sous licence dans son intégralité sous Creative Commons.L’annotation syntaxique contient deux niveaux: un niveau macro-syntactique, contenant une segmentation en unités illocutionnaires (y compris des marqueurs de discours, des parenthèses...) et un niveau micro-syntactique comprenant des relations de dépendance et diverses structures paradigmatiques, appelées constructions de pile, ces dernières étant particulièrement fréquentes et diversifiées dans le langage parlé.Le processus d’annotation micro-syntactique, présenté dans cet article, comprend une préparation semi-automatique de la transcription, l’application d’un analyseur de dépendance syntaxique, le transcodage des résultats d’analyse au schéma d’annotation Rhapsodie, la correction manuelle par plusieurs annotateurs suivie d’un processus de validation, et enfin l’application de règles de cohérence qui vérifient les erreurs courantes.Les bons scores d’accord inter-annotateurs sont présentés et analysés plus en détail.L’article comprend également la liste des fonctions utilisées dans l’annotation de dépendance et pour la distinction des différentes constructions de pieux et présente les idées qui sous-tendent ces choix.',\n",
       "              'ref': \"Correction et validation de la dépendance syntaxique dans Rhapsodie, le corpus arboré du français parléCet article présente les méthodes, les résultats et la précision du processus d’annotation syntaxique de Rhapsodie, la corpus arboré du français parlé.Le corpus Rhapsodie est un corpus de 33\\xa0000 mots annoté pour la prosodie et la syntaxe, publié entièrement sous licence Creative Commons.L’annotation syntaxique contient deux niveaux : un niveau macro-syntaxique, contenant une segmentation en unités illocutoires (y compris des marqueurs de discours, des parenthèses...) et un niveau micro-syntaxique comprenant des relations de dépendance et diverses structures paradigmatiques, appelées constructions de pile, ces dernières étant particulièrement fréquentes et diverses dans le langage parlé.Le processus d’annotation micro-syntaxique, présenté dans cet article, comprend une préparation semi-automatique de la transcription, l’application d’un analyseur en dépendance syntaxique, le transcodage des résultats de l'analyse syntaxique au schéma d’annotation Rhapsodie, la correction manuelle par plusieurs annotateurs suivie d’un processus de validation, et enfin l’application de règles de cohérence qui vérifient les erreurs courantes.Les bons scores d’accord inter-annotateur sont présentés et analysés plus en détail.L’article comprend également la liste des fonctions utilisées dans l’annotation en dépendances et pour la distinction des différentes constructions des piles et présente les idées qui sous-jacentes à ces choix.\"},\n",
       "             498: {'src': \"Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word.It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations.Previous work has applied machine translation (MT) techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters.In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT.We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup.We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality.Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task.In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.\",\n",
       "              'mt': 'La prédiction Cognate peut-elle être modélisée comme une tâche de traduction automatique à faible ressource?La prédiction cognate est la tâche de générer, dans une langue donnée, les cognats probables des mots dans une langue apparentée, où les cognates sont des mots dans des langues apparentées qui ont évolué à partir d’un mot ancêtre commun.C’est une tâche pour laquelle il existe peu de données et qui peut aider les linguistes à découvrir des relations inédites.Des travaux antérieurs ont appliqué des techniques de traduction automatique (MT) à cette tâche, basées sur les similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyperparamètres.Dans cet article, nous étudions si la prédiction cognitive peut tirer profit des connaissances de MT à faibles ressources.Nous comparons d’abord les architectures MT (SMT) statistiques (SMT) et neuronales (MTN) dans une configuration bilingue.Nous étudions ensuite l’impact de l’utilisation de techniques d’augmentation des données communément observées pour générer des gains en MT à faibles ressources: préformation monolingue, rétrotraduction et multilinguité.Nos expériences sur plusieurs langues romanes montrent que la prédiction cognitive ne se comporte que dans une certaine mesure comme une tâche MT standard à faible ressource.En particulier, les architectures MT, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l’utilisation de données monolingues supplémentaires n’est pas toujours aussi bénéfique que l’utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la MT.',\n",
       "              'ref': 'La prédiction Cognate peut-elle être modélisée comme une tâche de traduction automatique à faibles ressources ?La prédiction cognate est la tâche de générer, dans une langue donnée, les cognats probables des mots dans une langue apparentée, où les cognates sont des mots dans des langues apparentées qui ont évolué à partir d’un mot ancêtre commun.C’est une tâche pour laquelle il existe peu de données et qui peut aider les linguistes à découvrir des relations inédites.Des travaux antérieurs ont appliqué des techniques de traduction automatique (MT) à cette tâche, basées sur les similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyperparamètres.Dans cet article, nous étudions si la prédiction cognate peut tirer profit des connaissances de MT à faibles ressources.Nous comparons d’abord les architectures MT (SMT) statistiques (SMT) et neuronales (NMT) dans une configuration bilingue.Nous étudions ensuite l’impact de l’utilisation de techniques d’augmentation des données communément observées pour générer des gains en MT à faibles ressources : pré-entraînement monolingue, rétrotraduction et multilinguité.Nos expériences sur plusieurs langues romanes montrent que la prédiction cognate ne se comporte que dans une certaine mesure comme une tâche MT standard à faibles ressources.En particulier, les architectures MT, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l’utilisation de données monolingues supplémentaires n’est pas toujours aussi bénéfique que l’utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la MT.'},\n",
       "             157: {'src': 'Two Multilingual Corpora Extracted from the Tenders Electronic Daily for Machine Learning and Machine Translation ApplicationsThe European \"Tenders Electronic Daily\" (TED) is a large source of semi-structured and multilingual data that is very valuable to the Natural Language Processing community.This data sets can effectively be used to address complex machine translation, multilingual terminology extraction, text-mining, or to benchmark information retrieval systems.Despite of the services offered by the user-friendliness of the web site that is made available to the public to access the publishing of the EU call for tenders, collecting and managing such kind of data is a great burden and consumes a lot of time and computing resources.This could explain why such a resource is not very (if any) exploited today by computer scientists or engineers in NLP.The aim of this paper is to describe two documented and easy-to-use multilingual corpora (one of them is a parallel corpus), extracted from the TED web source that we will release for the benefit of the NLP community.',\n",
       "              'mt': 'Deux corpus multilingues extraits du Tenders Electronic Daily pour des applications d\\'apprentissage et de traduction automatiquesLe \"Tenders Electronic Daily\" (TED) européen est une source importante de données semi-structurées et multilingues très précieuses pour la communauté du traitement du langage naturel.Ces ensembles de données peuvent être utilisés efficacement pour la traduction automatique complexe, l\\'extraction de terminologie multilingue, l\\'exploration de texte ou l\\'évaluation des systèmes de recherche d\\'informations.Malgré les services offerts par la convivialité du site web mis à la disposition du public pour accéder à la publication des appels d\\'offres de l\\'UE, la collecte et la gestion de ce type de données représentent une charge importante et consomment beaucoup de temps et de ressources informatiques.Cela pourrait expliquer pourquoi une telle ressource n\\'est pas (ou très peu) exploitée aujourd\\'hui par les informaticiens ou les ingénieurs en TAL.L\\'objectif de cet article est de décrire deux corpus multilingues documentés et faciles à utiliser (l\\'un d\\'entre eux est un corpus parallèle), extraits de la source web TED, que nous mettrons à la disposition de la communauté du TAL.',\n",
       "              'ref': \"Deux corpus multilingues extraits du Tenders Electronic Daily pour des applications d'apprentissage et de traduction automatiquesLe &quot;Tenders Electronic Daily&quot; (TED) européen est une source abondante de données semi-structurées et multilingues très précieuses pour la communauté du traitement du langage naturel.Ces jeux de données peuvent être mis à profit dans des applications avancées de la traduction automatique, pour l'extraction de terminologies multilingues, pour la fouille de texte ou encore pour l'évaluation de systèmes de recherche d'informations.En dépit de la convivialité du site web mis à la disposition du public pour accéder aux appels d'offres publiés par l'UE, la collecte et la gestion de ce type de données représentent une charge importante et sont coûteuses, tant en temps qu'en ressources informatiques.Cela pourrait expliquer pourquoi une telle ressource n'est pas (ou très peu) exploitée aujourd'hui par les informaticiens et les ingénieurs en TAL.L'objectif de cet article est de décrire deux corpus multilingues documentés et faciles à utiliser (l'un d'entre eux est un corpus parallèle), extraits de TED sur le Web, que nous mettrons à la disposition de la communauté du TAL.\"},\n",
       "             83: {'src': 'KPTimes: A Large-Scale Dataset for Keyphrase Generation on News DocumentsKeyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text.Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations.In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases.Exploring the dataset, we show how editors tag documents , and how their annotations differ from those found in existing datasets.We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain.The dataset is available online at https:// github.com/ygorg/KPTimes.',\n",
       "              'mt': \"KPTimes : Un ensemble de données à grande échelle pour la génération de phrases-clés sur des documents d'actualitéLa génération de phrases-clés consiste à prédire un ensemble d'unités lexicales qui traduisent le contenu principal d'un texte source.Les ensembles de données existants pour la génération de phrases-clés ne sont disponibles que pour le domaine universitaire et comprennent des annotations non expertes.Dans cet article, nous présentons KPTimes, un ensemble de données à grande échelle de textes d'actualité associés à des phrases-clés créées par des éditeurs.En explorant l'ensemble de données, nous montrons comment les éditeurs étiquettent les documents et comment leurs annotations diffèrent de celles trouvées dans les ensembles de données existants.Nous entraînons et évaluons également des modèles neuronaux de génération de phrases-clés de pointe sur KPTimes afin d'obtenir un aperçu de leurs performances dans le domaine des actualités.L'ensemble de données est disponible en ligne à l'adresse https:// github.com/ygorg/KPTimes.\",\n",
       "              'ref': \"KPTimes : Un large jeu de données pour la génération de mots-clés pour les documents journalistiquesLa génération de mots-clés consiste à prédire un ensemble d'unités lexicales qui décrivent le contenu principal d'un texte.Les jeux de données existants pour la génération de mots-clés sont limités au domaine scientifique et ne sont pas annotés pas des experts.Dans cet article, nous présentons KPTimes, un jeu de données de document journalistiques associés à des mots-clés validés par des éditeurs.En explorant le jeu de données, nous montrons comment les éditeurs étiquettent les documents et comment leurs annotations diffèrent de celles trouvées dans les jeux de données existants.Nous entraînons et évaluons également des modèles neuronaux de génération de mots-clés état de l'art sur KPTimes pour mieux comprendre leurs performances dans le domaine journalistique.Le jeu de données est disponible en ligne à l'adresse https:// github.com/ygorg/KPTimes.\"},\n",
       "             181: {'src': 'A Study of Residual Adapters for Multi-Domain Neural Machine TranslationDomain adaptation is an old and vexing problem for machine translation systems.The most common and successful approach to supervised adaptation is to fine-tune a baseline system with in-domain parallel data.Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting.A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the model unchanged.This has the additional merit to leave the baseline model intact and adaptable to multiple domains.In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task.We contrast multiple implementations of this idea using two language pairs.Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors.',\n",
       "              'mt': 'Une étude des adaptateurs résiduels pour la traduction automatique de neurones multi-domainesL’adaptation de domaine est un problème ancien et vexant pour les systèmes de traduction automatique.L’approche la plus courante et la plus réussie en matière d’adaptation supervisée consiste à affiner un système de base avec des données parallèles dans le domaine.Le réglage standard modifie cependant tous les paramètres du réseau, ce qui rend cette approche coûteuse et sujette à un surajustement.Une approche récente et légère augmente plutôt un modèle de base avec des couches d’adaptateur supplémentaires (petites), gardant le reste du modèle inchangé.Cela a le mérite supplémentaire de laisser le modèle de base intact et adaptable à plusieurs domaines.Dans cet article, nous effectuons une analyse approfondie du modèle d’adaptateur dans le cadre d’une tâche de traduction automatique multidomaine.Nous contrastons plusieurs implémentations de cette idée en utilisant deux paires de langages.Nos principales conclusions sont que les adaptateurs résiduels fournissent une méthode rapide et bon marché pour l’adaptation multidomaine supervisée; nos deux variantes s’avèrent aussi efficaces que le modèle d’adaptateur d’origine et une perspective ouverte pour rendre également les modèles adaptés plus robustes pour étiqueter les erreurs de domaine.',\n",
       "              'ref': \"Une étude des adaptateurs résiduels pour la traduction automatique neuronale multi-domainesL’adaptation au domaine est un problème ancien et frustrant pour les systèmes de traduction automatique.L’approche la plus courante et la plus efficace en matière d’adaptation supervisée consiste à affiner un système de base avec des données parallèles dans le domaine.L'affinage standard modifie cependant tous les paramètres du réseau, ce qui rend cette approche coûteuse et sujette au surapprentissage.Une approche récente et moins lourde augmente plutôt un modèle de base avec de (petites) couches d’adaptateurs supplémentaires, gardant le reste du modèle inchangé.Cela a le mérite supplémentaire de laisser le modèle de base intact et adaptable à plusieurs domaines.Dans cet article, nous effectuons une analyse approfondie du modèle adapté dans le cadre d’une tâche de traduction automatique multi-domaine.Nous mettons en contraste plusieurs implémentations de cette approche en utilisant deux paires de langages.Nos principales conclusions sont que les adaptateurs résiduels fournissent une méthode rapide et bon marché pour l’adaptation multi-domaine supervisée; nos deux variantes s’avèrent aussi efficaces que le modèle adapté d’origine et laissent également entrevoir la possibilité de rendre les modèles adaptés plus robustes aux erreurs d'étiquetage des domaines.\"},\n",
       "             171: {'src': \"Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word.It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations.Previous work has applied machine translation (MT) techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters.In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT.We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup.We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality.Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task.In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.\",\n",
       "              'mt': \"La prédiction de cognats peut-elle être modélisée comme une tâche de traduction automatique à faibles ressources ?La prédiction des cognats est la tâche consistant à générer, dans une langue donnée, les cognats probables des mots d'une langue apparentée, les cognats étant des mots de langues apparentées qui ont évolué à partir d'un mot ancêtre commun.Il s'agit d'une tâche pour laquelle il existe peu de données et qui peut aider les linguistes à découvrir des relations inédites.Des travaux antérieurs ont appliqué des techniques de traduction automatique (TA) à cette tâche, en se basant sur les similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyperparamètres.Dans cet article, nous cherchons à savoir si la prédiction des cognats peut bénéficier des enseignements de la TA à faibles ressources.Nous comparons d'abord les architectures de la TA statistique (SMT) et de la TA neuronale (NMT) dans un contexte bilingue.Nous étudions ensuite l'impact de l'utilisation de techniques d'augmentation des données couramment utilisées pour obtenir des gains dans la TA à faibles ressources : le pré-entraînement monolingue, la rétro-traduction et le multilinguisme.Nos expériences sur plusieurs langues romanes montrent que la prédiction de cognats ne se comporte que dans une certaine mesure comme une tâche standard de TA à faibles ressources.En particulier, les architectures de TA, tant statistiques que neuronales, peuvent être utilisées avec succès pour cette tâche, mais l'utilisation de données monolingues supplémentaires n'est pas toujours aussi bénéfique que l'utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la TA.\",\n",
       "              'ref': \"La prédiction de cognats peut-elle être modélisée comme une tâche de traduction automatique à faibles ressources ?La prédiction des cognats est la tâche consistant à générer, dans une langue donnée, les cognats probables des mots d'une langue apparentée, les cognats étant des mots de langues apparentées qui ont évolué à partir d'un mot ancêtre commun.Il s'agit d'une tâche pour laquelle il existe peu de données et qui peut aider les linguistes à découvrir des relations inédites.Des travaux antérieurs ont appliqué des techniques de traduction automatique (TA) à cette tâche, en se basant sur les similitudes entre ces tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyperparamètres.Dans cet article, nous cherchons à savoir si la prédiction des cognats peut bénéficier des enseignements de la TA à faibles ressources.Nous comparons d'abord les architectures de la TA statistique (TAS) et de la TA neuronale (TAN) dans un contexte bilingue.Nous étudions ensuite l'impact de l'utilisation de techniques d'augmentation des données couramment utilisées pour obtenir des gains de performance en TA à faibles ressources : le pré-entraînement monolingue, la rétro-traduction et le multilinguisme.Nos expériences sur plusieurs langues romanes montrent que la prédiction de cognats ne s'apparente que jusqu'à un certain point à une tâche standard de TA à faibles ressources.En particulier, les architectures de TA, tant statistiques que neuronales, peuvent être utilisées avec succès pour cette tâche, mais l'utilisation de données monolingues additionnelles n'est pas toujours aussi bénéfique que l'utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la TA.\"},\n",
       "             342: {'src': 'PrepLex: a Lexicon of French Prepositions for ParsingPrepLex is a lexicon of French prepositions which provides all the syntactic information needed for parsing.It was built by compar- ing and merging several authoritative lexical sources.This lexicon also includes infor- mation about the prepositions or classes of prepositions that appear in French verb sub- categorization frames.This resource has been developed as a ﬁrst step in making cur- rent French preposition lexicons available for effective natural language processing.',\n",
       "              'mt': 'Préplexe: un lexique des prépositions françaises pour l’analysePreplex est un lexique de prépositions françaises qui fournit toutes les informations syntaxiques nécessaires à l’analyse.Il a été construit en comparant et en fusionnant plusieurs sources lexicales faisant autorité.Ce lexique comprend également infor- mation sur les prépositions ou classes de prépositions qui apparaissent dans les cadres de sous-catégorisation des verbes français.Cette ressource a été développée comme une étape la plus importante dans la mise à disposition des lexiques de préposition française pour un traitement efficace du langage naturel.',\n",
       "              'ref': 'PrepLex: un lexique des prépositions françaises pour l’analyse syntaxiquePreplex est un lexique de prépositions françaises qui fournit toutes les informations syntaxiques nécessaires à l’analyse syntaxique.Il a été construit en comparant et en fusionnant plusieurs sources lexicales faisant autorité.Ce lexique comprend également informations sur les prépositions ou les classes de prépositions qui apparaissent dans les frames de sous-catégorisation des verbes français.Cette ressource a été développée pour amorcer la mise à disposition de lexiques de prépositions françaises pour un traitement efficace du langage naturel.'},\n",
       "             173: {'src': 'Some Reflections on the Interface between Professional Machine Translation Literacy and Data LiteracyDue to the widespread use of data-driven neural machine translation, both by professional translators and layperson users, an adequate machine translation literacy on the part of the users of this technology is becoming more and more important.At the same time, the increasing datafication of both the private and the business sphere requires an adequate data literacy in modern society.The present article takes a closer look at machine translation literacy and data literacy and investigates the interface between the two concepts.This is done to lay the preliminary theoretical foundations for a didactic project aiming to develop didactic resources for teaching data literacy in its machine translation-specific form to students of BA programmes in translation/specialised communication.',\n",
       "              'mt': 'Quelques réflexions sur l’interface entre l’alphabétisation professionnelle en traduction automatique et la littératie des donnéesEn raison de l’utilisation généralisée de la traduction automatique neuronale axée sur les données, tant par les traducteurs professionnels que par les utilisateurs profanes, une connaissance adéquate de la traduction automatique de la part des utilisateurs de cette technologie devient de plus en plus importante.Dans le même temps, l’augmentation de la datafication tant du secteur privé que du monde des affaires exige une connaissance adéquate des données dans la société moderne.Le présent article examine de plus près la littératie en traduction automatique et la littératie des données et étudie l’interface entre les deux concepts.Ceci est fait pour jeter les bases théoriques préliminaires d’un projet didactique visant à développer des ressources didactiques pour l’enseignement de la littératie des données sous sa forme spécifique à la traduction automatique aux étudiants des programmes de baccalauréat en traduction/communication spécialisée.',\n",
       "              'ref': \"Quelques réflexions portant sur l’intersection de la maîtrise d'outils de traduction automatique en milieu professionnel et la culture des donnéesEn raison de l’utilisation généralisée de la traduction automatique neuronale basée sur l'exploitation de données textuelles, tant par les traducteurs professionnels que par les utilisateurs profanes, une connaissance adéquate de la traduction automatique de la part des utilisateurs de cette technologie devient de plus en plus importante.Dans le même temps, la &quot;datafication&quot; croissante tant du secteur privé que du monde des affaires exige une connaissance adéquate des données dans la société moderne.Le présent article examine de plus près l'habileté en traduction automatique et l'habileté des données et étudie l’intersection de ces deux concepts.Ceci est fait pour jeter les bases théoriques préliminaires d’un projet didactique visant à développer des ressources pédagogiques pour l’enseignement de l'habileté des données sous sa forme spécifique à la traduction automatique aux étudiants des programmes de licence en traduction/communication spécialisée.\"},\n",
       "             401: {'src': 'Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning ArborescenceWe present a new method for the joint task of tagging and non-projective dependency parsing.We demonstrate its usefulness with an application to discontinu-ous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly.The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation.We evaluate this model and obtain state-of-the-art results despite strong independence assumptions.',\n",
       "              'mt': 'Efficace Discontinue-Structure Parsing via l’arborescence maximale généraliséeNous présentons une nouvelle méthode pour la tâche conjointe de marquage et d’analyse de dépendance non-projective.Nous démontrons son utilité avec une application à l’analyse discontinue de phrase-structure où le décodage des épines lexicalisées et des dérivations syntaxiques est effectué conjointement.Les principales contributions de cet article sont (1) une réduction du marquage conjoint et l’analyse de dépendance non-projective au problème de l’arborescence maximale généralisée, et (2) un nouvel algorithme de décodage pour ce problème grâce à la relaxation lagrangienne.Nous évaluons ce modèle et obtenons des résultats de pointe malgré de solides hypothèses d’indépendance.',\n",
       "              'ref': \"Analyse en constituants discontinus efficace via l'arborescence couvrante généralisée de poids maximalNous présentons une nouvelle méthode pour la tâche jointe d'étiquetage et d’analyse en dépendances non-projectives.Nous démontrons son utilité avec une application à l’analyse en constituants discontinus où le décodage des arbres lexicalisés et des dérivations syntaxiques est réalisé de façon jointe.Les principales contributions de cet article sont (1) une réduction du problème joint de l'étiquetage et de l’analyse en dépendances non-projectives au problème de l’arborescence couvrante généralisée de poids maximal, et (2) un nouvel algorithme de décodage pour ce problème fondé sur la relaxation lagrangienne.Nous évaluons ce modèle et obtenons des résultats de à l'état-de-l'art malgré des hypothèses d’indépendance fortes.\"},\n",
       "             528: {'src': 'A French clinical corpus with comprehensive semantic annotations: development of the Medical Entity and Relation LIMSI annOtated Text corpus (MERLOT)Quality annotated resources are essential for Natural Language Processing.The objective of this work is to present a corpus of clinical narratives in French annotated for linguistic, semantic and structural information, aimed at clinical information extraction.Six annotators contributed to the corpus annotation, using a comprehensive annotation scheme covering 21 entities, 11 attributes and 37 relations.All annotators trained on a small, common portion of the corpus before proceeding independently.An automatic tool was used to produce entity and attribute pre-annotations.About a tenth of the corpus was doubly annotated and annotation differences were resolved in consensus meetings.To ensure annotation consistency throughout the corpus, we devised harmonization tools to automatically identify annotation differences to be addressed to improve the overall corpus quality.The annotation project spanned over 24 months and resulted in a corpus comprising 500 documents (148,476 tokens) annotated with 44,740 entities and 26,478 relations.The average inter-annotator agreement is 0.793 F-measure for entities and 0.789 for relations.The performance of the pre-annotation tool for entities reached 0.814 F-measure when sufficient training data was available.The performance of our entity pre-annotation tool shows the value of the corpus to build and evaluate information extraction methods.In addition, we introduced harmonization methods that further improved the quality of annotations in the corpus.',\n",
       "              'mt': \"Un corpus clinique français avec des annotations sémantiques complètes : développement du corpus Medical Entity and Relation LIMSI annOtated Text (MERLOT)Des ressources annotées de qualité sont essentielles pour le traitement du langage naturel.L'objectif de ce travail est de présenter un corpus de récits cliniques en français annoté pour les informations linguistiques, sémantiques et structurelles, en vue de l'extraction d'informations cliniques.Six annotateurs ont contribué à l'annotation du corpus, en utilisant un schéma d'annotation complet couvrant 21 entités, 11 attributs et 37 relations.Tous les annotateurs se sont entraînés sur une petite partie commune du corpus avant de travailler de manière indépendante.Un outil automatique a été utilisé pour produire des pré-annotations d'entités et d'attributs.Environ un dixième du corpus a été doublement annoté et les différences d'annotation ont été résolues lors de réunions de consensus.Pour garantir la cohérence des annotations dans l'ensemble du corpus, nous avons conçu des outils d'harmonisation permettant d'identifier automatiquement les différences d'annotation à traiter afin d'améliorer la qualité globale du corpus.Le projet d'annotation s'est déroulé sur 24 mois et a abouti à un corpus comprenant 500 documents (148 476 tokens) annotés avec 44 740 entités et 26 478 relations.L'accord inter-annotateur moyen est de 0,793 F-mesure pour les entités et de 0,789 pour les relations.La performance de l'outil de pré-annotation pour les entités a atteint 0,814 F-mesure lorsque des données d'entraînement suffisantes étaient disponibles.La performance de notre outil de pré-annotation d'entités montre la valeur du corpus pour construire et évaluer les méthodes d'extraction d'information.En outre, nous avons introduit des méthodes d'harmonisation qui ont permis d'améliorer la qualité des annotations dans le corpus.\",\n",
       "              'ref': \"Un corpus clinique français avec des annotations sémantiques complètes : développement du corpus Medical Entity and Relation LIMSI annOtated Text (MERLOT)Des ressources annotées de qualité sont essentielles pour le traitement du langage naturel.L'objectif de ce travail est de présenter un corpus de récits cliniques en français annoté pour les informations linguistiques, sémantiques et structurelles, en vue de l'extraction d'informations cliniques.Six annotateurs ont contribué à l'annotation du corpus, en utilisant un schéma d'annotation complet couvrant 21 entités, 11 attributs et 37 relations.Tous les annotateurs se sont entraînés sur une petite partie commune du corpus avant de travailler de manière indépendante.Un outil automatique a été utilisé pour produire des pré-annotations d'entités et d'attributs.Environ un dixième du corpus a été doublement annoté et les différences d'annotation ont été résolues lors de réunions de consensus.Pour garantir la cohérence des annotations dans l'ensemble du corpus, nous avons conçu des outils d'harmonisation permettant d'identifier automatiquement les différences d'annotation à traiter afin d'améliorer la qualité globale du corpus.Le projet d'annotation s'est déroulé sur 24 mois et a abouti à un corpus comprenant 500 documents (148 476 tokens) annotés avec 44 740 entités et 26 478 relations.L'accord inter-annotateur moyen est de 0,793 F-mesure pour les entités et de 0,789 pour les relations.La performance de l'outil de pré-annotation pour les entités a atteint 0,814 F-mesure lorsque des données d'entraînement suffisantes étaient disponibles.La performance de notre outil de pré-annotation d'entités montre la valeur du corpus pour construire et évaluer les méthodes d'extraction d'information.En outre, nous avons introduit des méthodes d'harmonisation qui ont permis d'améliorer la qualité des annotations dans le corpus.\"},\n",
       "             150: {'src': \"Evaluating the morphological competence of Machine Translation SystemsWhile recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models.This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena.Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast.This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.\",\n",
       "              'mt': \"Évaluation de la compétence morphologique des systèmes de traduction automatiqueBien que les récents changements dans l'état de l'art de la traduction automatique aient fait progresser la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d'informations pour mesurer pleinement l'impact des modèles neuronaux.Cet article propose un nouveau type d'évaluation axé spécifiquement sur la compétence morphologique d'un système par rapport à divers phénomènes grammaticaux.Notre approche utilise des paires de phrases sources générées automatiquement, où chaque paire teste un contraste morphologique.Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT'17 pour la traduction de l'anglais vers le tchèque et le letton.\",\n",
       "              'ref': \"Évaluation de la compétence morphologique des systèmes de traduction automatiqueBien que les récents changements dans l'état de l'art de la traduction automatique aient fait progresser la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d'informations pour mesurer pleinement l'impact des modèles neuronaux.Cet article propose un nouveau type d'évaluation axé spécifiquement sur la compétence morphologique d'un système par rapport à divers phénomènes grammaticaux.Notre approche utilise des paires de phrases sources générées automatiquement, où chaque paire teste un contraste morphologique.Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT'17 pour la traduction de l'anglais vers le tchèque et le letton.\"},\n",
       "             148: {'src': \"Evaluating Discourse Phenomena in Neural Machine TranslationFor machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context.There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena.In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences.We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French.We also explore a novel way of exploiting context from the previous sentence.Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%).A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for corefer-ence and 57% for coherence/cohesion), highlighting the importance of target-side context.\",\n",
       "              'mt': 'Évaluation des phénomènes de discours dans la traduction automatique neuronalePour que la traduction automatique aborde les phénomènes de discours, les modèles doivent avoir accès à un contexte linguistique extra-sentiel.Il y a eu récemment un intérêt pour le contexte de modélisation dans la traduction automatique neuronale (NMT), mais les modèles ont été principalement évalués avec des métriques automatiques standard, mal adaptées à l’évaluation des phénomènes de discours.Dans cet article, nous présentons des ensembles de tests de discours faits à la main, conçus pour tester la capacité des modèles à exploiter les phrases source et cible précédentes.Nous étudions la performance des modèles NMT multi-encodeurs récemment proposés formés sur les sous-titres pour l’anglais au français.Nous explorons également une nouvelle façon d’exploiter le contexte de la phrase précédente.Malgré les gains tirés de l’utilisation de BLEU, les modèles multi-encodeurs n’apportent qu’une amélioration limitée dans la gestion des phénomènes de discours: Précision de 50\\xa0% sur notre ensemble de tests de coréférence et 53,5\\xa0% pour la cohérence/cohésion (par rapport à une base de référence non contextuelle de 50\\xa0%).Une stratégie simple de décodage de la concaténation de la phrase précédente et actuelle conduit à de bonnes performances, et notre nouvelle stratégie de multi-codage et de décodage de deux phrases conduit à la meilleure performance (72,5\\xa0% pour le corefer-ence et 57\\xa0% pour la cohérence/cohésion), soulignant l’importance du contexte côté cible.',\n",
       "              'ref': 'Évaluation des phénomènes discursifs en traduction automatique neuronalePour que la traduction automatique soit en mesure de traiter les phénomènes discursifs, les modèles doivent avoir accès à un contexte linguistique extraphrastique.La modélisation du contexte en traduction automatique neuronale (TAN) est récemment devenue un enjeu de recherche, mais les modèles ont été principalement évalués avec des métriques automatiques standard, mal adaptées à l’évaluation des phénomènes discursifs.Dans cet article, nous présentons des jeux de données de tests discursifs conçus par des êtres humains afin de tester la capacité des modèles à tirer parti des phrases source et cible antérieures.Nous étudions les performances des modèles TAN multi-encodeurs récemment proposés entraînés sur des sous-titres anglais-français.Nous explorons également une approche nouvelle de prise en compte du contexte au départ des phrases antérieures.En dépit de leurs meilleurs scores BLEU, les modèles multi-encodeurs n’améliore que de façon limitée le traitement des phénomènes discursifs : 50\\xa0% de précision sur nos données de tests pour la coréférence et 53,5\\xa0% pour la cohérence/cohésion (par rapport à une base de référence non contextuelle de 50\\xa0%).Une stratégie simple consistant à décoder la concaténation de la phrase précédente et de la phrase en cours de traduction offre de bonnes performances, et notre stratégie nouvelle de multi-encodage et de décodage de deux phrases offre la meilleure performance (72,5\\xa0% pour le coréférence et 57\\xa0% pour la cohérence/cohésion), soulignant l’importance du contexte cible.'},\n",
       "             115: {'src': 'ReadME generation from an OWL ontology describing NLP toolsThe paper deals with the generation of ReadME files from an ontology-based description of NLP tool.ReadME files are structured and organised according to properties defined in the ontology.One of the problem is being able to deal with multilingual generation of texts.To do so, we propose to map the ontol-ogy elements to multilingual knowledge defined in a SKOS ontology.',\n",
       "              'mt': \"Génération ReadME à partir d'une ontologie OWL décrivant les outils NLPL'article traite de la génération de fichiers ReadME à partir d'une description ontologique de l'outil NLP.Les fichiers ReadME sont structurés et organisés en fonction des propriétés définies dans l'ontologie.L'un des problèmes est de pouvoir traiter la génération multilingue de textes.Pour ce faire, nous proposons de mettre en correspondance les éléments d'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\",\n",
       "              'ref': \"Génération de ReadME à partir d'une ontologie OWL décrivant des outils TALL'article traite de la génération de fichiers ReadME fondée sur une description ontologique d'un outil TAL.Les fichiers ReadME sont structurés et organisés en fonction des propriétés définies dans l'ontologie.La problématique est d'être en mesure de traiter la génération multilingue de textes.Pour ce faire, nous proposons de mettre en correspondance les éléments de l'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\"}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1679fcb5-1019-40e5-bb00-825d49ca5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and extract values from two columns\n",
    "data = []\n",
    "count = 0\n",
    "for index, value in paragraph.items():\n",
    "    data.append(value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c737683-30d2-4a3f-8a0e-f010d63b3bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': 'Transforming Dependency Structures to LTAG Derivation TreesWe propose a new algorithm for parsing Lexicalized Tree Adjoining Grammars (LTAGs) which uses pre-assigned bilexi-cal dependency relations as a filter.That is, given a sentence and its corresponding well-formed dependency structure, the parser assigns elementary trees to words of the sentence and return attachment sites compatible with these elementary trees and predefined dependencies.Moreover, we prove that this algorithm has a linear-time complexity in the input length.This algorithm returns all compatible derivation trees as a packed forest.This result is of practical interest to the development of efficient weighted LTAG parsers based on derivation tree decoding.',\n",
       " 'mt': \"Transformation de structures de dépendances en arbres de dérivation LTAGNous proposons un nouvel algorithme pour analyser les grammaires adjacentes à l'arbre lexicalisé (LTAG) qui utilise des relations de dépendance bilexi-cal pré-assignées comme filtre.C'est-à-dire, compte tenu d'une phrase et de sa structure de dépendance bien formée correspondante, le parseur attribue des arbres élémentaires à des mots de la phrase et retourne des sites d'attachement compatibles avec ces arbres élémentaires et des dépendances prédéfinies.Par ailleurs, on prouve que cet algorithme présente une complexité linéaire temporelle dans la longueur d'entrée.Cet algorithme retourne tous les arbres de dérivation compatibles en tant que forêt compactée.Ce résultat présente un intérêt pratique pour le développement d'analyseurs LTAG pondérés efficaces basés sur un décodage d'arbre de dérivation.\",\n",
       " 'ref': \"Transformation de structures de dépendances en arbres de dérivation LTAGNous proposons un nouvel algorithme pour analyser les grammaires d'arbres adjoints lexicalisés (LTAG) qui utilise des relations de dépendance bilexicale pré-assignées comme filtre.C'est-à-dire, étant donné une phrase et de sa structure en dépendances bien formé, l'analyseur attribue des arbres élémentaires à des mots de la phrase et retourne les sites d'attachement compatibles avec ces arbres élémentaires et ces dépendances prédéfinies.Par ailleurs, on prouve que cet algorithme a une complexité temporelle linéaire dans la longueur d'entrée.Cet algorithme retourne tous les arbres de dérivation compatibles sous forme d'une forêt compact.Ce résultat présente un intérêt pratique pour le développement d'analyseurs LTAG pondérés efficaces fondés sur un décodage de l'arbre de dérivation.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e89606a3-781a-447b-9f95-2c37a225674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-38101a57-acdb-0625-2da5-ae6faf020a3a]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================count=========================: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1, 2], 'offsets': [0, 14]}, 1: {'subwords': [3], 'offsets': [14, 17]}, 2: {'subwords': [4, 5], 'offsets': [17, 28]}, 3: {'subwords': [6], 'offsets': [28, 31]}, 4: {'subwords': [7, 8], 'offsets': [31, 43]}, 5: {'subwords': [9], 'offsets': [43, 46]}, 6: {'subwords': [10], 'offsets': [46, 53]}, 7: {'subwords': [11], 'offsets': [53, 56]}, 8: {'subwords': [12, 13, 14], 'offsets': [56, 67]}, 9: {'subwords': [15, 16, 17, 18], 'offsets': [67, 76]}, 10: {'subwords': [19], 'offsets': [76, 86]}, 11: {'subwords': [20], 'offsets': [86, 89]}, 12: {'subwords': [21], 'offsets': [89, 96]}, 13: {'subwords': [22, 23], 'offsets': [96, 107]}, 14: {'subwords': [24], 'offsets': [107, 112]}, 15: {'subwords': [25], 'offsets': [112, 121]}, 16: {'subwords': [26], 'offsets': [121, 125]}, 17: {'subwords': [27, 28], 'offsets': [125, 136]}, 18: {'subwords': [29, 30], 'offsets': [136, 147]}, 19: {'subwords': [31], 'offsets': [147, 149]}, 20: {'subwords': [32, 33, 34], 'offsets': [149, 157]}, 21: {'subwords': [35, 36, 37], 'offsets': [157, 168]}, 22: {'subwords': [38, 39, 40, 41], 'offsets': [168, 175]}, 23: {'subwords': [42], 'offsets': [175, 179]}, 24: {'subwords': [43, 44], 'offsets': [179, 187]}, 25: {'subwords': [45], 'offsets': [187, 191]}, 26: {'subwords': [46], 'offsets': [191, 201]}, 27: {'subwords': [47], 'offsets': [201, 204]}, 28: {'subwords': [48, 49], 'offsets': [204, 215]}, 29: {'subwords': [50, 51, 52, 53], 'offsets': [215, 226]}, 30: {'subwords': [54, 55, 56, 57, 58], 'offsets': [226, 240]}, 31: {'subwords': [59], 'offsets': [240, 246]}, 32: {'subwords': [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], 'offsets': [246, 267]}, 33: {'subwords': [70], 'offsets': [267, 274]}, 34: {'subwords': [71], 'offsets': [274, 279]}, 35: {'subwords': [72, 73, 74], 'offsets': [279, 285]}, 36: {'subwords': [75], 'offsets': [285, 292]}, 37: {'subwords': [76], 'offsets': [292, 295]}, 38: {'subwords': [77], 'offsets': [295, 298]}, 39: {'subwords': [78], 'offsets': [298, 301]}, 40: {'subwords': [79], 'offsets': [301, 311]}, 41: {'subwords': [80], 'offsets': [311, 314]}, 42: {'subwords': [81, 82], 'offsets': [314, 325]}, 43: {'subwords': [83], 'offsets': [325, 330]}, 44: {'subwords': [84, 85], 'offsets': [330, 337]}, 45: {'subwords': [86, 87, 88], 'offsets': [337, 353]}, 46: {'subwords': [89], 'offsets': [353, 356]}, 47: {'subwords': [90, 91, 92], 'offsets': [356, 364]}, 48: {'subwords': [93, 94, 95], 'offsets': [364, 373]}, 49: {'subwords': [96], 'offsets': [373, 377]}, 50: {'subwords': [97], 'offsets': [377, 384]}, 51: {'subwords': [98, 99], 'offsets': [384, 397]}, 52: {'subwords': [100], 'offsets': [397, 399]}, 53: {'subwords': [101], 'offsets': [399, 403]}, 54: {'subwords': [102], 'offsets': [403, 408]}, 55: {'subwords': [103], 'offsets': [408, 411]}, 56: {'subwords': [104], 'offsets': [411, 414]}, 57: {'subwords': [105], 'offsets': [414, 421]}, 58: {'subwords': [106], 'offsets': [421, 424]}, 59: {'subwords': [107, 108], 'offsets': [424, 433]}, 60: {'subwords': [109], 'offsets': [433, 437]}, 61: {'subwords': [110], 'offsets': [437, 443]}, 62: {'subwords': [111, 112, 113, 114], 'offsets': [443, 457]}, 63: {'subwords': [115, 116], 'offsets': [457, 469]}, 64: {'subwords': [117], 'offsets': [469, 474]}, 65: {'subwords': [118], 'offsets': [474, 478]}, 66: {'subwords': [119], 'offsets': [478, 485]}, 67: {'subwords': [120, 121], 'offsets': [485, 498]}, 68: {'subwords': [122], 'offsets': [498, 501]}, 69: {'subwords': [123], 'offsets': [501, 505]}, 70: {'subwords': [124, 125], 'offsets': [505, 517]}, 71: {'subwords': [126, 127, 128, 129, 130, 131], 'offsets': [517, 533]}, 72: {'subwords': [132, 133], 'offsets': [533, 543]}, 73: {'subwords': [134], 'offsets': [543, 546]}, 74: {'subwords': [135, 136], 'offsets': [546, 553]}, 75: {'subwords': [137], 'offsets': [553, 557]}, 76: {'subwords': [138], 'offsets': [557, 561]}, 77: {'subwords': [139, 140], 'offsets': [561, 572]}, 78: {'subwords': [141], 'offsets': [572, 581]}, 79: {'subwords': [142], 'offsets': [581, 585]}, 80: {'subwords': [143, 144], 'offsets': [585, 596]}, 81: {'subwords': [145, 146, 147], 'offsets': [596, 605]}, 82: {'subwords': [148, 149], 'offsets': [605, 616]}, 83: {'subwords': [150], 'offsets': [616, 621]}, 84: {'subwords': [151], 'offsets': [621, 624]}, 85: {'subwords': [152], 'offsets': [624, 633]}, 86: {'subwords': [153, 154, 155, 156, 157, 158], 'offsets': [633, 646]}, 87: {'subwords': [159, 160], 'offsets': [646, 657]}, 88: {'subwords': [161, 162], 'offsets': [657, 666]}, 89: {'subwords': [163], 'offsets': [666, 671]}, 90: {'subwords': [164], 'offsets': [671, 675]}, 91: {'subwords': [165], 'offsets': [675, 682]}, 92: {'subwords': [166], 'offsets': [682, 685]}, 93: {'subwords': [167, 168, 169], 'offsets': [685, 696]}, 94: {'subwords': [170, 171], 'offsets': [696, 708]}, 95: {'subwords': [172], 'offsets': [708, 711]}, 96: {'subwords': [173], 'offsets': [711, 716]}, 97: {'subwords': [174], 'offsets': [716, 720]}, 98: {'subwords': [175], 'offsets': [720, 726]}, 99: {'subwords': [176, 177, 178, 179], 'offsets': [726, 739]}, 100: {'subwords': [180], 'offsets': [739, 748]}, 101: {'subwords': [181], 'offsets': [748, 757]}, 102: {'subwords': [182], 'offsets': [757, 760]}, 103: {'subwords': [183, 184], 'offsets': [760, 768]}, 104: {'subwords': [185], 'offsets': [768, 777]}, 105: {'subwords': [186], 'offsets': [777, 782]}, 106: {'subwords': [187], 'offsets': [782, 785]}, 107: {'subwords': [188], 'offsets': [785, 799]}, 108: {'subwords': [189, 190, 191, 192], 'offsets': [799, 812]}, 109: {'subwords': [193, 194], 'offsets': [812, 817]}, 110: {'subwords': [195, 196, 197], 'offsets': [817, 826]}, 111: {'subwords': [198, 199], 'offsets': [826, 836]}, 112: {'subwords': [200, 201], 'offsets': [836, 842]}, 113: {'subwords': [202], 'offsets': [842, 846]}, 114: {'subwords': [203], 'offsets': [846, 849]}, 115: {'subwords': [204, 205, 206], 'offsets': [849, 858]}, 116: {'subwords': [207, 208, 209], 'offsets': [858, 866]}, 117: {'subwords': [210], 'offsets': [866, 869]}, 118: {'subwords': [211, 212, 213, 214], 'offsets': [869, 881]}}\n",
      "word:  6\n",
      "word:  7\n",
      "word:  8\n",
      "word:  9\n",
      "word:  17\n",
      "word:  18\n",
      "word:  19\n",
      "word:  20\n",
      "word:  21\n",
      "word:  22\n",
      "word:  28\n",
      "word:  29\n",
      "word:  30\n",
      "word:  32\n",
      "word:  36\n",
      "word:  43\n",
      "word:  44\n",
      "word:  47\n",
      "word:  85\n",
      "word:  86\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=====================count=========================:\u001b[39m\u001b[33m\"\u001b[39m, count)\n\u001b[32m      4\u001b[39m count += \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/base.py:655\u001b[39m, in \u001b[36mCometModel.predict\u001b[39m\u001b[34m(self, samples, batch_size, gpus, devices, mc_dropout, progress_bar, accelerator, num_workers, length_batching)\u001b[39m\n\u001b[32m    646\u001b[39m trainer = ptl.Trainer(\n\u001b[32m    647\u001b[39m     devices=devices,\n\u001b[32m    648\u001b[39m     logger=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    652\u001b[39m     enable_progress_bar=enable_progress_bar,\n\u001b[32m    653\u001b[39m )\n\u001b[32m    654\u001b[39m return_predictions = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_predictions\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m:\n\u001b[32m    659\u001b[39m     torch.distributed.barrier()  \u001b[38;5;66;03m# Waits for all processes to finish predict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    857\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    894\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    895\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    896\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    900\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:124\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:253\u001b[39m, in \u001b[36m_PredictionLoop._predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[32m    248\u001b[39m step_args = (\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    252\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m predictions = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:319\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    322\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:437\u001b[39m, in \u001b[36mStrategy.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:961\u001b[39m, in \u001b[36mUnifiedMetric.predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx)\u001b[39m\n\u001b[32m    959\u001b[39m MT_dict = batch[-\u001b[32m1\u001b[39m].copy() \u001b[38;5;66;03m# contains the dictionary with word_ids, \u001b[39;00m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m#mt_sentences,mt_sentences_tokenized\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m error_spans, word_level_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubword_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmt_offsets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMT_dict\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33merror_spans\u001b[39m\u001b[33m\"\u001b[39m] = error_spans\n\u001b[32m    965\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33mword_level_probability\u001b[39m\u001b[33m\"\u001b[39m]=word_level_prob\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:896\u001b[39m, in \u001b[36mUnifiedMetric.decode\u001b[39m\u001b[34m(self, subword_probs, input_ids, mt_offsets, MT_dict)\u001b[39m\n\u001b[32m    893\u001b[39m     count_index = count_index + \u001b[32m1\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;66;03m#print(\"track_spans: \", track_spans) \u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m#get word level error span\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m word_level_error_span = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_level_error_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrack_spans\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmt_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mMT_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_tokenized_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m sentence_output = []\n\u001b[32m    901\u001b[39m count = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# to access the spans in the word_level_error_span\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:771\u001b[39m, in \u001b[36mUnifiedMetric.word_level_error_span\u001b[39m\u001b[34m(self, track_spans, mt_offsets, word_ids, Tokenized_Words)\u001b[39m\n\u001b[32m    769\u001b[39m word_span = defaultdict()\n\u001b[32m    770\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] = text.strip()\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m] = mapping[\u001b[43mwords_in_span\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m][\u001b[33m'\u001b[39m\u001b[33moffsets\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    772\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m] = mapping[words_in_span[-\u001b[32m1\u001b[39m]][\u001b[33m'\u001b[39m\u001b[33moffsets\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m    773\u001b[39m all_word_spans[index] = word_span\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in data:\n",
    "    print(\"=====================count=========================:\", count)\n",
    "    count += 1\n",
    "    model_output = model.predict([item], batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0592298b-e1c3-4fd9-adb6-e3e5f62748ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and extract values from two columns\n",
    "data = []\n",
    "count = 0\n",
    "for index, row in grouped_df.iterrows():\n",
    "    Postedit_id = row['Postedit_id']\n",
    "    source      = row['source']\n",
    "    translation = row['translation']\n",
    "    postedition  = row['postedition']\n",
    "    #print(\"index number of the dataset: \", index)\n",
    "    #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "    src = \"\"\n",
    "    mt  = \"\"\n",
    "    ref = \"\"\n",
    "    for i, sentence in enumerate(source):\n",
    "        src += sentence\n",
    "        mt  += translation[i]\n",
    "        ref += postedition[i]\n",
    "    data.append({\n",
    "        \"src\": src,\n",
    "        \"mt\" : mt,\n",
    "        \"ref\": ref,\n",
    "    })\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f6dd0e-2733-44d2-ac2c-f8c582447667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [data[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebc3b007-3248-4c77-961e-dffeddf08933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src': 'One Source, Two Targets: Challenges and Rewards of Dual DecodingMachine translation is generally understood as generating one target text from an input source document.In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other.As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text.We present an analysis of possible implementations of dual decoding, and experiment with four applications.Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.',\n",
       "  'mt': 'Une source, deux cibles: Défis et récompenses de la double décorationLa traduction automatique est généralement comprise comme la génération d’un texte cible à partir d’un document source d’entrée.Dans ce document, nous considérons une exigence plus forte: générer conjointement deux textes de sorte que chaque côté de sortie dépende efficacement de l’autre.Comme nous en discutons, un tel dispositif sert plusieurs fins pratiques, de la traduction automatique multi-cibles à la génération de variations contrôlées du texte cible.Nous présentons une analyse des implémentations possibles du double décodage et expérimentons quatre applications.Voir le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du double décodage et d’analyser en profondeur les avantages de générer des traductions appariées, plutôt que indépendantes.',\n",
       "  'ref': \"Une source, deux cibles: Défis et bénéfices d'un double décodageLa traduction automatique est généralement comprise comme la génération d’un texte cible à partir d’un document source en entrée.Dans cet article, nous considérons une exigence plus forte: engendrer conjointement deux textes, de sorte que chacune des sorties dépende effectivement de l’autre.Comme nous le discutons, un tel dispositif a plusieurs applications pratiques, de la traduction automatique multi-cible à la génération de variations contrôlées du texte cible.Nous présentons une analyse des implantations possibles du double décodage et expérimentons avec quatre applications.Considérer le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du double décodage et d’analyser en profondeur les avantages d'engendrer des traductions appariées, plutôt qu'indépendantes.\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d400f61-233b-4a0f-bfd4-79179a1ec5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-38101a57-acdb-0625-2da5-ae6faf020a3a]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:  {0: {'subwords': [1], 'offsets': [0, 4]}, 1: {'subwords': [2, 3, 4], 'offsets': [4, 17]}, 2: {'subwords': [5, 6, 7], 'offsets': [17, 30]}, 3: {'subwords': [8], 'offsets': [30, 35]}, 4: {'subwords': [9], 'offsets': [35, 38]}, 5: {'subwords': [10, 11], 'offsets': [38, 49]}, 6: {'subwords': [12], 'offsets': [49, 52]}, 7: {'subwords': [13, 14, 15], 'offsets': [52, 62]}, 8: {'subwords': [16], 'offsets': [62, 67]}, 9: {'subwords': [17, 18, 19, 20], 'offsets': [67, 79]}}\n",
      "word:  6\n",
      "word:  7\n",
      "word:  8\n",
      "word:  9\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': \"de n'importe quel texterésumé\", 'start': 49, 'end': 79})})\n",
      "mapping:  {0: {'subwords': [1, 2], 'offsets': [0, 10]}, 1: {'subwords': [3, 4, 5], 'offsets': [10, 25]}, 2: {'subwords': [6, 7], 'offsets': [25, 37]}, 3: {'subwords': [8], 'offsets': [37, 39]}, 4: {'subwords': [9], 'offsets': [39, 46]}, 5: {'subwords': [10], 'offsets': [46, 49]}, 6: {'subwords': [11, 12], 'offsets': [49, 56]}, 7: {'subwords': [13, 14, 15, 16], 'offsets': [56, 70]}, 8: {'subwords': [17], 'offsets': [70, 74]}, 9: {'subwords': [18, 19], 'offsets': [74, 83]}, 10: {'subwords': [20], 'offsets': [83, 88]}, 11: {'subwords': [21, 22], 'offsets': [88, 99]}, 12: {'subwords': [23], 'offsets': [99, 103]}, 13: {'subwords': [24], 'offsets': [103, 111]}, 14: {'subwords': [25, 26, 27, 28], 'offsets': [111, 124]}, 15: {'subwords': [29], 'offsets': [124, 127]}, 16: {'subwords': [30], 'offsets': [127, 137]}, 17: {'subwords': [31, 32], 'offsets': [137, 149]}, 18: {'subwords': [33], 'offsets': [149, 151]}, 19: {'subwords': [34], 'offsets': [151, 158]}, 20: {'subwords': [35], 'offsets': [158, 161]}, 21: {'subwords': [36, 37, 38], 'offsets': [161, 168]}, 22: {'subwords': [39, 40], 'offsets': [168, 178]}, 23: {'subwords': [41], 'offsets': [178, 181]}, 24: {'subwords': [42], 'offsets': [181, 190]}, 25: {'subwords': [43], 'offsets': [190, 193]}, 26: {'subwords': [44], 'offsets': [193, 196]}, 27: {'subwords': [45, 46, 47, 48], 'offsets': [196, 209]}, 28: {'subwords': [49, 50, 51, 52], 'offsets': [209, 223]}, 29: {'subwords': [53], 'offsets': [223, 227]}, 30: {'subwords': [54], 'offsets': [227, 232]}, 31: {'subwords': [55], 'offsets': [232, 239]}, 32: {'subwords': [56, 57, 58], 'offsets': [239, 252]}, 33: {'subwords': [59], 'offsets': [252, 255]}, 34: {'subwords': [60], 'offsets': [255, 262]}, 35: {'subwords': [61, 62], 'offsets': [262, 269]}, 36: {'subwords': [63], 'offsets': [269, 274]}, 37: {'subwords': [64], 'offsets': [274, 277]}, 38: {'subwords': [65], 'offsets': [277, 286]}, 39: {'subwords': [66], 'offsets': [286, 289]}, 40: {'subwords': [67], 'offsets': [289, 292]}, 41: {'subwords': [68], 'offsets': [292, 299]}, 42: {'subwords': [69, 70, 71], 'offsets': [299, 305]}, 43: {'subwords': [72], 'offsets': [305, 310]}, 44: {'subwords': [73, 74, 75, 76, 77], 'offsets': [310, 321]}, 45: {'subwords': [78], 'offsets': [321, 324]}, 46: {'subwords': [79, 80], 'offsets': [324, 333]}, 47: {'subwords': [81, 82, 83], 'offsets': [333, 340]}, 48: {'subwords': [84, 85], 'offsets': [340, 349]}, 49: {'subwords': [86], 'offsets': [349, 357]}, 50: {'subwords': [87], 'offsets': [357, 362]}, 51: {'subwords': [88], 'offsets': [362, 367]}, 52: {'subwords': [89], 'offsets': [367, 376]}, 53: {'subwords': [90], 'offsets': [376, 381]}, 54: {'subwords': [91], 'offsets': [381, 385]}, 55: {'subwords': [92], 'offsets': [385, 390]}, 56: {'subwords': [93, 94, 95], 'offsets': [390, 399]}}\n",
      "word:  28\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'expérimentons', 'start': 209, 'end': 223})})\n",
      "mapping:  {0: {'subwords': [1, 2, 3, 4, 5, 6], 'offsets': [0, 9]}, 1: {'subwords': [7], 'offsets': [9, 11]}, 2: {'subwords': [8, 9, 10, 11], 'offsets': [11, 24]}, 3: {'subwords': [12, 13, 14], 'offsets': [24, 30]}, 4: {'subwords': [15], 'offsets': [30, 33]}, 5: {'subwords': [16], 'offsets': [33, 35]}, 6: {'subwords': [17, 18], 'offsets': [35, 48]}, 7: {'subwords': [19, 20], 'offsets': [48, 58]}, 8: {'subwords': [21], 'offsets': [58, 63]}, 9: {'subwords': [22, 23, 24, 25], 'offsets': [63, 76]}, 10: {'subwords': [26, 27, 28], 'offsets': [76, 91]}, 11: {'subwords': [29, 30], 'offsets': [91, 103]}, 12: {'subwords': [31], 'offsets': [103, 105]}, 13: {'subwords': [32], 'offsets': [105, 112]}, 14: {'subwords': [33], 'offsets': [112, 115]}, 15: {'subwords': [34, 35, 36], 'offsets': [115, 122]}, 16: {'subwords': [37, 38, 39, 40, 41], 'offsets': [122, 137]}, 17: {'subwords': [42], 'offsets': [137, 141]}, 18: {'subwords': [43, 44], 'offsets': [141, 150]}, 19: {'subwords': [45], 'offsets': [150, 155]}, 20: {'subwords': [46, 47], 'offsets': [155, 166]}, 21: {'subwords': [48], 'offsets': [166, 172]}, 22: {'subwords': [49], 'offsets': [172, 186]}, 23: {'subwords': [50], 'offsets': [186, 188]}, 24: {'subwords': [51], 'offsets': [188, 191]}, 25: {'subwords': [52], 'offsets': [191, 197]}, 26: {'subwords': [53], 'offsets': [197, 200]}, 27: {'subwords': [54], 'offsets': [200, 203]}, 28: {'subwords': [55, 56, 57], 'offsets': [203, 211]}, 29: {'subwords': [58, 59, 60], 'offsets': [211, 221]}, 30: {'subwords': [61], 'offsets': [221, 227]}, 31: {'subwords': [62], 'offsets': [227, 235]}, 32: {'subwords': [63], 'offsets': [235, 239]}, 33: {'subwords': [64, 65], 'offsets': [239, 248]}, 34: {'subwords': [66, 67], 'offsets': [248, 254]}, 35: {'subwords': [68], 'offsets': [254, 258]}, 36: {'subwords': [69], 'offsets': [258, 262]}, 37: {'subwords': [70], 'offsets': [262, 270]}, 38: {'subwords': [71], 'offsets': [270, 273]}, 39: {'subwords': [72, 73], 'offsets': [273, 282]}, 40: {'subwords': [74], 'offsets': [282, 287]}, 41: {'subwords': [75, 76, 77, 78], 'offsets': [287, 300]}, 42: {'subwords': [79, 80, 81, 82], 'offsets': [300, 310]}, 43: {'subwords': [83], 'offsets': [310, 313]}, 44: {'subwords': [84], 'offsets': [313, 316]}, 45: {'subwords': [85], 'offsets': [316, 326]}, 46: {'subwords': [86, 87, 88], 'offsets': [326, 339]}, 47: {'subwords': [89], 'offsets': [339, 342]}, 48: {'subwords': [90], 'offsets': [342, 347]}, 49: {'subwords': [91], 'offsets': [347, 353]}, 50: {'subwords': [92, 93, 94, 95], 'offsets': [353, 365]}, 51: {'subwords': [96], 'offsets': [365, 370]}, 52: {'subwords': [97, 98], 'offsets': [370, 381]}, 53: {'subwords': [99, 100, 101], 'offsets': [381, 394]}, 54: {'subwords': [102], 'offsets': [394, 397]}, 55: {'subwords': [103, 104, 105, 106], 'offsets': [397, 410]}, 56: {'subwords': [107], 'offsets': [410, 416]}, 57: {'subwords': [108], 'offsets': [416, 423]}, 58: {'subwords': [109], 'offsets': [423, 427]}, 59: {'subwords': [110, 111], 'offsets': [427, 440]}, 60: {'subwords': [112, 113, 114, 115], 'offsets': [440, 453]}, 61: {'subwords': [116], 'offsets': [453, 458]}, 62: {'subwords': [117], 'offsets': [458, 462]}, 63: {'subwords': [118], 'offsets': [462, 467]}, 64: {'subwords': [119, 120], 'offsets': [467, 474]}}\n",
      "word:  0\n",
      "word:  1\n",
      "word:  15\n",
      "word:  54\n",
      "word:  55\n",
      "word:  64\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'LIMSI-COT à', 'start': 0, 'end': 11}), 1: defaultdict(None, {'text': 'récits', 'start': 115, 'end': 122}), 2: defaultdict(None, {'text': 'au domaine.Nous', 'start': 394, 'end': 410}), 3: defaultdict(None, {'text': 'tâches', 'start': 467, 'end': 474})})\n",
      "mapping:  {0: {'subwords': [1, 2, 3], 'offsets': [0, 10]}, 1: {'subwords': [4, 5], 'offsets': [10, 17]}, 2: {'subwords': [6], 'offsets': [17, 19]}, 3: {'subwords': [7], 'offsets': [19, 26]}, 4: {'subwords': [8, 9, 10], 'offsets': [26, 32]}, 5: {'subwords': [11, 12], 'offsets': [32, 42]}, 6: {'subwords': [13, 14], 'offsets': [42, 46]}, 7: {'subwords': [15, 16, 17], 'offsets': [46, 56]}, 8: {'subwords': [18], 'offsets': [56, 60]}, 9: {'subwords': [19], 'offsets': [60, 67]}, 10: {'subwords': [20, 21, 22, 23, 24], 'offsets': [67, 80]}, 11: {'subwords': [25, 26], 'offsets': [80, 87]}, 12: {'subwords': [27], 'offsets': [87, 90]}, 13: {'subwords': [28], 'offsets': [90, 93]}, 14: {'subwords': [29], 'offsets': [93, 104]}, 15: {'subwords': [30], 'offsets': [104, 107]}, 16: {'subwords': [31, 32], 'offsets': [107, 116]}, 17: {'subwords': [33, 34], 'offsets': [116, 123]}, 18: {'subwords': [35], 'offsets': [123, 125]}, 19: {'subwords': [36], 'offsets': [125, 132]}, 20: {'subwords': [37, 38, 39], 'offsets': [132, 138]}, 21: {'subwords': [40], 'offsets': [138, 150]}, 22: {'subwords': [41, 42], 'offsets': [150, 162]}, 23: {'subwords': [43], 'offsets': [162, 165]}, 24: {'subwords': [44, 45, 46, 47], 'offsets': [165, 173]}, 25: {'subwords': [48, 49, 50, 51], 'offsets': [173, 181]}, 26: {'subwords': [52, 53], 'offsets': [181, 190]}, 27: {'subwords': [54, 55], 'offsets': [190, 197]}, 28: {'subwords': [56], 'offsets': [197, 202]}, 29: {'subwords': [57, 58, 59, 60], 'offsets': [202, 213]}, 30: {'subwords': [61], 'offsets': [213, 216]}, 31: {'subwords': [62, 63], 'offsets': [216, 226]}, 32: {'subwords': [64], 'offsets': [226, 229]}, 33: {'subwords': [65], 'offsets': [229, 238]}, 34: {'subwords': [66], 'offsets': [238, 242]}, 35: {'subwords': [67, 68], 'offsets': [242, 253]}, 36: {'subwords': [69, 70], 'offsets': [253, 262]}, 37: {'subwords': [71], 'offsets': [262, 267]}, 38: {'subwords': [72, 73, 74, 75, 76, 77, 78, 79], 'offsets': [267, 284]}, 39: {'subwords': [80], 'offsets': [284, 288]}, 40: {'subwords': [81], 'offsets': [288, 298]}, 41: {'subwords': [82], 'offsets': [298, 302]}, 42: {'subwords': [83], 'offsets': [302, 305]}, 43: {'subwords': [84], 'offsets': [305, 313]}, 44: {'subwords': [85, 86], 'offsets': [313, 321]}, 45: {'subwords': [87], 'offsets': [321, 324]}, 46: {'subwords': [88], 'offsets': [324, 335]}, 47: {'subwords': [89, 90, 91], 'offsets': [335, 347]}, 48: {'subwords': [92], 'offsets': [347, 350]}, 49: {'subwords': [93, 94, 95, 96, 97], 'offsets': [350, 362]}, 50: {'subwords': [98], 'offsets': [362, 365]}, 51: {'subwords': [99, 100], 'offsets': [365, 372]}, 52: {'subwords': [101], 'offsets': [372, 377]}, 53: {'subwords': [102], 'offsets': [377, 387]}, 54: {'subwords': [103], 'offsets': [387, 390]}, 55: {'subwords': [104], 'offsets': [390, 397]}, 56: {'subwords': [105], 'offsets': [397, 400]}, 57: {'subwords': [106, 107], 'offsets': [400, 415]}, 58: {'subwords': [108], 'offsets': [415, 419]}, 59: {'subwords': [109], 'offsets': [419, 428]}, 60: {'subwords': [110, 111, 112, 113], 'offsets': [428, 440]}, 61: {'subwords': [114], 'offsets': [440, 445]}, 62: {'subwords': [115], 'offsets': [445, 449]}, 63: {'subwords': [116, 117], 'offsets': [449, 463]}, 64: {'subwords': [118, 119, 120], 'offsets': [463, 476]}, 65: {'subwords': [121, 122], 'offsets': [476, 485]}, 66: {'subwords': [123], 'offsets': [485, 490]}, 67: {'subwords': [124], 'offsets': [490, 494]}, 68: {'subwords': [125, 126], 'offsets': [494, 504]}, 69: {'subwords': [127, 128, 129], 'offsets': [504, 510]}}\n",
      "word:  0\n",
      "word:  1\n",
      "word:  2\n",
      "word:  4\n",
      "word:  5\n",
      "word:  6\n",
      "word:  7\n",
      "word:  8\n",
      "word:  9\n",
      "word:  10\n",
      "word:  11\n",
      "word:  24\n",
      "word:  25\n",
      "word:  40\n",
      "word:  43\n",
      "word:  46\n",
      "word:  48\n",
      "word:  49\n",
      "word:  60\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/base.py:655\u001b[39m, in \u001b[36mCometModel.predict\u001b[39m\u001b[34m(self, samples, batch_size, gpus, devices, mc_dropout, progress_bar, accelerator, num_workers, length_batching)\u001b[39m\n\u001b[32m    646\u001b[39m trainer = ptl.Trainer(\n\u001b[32m    647\u001b[39m     devices=devices,\n\u001b[32m    648\u001b[39m     logger=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    652\u001b[39m     enable_progress_bar=enable_progress_bar,\n\u001b[32m    653\u001b[39m )\n\u001b[32m    654\u001b[39m return_predictions = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_predictions\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m:\n\u001b[32m    659\u001b[39m     torch.distributed.barrier()  \u001b[38;5;66;03m# Waits for all processes to finish predict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    857\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    894\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    895\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    896\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    900\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:124\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:253\u001b[39m, in \u001b[36m_PredictionLoop._predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[32m    248\u001b[39m step_args = (\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    252\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m predictions = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:319\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    322\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:437\u001b[39m, in \u001b[36mStrategy.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:961\u001b[39m, in \u001b[36mUnifiedMetric.predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx)\u001b[39m\n\u001b[32m    959\u001b[39m MT_dict = batch[-\u001b[32m1\u001b[39m].copy() \u001b[38;5;66;03m# contains the dictionary with word_ids, \u001b[39;00m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m#mt_sentences,mt_sentences_tokenized\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m error_spans, word_level_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubword_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmt_offsets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMT_dict\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33merror_spans\u001b[39m\u001b[33m\"\u001b[39m] = error_spans\n\u001b[32m    965\u001b[39m batch_prediction.metadata[\u001b[33m\"\u001b[39m\u001b[33mword_level_probability\u001b[39m\u001b[33m\"\u001b[39m]=word_level_prob\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:896\u001b[39m, in \u001b[36mUnifiedMetric.decode\u001b[39m\u001b[34m(self, subword_probs, input_ids, mt_offsets, MT_dict)\u001b[39m\n\u001b[32m    893\u001b[39m     count_index = count_index + \u001b[32m1\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;66;03m#print(\"track_spans: \", track_spans) \u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m#get word level error span\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m word_level_error_span = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_level_error_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrack_spans\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmt_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mMT_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_tokenized_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m sentence_output = []\n\u001b[32m    901\u001b[39m count = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# to access the spans in the word_level_error_span\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/multitask/unified_metric.py:771\u001b[39m, in \u001b[36mUnifiedMetric.word_level_error_span\u001b[39m\u001b[34m(self, track_spans, mt_offsets, word_ids, Tokenized_Words)\u001b[39m\n\u001b[32m    769\u001b[39m word_span = defaultdict()\n\u001b[32m    770\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] = text.strip()\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m] = mapping[\u001b[43mwords_in_span\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m][\u001b[33m'\u001b[39m\u001b[33moffsets\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    772\u001b[39m word_span[\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m] = mapping[words_in_span[-\u001b[32m1\u001b[39m]][\u001b[33m'\u001b[39m\u001b[33moffsets\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m    773\u001b[39m all_word_spans[index] = word_span\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "model_output = model.predict(data, batch_size=8, gpus=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
