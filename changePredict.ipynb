{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c74feef-ca12-471f-a459-efbe05312be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/rahmang/xcomet/COMET\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb94a1a-08ad-49e0-b98e-016bfb31c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 19 16:55:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:61:00.0 Off |                    0 |\n",
      "|  0%   37C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa2059f-86d8-40d4-aee9-a608b1843dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-693c7989-017a-ce7e-2094-6a44dd0f1028]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"mt\"]: ['Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst']\n",
      "input input_sequences just with MT: in the prepare_sample in unified_metric  [{'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2]]), 'label_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]], 'word_ids': [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]]}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/base.py\", line 557, in prepare_for_inference\n    return self.prepare_sample(sample, stage=\"predict\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch.ssd/rahmang/job_9766054.pbs-m1.metacentrum.cz/ipykernel_60858/3382871535.py\", line 74, in prepare_sample\n    \"words_id\": model_inputs[\"word_ids\"],\n                ~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'word_ids'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 620\u001b[39m\n\u001b[32m    601\u001b[39m data = [\n\u001b[32m    602\u001b[39m     {\n\u001b[32m    603\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBoris Johnson teeters on edge of favour with Tory MPs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m     }\n\u001b[32m    607\u001b[39m ]\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# data = [\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m#     {\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m#         \"src\": \"10 到 15 分钟可以送到吗\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m model_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;66;03m# Segment-level scores\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[38;5;28mprint\u001b[39m (model_output.scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/base.py:664\u001b[39m, in \u001b[36mCometModel.predict\u001b[39m\u001b[34m(self, samples, batch_size, gpus, devices, mc_dropout, progress_bar, accelerator, num_workers, length_batching)\u001b[39m\n\u001b[32m    655\u001b[39m trainer = ptl.Trainer(\n\u001b[32m    656\u001b[39m     devices=devices,\n\u001b[32m    657\u001b[39m     logger=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    661\u001b[39m     enable_progress_bar=enable_progress_bar,\n\u001b[32m    662\u001b[39m )\n\u001b[32m    663\u001b[39m return_predictions = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_predictions\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpus > \u001b[32m1\u001b[39m:\n\u001b[32m    668\u001b[39m     torch.distributed.barrier()  \u001b[38;5;66;03m# Waits for all processes to finish predict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    857\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    894\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    895\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    896\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    900\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:121\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    120\u001b[39m     dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     batch, batch_idx, dataloader_idx = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batches\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m._start_profiler()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ITERATOR_RETURN:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[39m, in \u001b[36m_Sequential.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._use_next_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/xcomet/lib/python3.11/site-packages/torch/_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mKeyError\u001b[39m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/auto/brno2/home/rahmang/xcomet/comet_forked/COMET_GR/comet/models/base.py\", line 557, in prepare_for_inference\n    return self.prepare_sample(sample, stage=\"predict\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch.ssd/rahmang/job_9766054.pbs-m1.metacentrum.cz/ipykernel_60858/3382871535.py\", line 74, in prepare_sample\n    \"words_id\": model_inputs[\"word_ids\"],\n                ~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'word_ids'\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "from comet.models.utils import Prediction\n",
    "from typing import Dict, Optional\n",
    "from typing import List, Dict\n",
    "from typing import Union, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import inspect \n",
    "import torch\n",
    "import torch.nn as nn  # <-- Add thiss\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "\n",
    "    def prepare_sample(\n",
    "        self, sample: List[Dict[str, Union[str, float]]], stage: str = \"fit\"\n",
    "    ) -> Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Tokenizes input data and prepares targets for training.\n",
    "\n",
    "        Args:\n",
    "            sample (List[Dict[str, Union[str, float]]]): Mini-batch\n",
    "            stage (str, optional): Model stage ('train' or 'predict'). Defaults to \"fit\".\n",
    "\n",
    "        Returns:\n",
    "            Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]: Model input\n",
    "                and targets.\n",
    "        \"\"\"\n",
    "        # print(\"=================++++++++++++++++++++++++++++++++++==========================\")\n",
    "\n",
    "        # Get the caller's function name and file location\n",
    "        # caller_frame = inspect.stack()[1]\n",
    "        # caller_function = caller_frame.function\n",
    "        # caller_file = caller_frame.filename\n",
    "    \n",
    "        # # Print caller details\n",
    "        # print(f\"prepare_sample called by: {caller_function} (from {caller_file})\")\n",
    "        # print(\"sample in the prepare_sample: \", sample)\n",
    "        # print(\"the stage is: \", stage)\n",
    "        # for k in sample[0]:\n",
    "        #     print(\"K is: \", k)\n",
    "        inputs = {k: [d[k] for d in sample] for k in sample[0]}\n",
    "        print(f'''inputs[\"mt\"]: {inputs[\"mt\"]}''')\n",
    "        # only this will return word_ids from self.encoder.prepare_sample ->\n",
    "        # self.encoder.subword_tokenize as it is set self.word_level\n",
    "        # for src and ref, self.word_level = False by default\n",
    "        input_sequences = [\n",
    "            self.encoder.prepare_sample(inputs[\"mt\"], self.word_level, None),\n",
    "        ]\n",
    "        input_sequences_mt = input_sequences.copy()  # Now independent of input_sequences #added by me\n",
    "        print(\"input input_sequences just with MT: in the prepare_sample in unified_metric \", input_sequences_mt)\n",
    "\n",
    "        src_input, ref_input = False, False\n",
    "        if (\"src\" in inputs) and (\"src\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"src\"]))\n",
    "            src_input = True\n",
    "\n",
    "        if (\"ref\" in inputs) and (\"ref\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"ref\"]))\n",
    "            ref_input = True\n",
    "        # print(\"input_sequences after adding source and ref: \")\n",
    "        # for inp in input_sequences:\n",
    "        #     print(inp)\n",
    "        # print(\"input_sequences after adding source and ref: \")\n",
    "        unified_input = src_input and ref_input\n",
    "        model_inputs = self.concat_inputs(input_sequences, unified_input) #updated unified_metric's\n",
    "        #concat_inputs function to return word_ids\n",
    "        #print(\"model inputs ++++++++++++++: \", model_inputs)\n",
    "        if stage == \"predict\":\n",
    "            #print(\"word ids: \", model_inputs[])\n",
    "            #return model_inputs[\"inputs\"]\n",
    "            #, model_inputs[\"word_ids\"] #model_inputs[\"word_ids\"] added by me\n",
    "            all_inputs = model_inputs[\"inputs\"] \n",
    "            #added by me\n",
    "            words_id_dict = {\n",
    "                \"words_id\": model_inputs[\"word_ids\"],\n",
    "                \"mt_sentences\": inputs[\"mt\"],\n",
    "                \"mt_sentences_tokenized\": input_sequences_mt\n",
    "            }\n",
    "            updated = all_inputs + (words_id_dict,)\n",
    "            #print(\"updated dict ========\", updated)\n",
    "            #Update the OrderedDict\n",
    "            model_inputs[\"inputs\"] = updated\n",
    "            return model_inputs[\"inputs\"]\n",
    "        scores = [float(s) for s in inputs[\"score\"]]\n",
    "        targets = Target(score=torch.tensor(scores, dtype=torch.float))\n",
    "\n",
    "        if \"system\" in inputs:\n",
    "            targets[\"system\"] = inputs[\"system\"]\n",
    "\n",
    "        if self.word_level:\n",
    "            # Labels will be the same accross all inputs because we are only\n",
    "            # doing sequence tagging on the MT. We will only use the mask corresponding\n",
    "            # to the MT segment.\n",
    "            seq_len = model_inputs[\"mt_length\"].max()\n",
    "            targets[\"mt_length\"] = model_inputs[\"mt_length\"]\n",
    "            targets[\"labels\"] = model_inputs[\"inputs\"][0][\"label_ids\"][:, :seq_len]\n",
    "\n",
    "        return model_inputs[\"inputs\"], targets\n",
    "        \n",
    "    def word_level_prob(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        word_ids: Dict[\n",
    "        str,\n",
    "        Union[\n",
    "            List[List[Optional[int]]],  # words_id\n",
    "            List[str],  # mt_sentences\n",
    "            List[Dict[\n",
    "                str,\n",
    "                Union[\n",
    "                    torch.Tensor,  # input_ids/label_ids/attention_mask\n",
    "                    List[List[Tuple[int, int]]],  # offsets\n",
    "                    List[List[Optional[int]]]  # word_ids\n",
    "            ]]\n",
    "        ]\n",
    "    ]\n",
    "        ]\n",
    "    ) -> List[List[Dict[str, float]]]:\n",
    "        \"\"\" Returns word level probability score\n",
    "        word_ids = {\n",
    "    'words_id': [\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "    ],\n",
    "    'mt_sentences': [\n",
    "        \"Can I receive my food in 10 to 15 minutes?\",\n",
    "        \"Can you send it for 10 to 15 minutes?\"\n",
    "    ],\n",
    "    'mt_sentences_tokenized': [\n",
    "        {\n",
    "            'input_ids': tensor([\n",
    "                [0, 4171, 87, 53299, 759, 15381, 23, 209, 47, 423, 14633, 32, 2],\n",
    "                [0, 4171, 398, 25379, 442, 100, 209, 47, 423, 14633, 32, 2, 1]\n",
    "            ], device='cuda:0'),\n",
    "            'label_ids': tensor([\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n",
    "            ], device='cuda:0'),\n",
    "            'attention_mask': tensor([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "            ], device='cuda:0'),\n",
    "            'offsets': [\n",
    "                [(0, 0), (0, 3), (3, 5), (5, 13), (13, 16), (16, 21), \n",
    "                 (21, 24), (24, 27), (27, 30), (30, 33), (33, 41), (41, 42), (0, 0)],\n",
    "                [(0, 0), (0, 3), (3, 7), (7, 12), (12, 15), (15, 19), \n",
    "                 (19, 22), (22, 25), (25, 28), (28, 36), (36, 37), (0, 0)]\n",
    "            ],\n",
    "            'word_ids': [\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "            ]\n",
    "            }\n",
    "                ]\n",
    "                    }\n",
    "\n",
    "        \"\"\"\n",
    "        tokenizer = self.encoder.tokenizer\n",
    "        # ====== Reconstruct MT sentence from batch ======\n",
    "        # input_ids = batch[0][\"input_ids\"]  # Tokenized MT input\n",
    "        # mt_sentence = self.encoder.tokenizer.decode(\n",
    "        #         input_ids[0],\n",
    "        #         skip_special_tokens=True,\n",
    "        #         clean_up_tokenization_spaces=True\n",
    "        # )\n",
    "        # print(\"mt sentence:++++++++++++++++++ \", mt_sentence)\n",
    "\n",
    "        \n",
    "        ## run over the mt sentences in the dict word_ids\n",
    "        word_level_prob = []\n",
    "        all_tokenized_sentences = []\n",
    "        for index, item in enumerate(word_ids[\"words_id\"]):\n",
    "            mt_sentence = word_ids[\"mt_sentences\"][index]\n",
    "            print(\"mt_sentence: \", mt_sentence)\n",
    "            # Tokenize the MT sentence to get subword-to-token alignment\n",
    "            tokenized = self.encoder.tokenizer(\n",
    "                    mt_sentence,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                            \n",
    "            )\n",
    "        \n",
    "            #print(\"tokenized: \", tokenized)\n",
    "            #subword_ids = tokenized.word_ids()\n",
    "            #subword_ids = subword_ids[:seq_len]\n",
    "            #subword_ids[-1] = None\n",
    "            #print(\"subword_ids after mt sentence extractions: \", subword_ids)\n",
    "            # Group subword probabilities by original tokens\n",
    "            subword_ids = item\n",
    "            token_probs = {}\n",
    "            print(f\"subword_probs[{index}]:  {subword_probs[index]}\")\n",
    "            attention_mask = word_ids[\"mt_sentences_tokenized\"][0][\"attention_mask\"][index]\n",
    "            for idx, prob in enumerate(subword_probs[index]):\n",
    "                # if attention_mask[index] == 0:\n",
    "                #     break\n",
    "                if idx >= len(subword_ids):\n",
    "                    break\n",
    "                subword_idx = subword_ids[idx]\n",
    "                if subword_idx is None:  # Skip special tokens\n",
    "                    continue\n",
    "                if subword_idx not in token_probs:\n",
    "                    token_probs[subword_idx] = []\n",
    "                token_probs[subword_idx].append(prob.cpu().numpy())\n",
    "            print(\"token_probs: \", token_probs)\n",
    "      \n",
    "            # Aggregate probabilities (average for each class)\n",
    "            token_level_probs = []\n",
    "            for token_idx in sorted(token_probs.keys()):\n",
    "                # Stack subword probabilities for this token\n",
    "                subword_probs_for_token = torch.stack([torch.tensor(p) for p in token_probs[token_idx]])\n",
    "        \n",
    "                # Compute mean across subwords (dim=0 → average over subwords, per class)\n",
    "                mean_probs = torch.mean(subword_probs_for_token, dim=0)\n",
    "        \n",
    "                token_level_probs.append(mean_probs.numpy())\n",
    "            print(\"token_level_probs: \", token_level_probs)\n",
    "    \n",
    "            # After computing token_level_probs:\n",
    "    \n",
    "            # Tolerance for floating-point errors (e.g., 1e-3 = 0.1% tolerance)\n",
    "            tolerance = 1e-3\n",
    "    \n",
    "            for token_idx, probs in enumerate(token_level_probs):\n",
    "                total = numpy.sum(probs)\n",
    "                if not numpy.isclose(total, 1.0, atol=tolerance):\n",
    "                    print(f\"Token {token_idx} probabilities sum to {total:.4f} (expected ~1.0)\")\n",
    "                else:\n",
    "                    print(f\"Token {token_idx} probabilities sum to {total:.4f}\")\n",
    "\n",
    "                        \n",
    "            # Extract word IDs (index of the original word for each token)\n",
    "            #word_ids = tokenized.word_ids()[:mt_length]    #  [None, 0, 0, 1, 1, 2, ...]\n",
    "    \n",
    "            # Convert token IDs to tokens (subwords)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "            print(\"tokens: \",tokens)\n",
    "            # Group tokens by their word ID\n",
    "            word_to_tokens = {}\n",
    "            #print(\"word_ids: \", subword_ids)\n",
    "            for idx, word_id in enumerate(subword_ids):\n",
    "                if word_id is None:\n",
    "                    continue  # Skip special tokens like [CLS], [SEP]\n",
    "                if word_id not in word_to_tokens:\n",
    "                    word_to_tokens[word_id] = []\n",
    "                word_to_tokens[word_id].append(tokens[idx])\n",
    "            print(\"word_to_tokens: \", word_to_tokens)\n",
    "    \n",
    "            print(\"sorted(word_to_tokens.keys()) :\", sorted(word_to_tokens.keys()))\n",
    "            # Reconstruct original words from grouped tokens\n",
    "            word_mapping = []\n",
    "            for word_id in sorted(word_to_tokens.keys()):\n",
    "                tokens = word_to_tokens[word_id]\n",
    "                # Merge subwords into a single string (handles ## prefixes)\n",
    "                word = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "                word_mapping.append(word)\n",
    "    \n",
    "            # Print results\n",
    "            print(\"Tokenized Words:\", word_mapping)\n",
    "            all_tokenized_sentences.append(word_mapping)\n",
    "            # Map tokens to probabilities\n",
    "            token_predictions = [\n",
    "                    {\"token\": token, \"probabilities\": probs.tolist()}\n",
    "                    for token, probs in zip(word_mapping, token_level_probs)\n",
    "            ]\n",
    "    \n",
    "            # print(\"Token-Level Probabilities:\")\n",
    "            for pred in token_predictions:\n",
    "                print(f\"{pred['token']}: {pred['probabilities']}\")\n",
    "            print(\"first sentence finished=====================\")\n",
    "            word_level_prob.append(token_predictions)\n",
    "        return word_level_prob, all_tokenized_sentences\n",
    "\n",
    "    def correct_span(\n",
    "        self,\n",
    "        track_token_to_words: List[int],\n",
    "        mt_offsets: List[Tuple[int, int]],\n",
    "        word_ids: Dict,\n",
    "        Tokenized_Words: List[List[str]]\n",
    "    ):\n",
    "        # track_token_to_words = [-1, -1, -1, 3, 4, -1, -1, -1, 8, -1, 10, 11, 12, 13, 14, -1]\n",
    "        # mt_offsets = [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]\n",
    "        # word_ids =  [None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]\n",
    "        mapping = {}\n",
    "        print(\"==========================================\")\n",
    "        print(\"mt_offsets in the correct_span: \", mt_offsets)\n",
    "        for index, item in enumerate(word_ids):\n",
    "            if item is None:\n",
    "                continue\n",
    "        \n",
    "            if item in mapping:\n",
    "                # Append to existing lists\n",
    "                mapping[item]['subwords'].append(index)\n",
    "                # last_offset = mt_offsets[0][index][1]\n",
    "                last_offset = mt_offsets[index][1]\n",
    "                mapping[item]['offsets'][1] = last_offset\n",
    "            else:\n",
    "                # Initialize new entry\n",
    "                \n",
    "                mapping[item] = {\n",
    "                    'subwords': [index],\n",
    "                    'offsets': list(mt_offsets[index])\n",
    "                }\n",
    "        \n",
    "        print(mapping)\n",
    "                \n",
    "        start = False\n",
    "        from collections import OrderedDict\n",
    "        words_in_span = []  # Creating an ordered set\n",
    "        all_word_spans = defaultdict()\n",
    "        set_to_check_multiple_subwords = set()\n",
    "        index = 0\n",
    "        for item in track_token_to_words:\n",
    "            if item == -1:\n",
    "                if start == True:\n",
    "                    start = False\n",
    "                    text = \"\"\n",
    "                    print(words_in_span)\n",
    "                    for item in words_in_span:\n",
    "                        text += f\" {Tokenized_Words[item]}\"\n",
    "                    print(text)\n",
    "                    word_span = defaultdict()\n",
    "                    word_span['text'] = text.strip()\n",
    "                    word_span['start'] = mapping[words_in_span[0]]['offsets'][0]\n",
    "                    word_span['end'] = mapping[words_in_span[-1]]['offsets'][1]\n",
    "                    print(\"word span: \", word_span)\n",
    "                    all_word_spans[index] = word_span\n",
    "                    index += 1\n",
    "                    words_in_span= []\n",
    "            else:\n",
    "                print(\"item: \", item)\n",
    "                start = True\n",
    "                word = word_ids[item]\n",
    "                if word not in set_to_check_multiple_subwords:\n",
    "                    set_to_check_multiple_subwords.add(word)\n",
    "                    words_in_span.append(word)\n",
    "                    \n",
    "        print(\"all_word_spans: \", all_word_spans)\n",
    "        return all_word_spans\n",
    "\n",
    "\n",
    "        \n",
    "    def decode(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        mt_offsets: torch.Tensor,\n",
    "        word_id:Dict[\n",
    "        str,\n",
    "        Union[\n",
    "            List[List[Optional[int]]],  # words_id\n",
    "            List[str],  # mt_sentences\n",
    "            List[Dict[\n",
    "                str,\n",
    "                Union[\n",
    "                    torch.Tensor,  # input_ids/label_ids/attention_mask\n",
    "                    List[List[Tuple[int, int]]],  # offsets\n",
    "                    List[List[Optional[int]]]  # word_ids\n",
    "            ]]\n",
    "        ]\n",
    "    ]\n",
    "        ]  # Added by me\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Decode error spans from subwords.\n",
    "\n",
    "        Args:\n",
    "            subword_probs (torch.Tensor): probabilities of each label for each subword.\n",
    "            input_ids (torch.Tensor): input ids from the model.\n",
    "            mt_offsets (torch.Tensor): subword offsets.\n",
    "            word_id(dict): A dictionary that contains words_id mapping to all MT sentences,\n",
    "            raw MT sentences and tokenized mt sentences\n",
    "        Return:\n",
    "            List with of dictionaries with text, start, end, severity and a\n",
    "            confidence score which is the average of the probs for that label.\n",
    "        \"\"\"\n",
    "        print(\"====================== decode function ========================\")\n",
    "        # print(\"subword_probs: \", subword_probs)\n",
    "        # print(\"input_ids: \", input_ids)\n",
    "        # print(\"mt_offsets: \", mt_offsets)\n",
    "        decoded_output = []\n",
    "        decoded_output_corrected = []\n",
    "        print(\"now the word_level_prob function is being called: \")\n",
    "        word_level_prob, all_tokenized_sentences = self.word_level_prob(subword_probs,word_id)\n",
    "        #all_tokenized_sentences:List[List[str]]\n",
    "        print(\"all_tokenized_sentences: \", all_tokenized_sentences)\n",
    "        #print(\"length of mt_offsets: \", len(mt_offsets))\n",
    "        for i in range(len(mt_offsets)):\n",
    "            print(\"the value of i is ========= before inner for loop: \", i)\n",
    "            seq_len = len(mt_offsets[i])\n",
    "            #print(\"seq_len: \", seq_len)\n",
    "            error_spans, in_span, span = [], False, {}\n",
    "\n",
    "            track_token_to_words = []\n",
    "            count_index = 0\n",
    "            for token_id, probs, token_offset in zip(\n",
    "                input_ids[i, :seq_len], subword_probs[i][:seq_len], mt_offsets[i]\n",
    "            ):  \n",
    "                #print(\"the value of i is: \", i)\n",
    "                #print(\"token_id :\", token_id, \", probs: \", probs, \", token_offset: \", token_offset)\n",
    "                if self.decoding_threshold:\n",
    "                    print(\"decoding threshold is set\")\n",
    "                    if torch.sum(probs[1:]) > self.decoding_threshold:\n",
    "                        \n",
    "                        print(\"token_id who has higher error sums than threshold: \",token_id)\n",
    "                        print(\"and sum of it: \", torch.sum(probs[1:]))\n",
    "                        probability, label_value = torch.topk(probs[1:], 1)\n",
    "                        label_value += 1  # offset from removing label 0\n",
    "                    else:\n",
    "                        print(\"token_id who has higher error sums than threshold: \",token_id)\n",
    "\n",
    "                        # This is just to ensure same format but at this point\n",
    "                        # we will only look at label 0 and its prob\n",
    "                        probability, label_value = torch.topk(probs[0], 1)\n",
    "                        #print(\"probs[0] =============\", probs[0])\n",
    "                else:\n",
    "                    print(\"no decoding threshold set\")\n",
    "                    probability, label_value = torch.topk(probs, 1)\n",
    "\n",
    "                # Some torch versions topk returns a shape 1 tensor with only\n",
    "                # a item inside\n",
    "                label_value = (\n",
    "                    label_value.item()\n",
    "                    if label_value.dim() < 1\n",
    "                    else label_value[0].item()\n",
    "                )\n",
    "                label = self.label_encoder.ids_to_label.get(label_value)\n",
    "                #print(\"===================================================\")\n",
    "                #print(\"label: \", label)\n",
    "                # Label set:\n",
    "                # O I-minor I-major\n",
    "                # Begin of annotation span\n",
    "                if label.startswith(\"I\") and not in_span:\n",
    "                    in_span = True\n",
    "                    span[\"tokens\"] = [\n",
    "                        token_id,\n",
    "                    ]\n",
    "                    span[\"severity\"] = label.split(\"-\")[1]\n",
    "                    span[\"offset\"] = list(token_offset)\n",
    "                    #span[\"offset_word] = [list(token_offset)]\n",
    "                    span[\"confidence\"] = [\n",
    "                        probability,\n",
    "                    ]\n",
    "                    span[\"check severity\"] = [label.split(\"-\")[1]]\n",
    "                    track_token_to_words.append(count_index)\n",
    "                    #span[\"word_indices\"] =  set(word_id) if isinstance(word_id, list) else {word_id},  # Track word indices\n",
    "                # Inside an annotation span\n",
    "                elif label.startswith(\"I\") and in_span:\n",
    "                    span[\"tokens\"].append(token_id)\n",
    "                    span[\"confidence\"].append(probability)\n",
    "                    # Update offset end\n",
    "                    span[\"offset\"][1] = token_offset[1]\n",
    "                    #span[\"offset_word] = [list(token_offset)]\n",
    "                    span[\"check severity\"].append(label.split(\"-\")[1])\n",
    "                    # if isinstance(word_id, list):\n",
    "                    #     span[\"word_indices\"].update(word_id)\n",
    "                    # else:\n",
    "                    #     span[\"word_indices\"].add(word_id)\n",
    "                    track_token_to_words.append(count_index)\n",
    "                # annotation span finished.\n",
    "                elif label == \"O\" and in_span:\n",
    "                    error_spans.append(span)\n",
    "                    in_span, span = False, {}\n",
    "                    track_token_to_words.append(-1)\n",
    "                #added by me\n",
    "                elif label == \"O\" and not in_span:\n",
    "                    track_token_to_words.append(-1)\n",
    "                count_index = count_index + 1\n",
    "\n",
    "            print(\"track_token_to_words: \", track_token_to_words)\n",
    "\n",
    "            sentence_output = []\n",
    "            for span in error_spans:\n",
    "                # # Collect unique word indices for the span\n",
    "                # unique_word_indices = sorted(set(span[\"word_indices\"]) - {None})  # Remove None safely\n",
    "\n",
    "                # # Extract words belonging to this specific span\n",
    "                # span_words = [tokenized_words[idx] for idx in unique_word_indices]\n",
    "\n",
    "                sentence_output.append(\n",
    "                    {\n",
    "                        \n",
    "                        \"text\": self.encoder.tokenizer.decode(span[\"tokens\"]),\n",
    "                        #\"text\": \" \".join(span_words),  # Use words instead of tokens\n",
    "                        \"confidence\": torch.concat(span[\"confidence\"]).mean().item(),\n",
    "                        \"severity\": span[\"severity\"],\n",
    "                        \"start\": span[\"offset\"][0],\n",
    "                        \"end\": span[\"offset\"][1],\n",
    "                        \"check severity\": span[\"check severity\"]\n",
    "                    }\n",
    "                )\n",
    "            decoded_output.append(sentence_output)\n",
    "            print(\"sentence_output: \", sentence_output)\n",
    "            \n",
    "            #get corrected word level error span\n",
    "            corrected_span = self.correct_span(track_token_to_words,mt_offsets[i], word_id[\"words_id\"][i],all_tokenized_sentences[i])\n",
    "            sentence_out_word_level = []\n",
    "            count = 0\n",
    "            for error_span in sentence_output:\n",
    "                sentence_out_word_level.append({\n",
    "                    \"text\": corrected_span[count]['text'],\n",
    "                    \"confidence\": error_span['confidence'],\n",
    "                    \"severity\": error_span[\"severity\"],\n",
    "                    \"start\": corrected_span[count]['start'],\n",
    "                    \"end\": corrected_span[count]['end'],\n",
    "                })\n",
    "                count += 1\n",
    "                    \n",
    "            print(\"sentence_out_word_level: \",sentence_out_word_level)\n",
    "            decoded_output_corrected.append(sentence_out_word_level)\n",
    "        print(\"decoded_output_corrected\", decoded_output_corrected)\n",
    "        #decoded_output.append(decoded_output_corrected)\n",
    "        return decoded_output, decoded_output_corrected, word_level_prob\n",
    "\n",
    "    \n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: Dict[str, torch.Tensor],\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Prediction:\n",
    "        \"\"\"PyTorch Lightning predict_step\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, torch.Tensor]): The output of your prepare_sample function\n",
    "            batch_idx (Optional[int], optional): Integer displaying which batch this is\n",
    "                    Defaults to None.\n",
    "            dataloader_idx (Optional[int], optional): Integer displaying which\n",
    "                dataloader this is. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Prediction: Model Prediction\n",
    "        \"\"\"\n",
    "        # # Get the caller's function name and file location\n",
    "        # caller_frame = inspect.stack()[1]\n",
    "        # caller_function = caller_frame.function\n",
    "        # caller_file = caller_frame.filename\n",
    "    \n",
    "        # # Print caller details\n",
    "        # print(f\"predict_step called by: {caller_function} (from {caller_file})\")\n",
    "        if len(batch) == 4: # after adding word_ids, the batch length will increase by 1S\n",
    "            print(\"batch: \", batch)\n",
    "            print(\"i am inside when len of the batch is 3\")\n",
    "            #print(\"word ids: \", batch[-1])\n",
    "            # predictions = [self.forward(**input_seq) for input_seq in batch]\n",
    "            # now the length of the batch is 4, 4th dictionary is the words_id \n",
    "            #dictionary, and we will skip the forward call to it\n",
    "            predictions = [self.forward(**input_seq) for input_seq in batch[:-1]]\n",
    "            \n",
    "            #print(\"predictions: \", predictions)\n",
    "            avg_scores = torch.stack([pred.score for pred in predictions], dim=0).mean(dim=0)\n",
    "            #print(\"avg scores\", avg_scores)\n",
    "            batch_prediction = Prediction(\n",
    "                scores=avg_scores,\n",
    "                metadata=Prediction(\n",
    "                    src_scores=predictions[0].score,\n",
    "                    ref_scores=predictions[1].score,\n",
    "                    unified_scores=predictions[2].score,\n",
    "                ),\n",
    "            )\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max() \n",
    "                subword_probs = [\n",
    "                    nn.functional.softmax(o.logits, dim=2)[:, :seq_len, :] * w\n",
    "                    for w, o in zip(self.input_weights_spans, predictions)\n",
    "                ]\n",
    "                subword_probs = torch.sum(torch.stack(subword_probs), dim=0)\n",
    "                \n",
    "                ########################################################3\n",
    "                ## create error span using decode function\n",
    "                word_ids = batch[-1].copy()\n",
    "                error_spans, corrected_error_spans, word_level_prob = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"], word_ids\n",
    "                    #,word_ids, word_mapping\n",
    "                )\n",
    "                batch_prediction.metadata[\"error_spans\"] = error_spans\n",
    "                batch_prediction.metadata[\"corrected_error_spans\"] = corrected_error_spans\n",
    "                batch_prediction.metadata[\"word_level_probability\"]=word_level_prob\n",
    "        else:\n",
    "            print(\"i am inside when len of the batch is not 3\")\n",
    "            model_output = self.forward(**batch[0])\n",
    "            batch_prediction = Prediction(scores=model_output.score)\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = nn.functional.softmax(model_output.logits, dim=2)[:, :seq_len, :]\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction = Prediction(\n",
    "                    scores=model_output.score,\n",
    "                    metadata=Prediction(error_spans=error_spans),\n",
    "                )\n",
    "        return batch_prediction\n",
    "\n",
    "# Load checkpoint into your custom class\n",
    "path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(path,strict = False)\n",
    "data = [\n",
    "    {\n",
    "        \"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n",
    "        \"mt\": \"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\",\n",
    "        \"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n",
    "    }\n",
    "]\n",
    "# data = [\n",
    "#     {\n",
    "#         \"src\": \"10 到 15 分钟可以送到吗\",\n",
    "#         \"mt\": \"Can I receive my food in 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"src\": \"Pode ser entregue dentro de 10 a 15 minutos?\",\n",
    "#         \"mt\": \"Can you send it for 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     }\n",
    "# ]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)\n",
    "\n",
    "#added by me\n",
    "print (model_output.metadata.corrected_error_spans)\n",
    "print (model_output.metadata.word_level_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08a596-ed05-4bd8-9084-2bd183591135",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d5f4f-bce7-4a98-a396-f2caa58fedbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
