{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c74feef-ca12-471f-a459-efbe05312be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/rahmang/xcomet/COMET\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb94a1a-08ad-49e0-b98e-016bfb31c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 19 16:55:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:61:00.0 Off |                    0 |\n",
      "|  0%   37C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa2059f-86d8-40d4-aee9-a608b1843dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-ab9f9e85-81f2-18d0-531c-1b30853fbaad]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"mt\"]: ['Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst']\n",
      "input input_sequences just with MT: in the prepare_sample in unified_metric  [{'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2]]), 'label_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]], 'word_ids': [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ({'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,  32686,  23962,     98, 121303,    111,   1238, 141775,    678,\n",
      "           6653,     53,  10646,      7,      2]], device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]], device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}, {'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,      7,    873,  54359,     18,  16587,   1079,   6653,     53,\n",
      "              9,   9088,      7,  16158,    644,    122,   1519,   7340,      2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]],\n",
      "       device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}, {'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2,      2,  67151,\n",
      "          59520,  32686,  23962,     98, 121303,    111,   1238, 141775,    678,\n",
      "           6653,     53,  10646,      7,      2,      2,  67151,  59520,      7,\n",
      "            873,  54359,     18,  16587,   1079,   6653,     53,      9,   9088,\n",
      "              7,  16158,    644,    122,   1519,   7340,      2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True]], device='cuda:0'), 'label_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "       device='cuda:0'), 'mt_offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]}, {'words_id': [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]], 'mt_sentences': ['Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst'], 'mt_sentences_tokenized': [{'input_ids': tensor([[     0,  67151,  59520,    443,   1079,   6653,     53,      9, 241832,\n",
      "             19,  86454,     23,    122,  25706,    271,      2]],\n",
      "       device='cuda:0'), 'label_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'offsets': [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]], 'word_ids': [[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]]}]})\n",
      "i am inside when len of the batch is 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== decode function ========================\n",
      "now the word_level_prob function is being called: \n",
      "mt_sentence:  Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\n",
      "subword_probs[0]:  tensor([[0.3554, 0.2145, 0.2102, 0.2199],\n",
      "        [0.5846, 0.1495, 0.1520, 0.1139],\n",
      "        [0.6238, 0.1366, 0.1445, 0.0952],\n",
      "        [0.1280, 0.1513, 0.2221, 0.4986],\n",
      "        [0.3183, 0.1446, 0.2166, 0.3205],\n",
      "        [0.5808, 0.1740, 0.1477, 0.0975],\n",
      "        [0.4583, 0.2637, 0.2076, 0.0703],\n",
      "        [0.4516, 0.2554, 0.2178, 0.0751],\n",
      "        [0.2695, 0.2651, 0.2737, 0.1917],\n",
      "        [0.2902, 0.2185, 0.2517, 0.2396],\n",
      "        [0.1104, 0.1495, 0.2237, 0.5164],\n",
      "        [0.1130, 0.1312, 0.1900, 0.5659],\n",
      "        [0.1128, 0.1399, 0.2107, 0.5366],\n",
      "        [0.1422, 0.1559, 0.2223, 0.4797],\n",
      "        [0.1456, 0.1444, 0.1989, 0.5111],\n",
      "        [0.5671, 0.1140, 0.1744, 0.1444]], device='cuda:0')\n",
      "token_probs:  {0: [array([0.58458394, 0.14953566, 0.15199055, 0.11388987], dtype=float32)], 1: [array([0.6237879 , 0.13660619, 0.14445591, 0.09515005], dtype=float32)], 2: [array([0.12797818, 0.15131377, 0.22207925, 0.4986288 ], dtype=float32)], 3: [array([0.31826818, 0.14464115, 0.21661992, 0.32047075], dtype=float32)], 4: [array([0.5808079 , 0.17400217, 0.1476797 , 0.09751016], dtype=float32), array([0.45830816, 0.26374027, 0.20764956, 0.07030205], dtype=float32), array([0.45157897, 0.25544292, 0.21784857, 0.07512955], dtype=float32), array([0.26951373, 0.265112  , 0.27366346, 0.19171074], dtype=float32), array([0.29019523, 0.21847951, 0.25173083, 0.23959441], dtype=float32)], 5: [array([0.11040767, 0.1495493 , 0.22367153, 0.5163715 ], dtype=float32)], 6: [array([0.11297599, 0.13117053, 0.18996915, 0.5658843 ], dtype=float32)], 7: [array([0.11280775, 0.13989264, 0.21065268, 0.53664696], dtype=float32)], 8: [array([0.14216733, 0.15589082, 0.22228944, 0.47965246], dtype=float32), array([0.14558803, 0.14441752, 0.19892526, 0.5110692 ], dtype=float32)]}\n",
      "token_level_probs:  [array([0.58458394, 0.14953566, 0.15199055, 0.11388987], dtype=float32), array([0.6237879 , 0.13660619, 0.14445591, 0.09515005], dtype=float32), array([0.12797818, 0.15131377, 0.22207925, 0.4986288 ], dtype=float32), array([0.31826818, 0.14464115, 0.21661992, 0.32047075], dtype=float32), array([0.41008082, 0.23535538, 0.21971443, 0.13484938], dtype=float32), array([0.11040767, 0.1495493 , 0.22367153, 0.5163715 ], dtype=float32), array([0.11297599, 0.13117053, 0.18996915, 0.5658843 ], dtype=float32), array([0.11280775, 0.13989264, 0.21065268, 0.53664696], dtype=float32), array([0.14387769, 0.15015417, 0.21060735, 0.49536082], dtype=float32)]\n",
      "Token 0 probabilities sum to 1.0000\n",
      "Token 1 probabilities sum to 1.0000\n",
      "Token 2 probabilities sum to 1.0000\n",
      "Token 3 probabilities sum to 1.0000\n",
      "Token 4 probabilities sum to 1.0000\n",
      "Token 5 probabilities sum to 1.0000\n",
      "Token 6 probabilities sum to 1.0000\n",
      "Token 7 probabilities sum to 1.0000\n",
      "Token 8 probabilities sum to 1.0000\n",
      "tokens:  ['<s>', '▁Boris', '▁Johnson', '▁ist', '▁bei', '▁Tor', 'y', '-', 'Abgeordnete', 'n', '▁völlig', '▁in', '▁der', '▁Gun', 'st', '</s>']\n",
      "word_to_tokens:  {0: ['▁Boris'], 1: ['▁Johnson'], 2: ['▁ist'], 3: ['▁bei'], 4: ['▁Tor', 'y', '-', 'Abgeordnete', 'n'], 5: ['▁völlig'], 6: ['▁in'], 7: ['▁der'], 8: ['▁Gun', 'st']}\n",
      "sorted(word_to_tokens.keys()) : [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Tokenized Words: ['Boris', 'Johnson', 'ist', 'bei', 'Tory-Abgeordneten', 'völlig', 'in', 'der', 'Gunst']\n",
      "Boris: [0.5845839381217957, 0.1495356559753418, 0.15199054777622223, 0.11388987302780151]\n",
      "Johnson: [0.6237878799438477, 0.13660618662834167, 0.1444559097290039, 0.09515005350112915]\n",
      "ist: [0.12797817587852478, 0.15131376683712006, 0.22207924723625183, 0.49862879514694214]\n",
      "bei: [0.31826817989349365, 0.14464114606380463, 0.21661992371082306, 0.32047075033187866]\n",
      "Tory-Abgeordneten: [0.41008082032203674, 0.23535537719726562, 0.2197144329547882, 0.13484938442707062]\n",
      "völlig: [0.11040766537189484, 0.14954930543899536, 0.22367152571678162, 0.516371488571167]\n",
      "in: [0.11297599226236343, 0.13117052614688873, 0.18996915221214294, 0.5658842921257019]\n",
      "der: [0.1128077507019043, 0.13989263772964478, 0.2106526792049408, 0.5366469621658325]\n",
      "Gunst: [0.14387768507003784, 0.15015417337417603, 0.2106073498725891, 0.4953608214855194]\n",
      "first sentence finished=====================\n",
      "all_tokenized_sentences:  [['Boris', 'Johnson', 'ist', 'bei', 'Tory-Abgeordneten', 'völlig', 'in', 'der', 'Gunst']]\n",
      "the value of i is ========= before inner for loop:  0\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "no decoding threshold set\n",
      "track_token_to_words:  [-1, -1, -1, 3, 4, -1, -1, -1, 8, -1, 10, 11, 12, 13, 14, -1]\n",
      "sentence_output:  [{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21, 'check severity': ['critical', 'critical']}, {'text': 'Abgeordnete', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 27, 'end': 38, 'check severity': ['major']}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59, 'check severity': ['critical', 'critical', 'critical', 'critical', 'critical']}]\n",
      "==========================================\n",
      "mt_offsets in the correct_span:  [(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]\n",
      "{0: {'subwords': [1], 'offsets': [0, 5]}, 1: {'subwords': [2], 'offsets': [5, 13]}, 2: {'subwords': [3], 'offsets': [13, 17]}, 3: {'subwords': [4], 'offsets': [17, 21]}, 4: {'subwords': [5, 6, 7, 8, 9], 'offsets': [21, 39]}, 5: {'subwords': [10], 'offsets': [39, 46]}, 6: {'subwords': [11], 'offsets': [46, 49]}, 7: {'subwords': [12], 'offsets': [49, 53]}, 8: {'subwords': [13, 14], 'offsets': [53, 59]}}\n",
      "item:  3\n",
      "item:  4\n",
      "[2, 3]\n",
      " ist bei\n",
      "word span:  defaultdict(None, {'text': 'ist bei', 'start': 13, 'end': 21})\n",
      "item:  8\n",
      "[4]\n",
      " Tory-Abgeordneten\n",
      "word span:  defaultdict(None, {'text': 'Tory-Abgeordneten', 'start': 21, 'end': 39})\n",
      "item:  10\n",
      "item:  11\n",
      "item:  12\n",
      "item:  13\n",
      "item:  14\n",
      "[5, 6, 7, 8]\n",
      " völlig in der Gunst\n",
      "word span:  defaultdict(None, {'text': 'völlig in der Gunst', 'start': 39, 'end': 59})\n",
      "all_word_spans:  defaultdict(None, {0: defaultdict(None, {'text': 'ist bei', 'start': 13, 'end': 21}), 1: defaultdict(None, {'text': 'Tory-Abgeordneten', 'start': 21, 'end': 39}), 2: defaultdict(None, {'text': 'völlig in der Gunst', 'start': 39, 'end': 59})})\n",
      "sentence_out_word_level:  [{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21}, {'text': 'Tory-Abgeordneten', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 21, 'end': 39}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59}]\n",
      "decoded_output_corrected [[{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21}, {'text': 'Tory-Abgeordneten', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 21, 'end': 39}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6365968585014343]\n",
      "0.6365968585014343\n",
      "[[{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21, 'check severity': ['critical', 'critical']}, {'text': 'Abgeordnete', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 27, 'end': 38, 'check severity': ['major']}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59, 'check severity': ['critical', 'critical', 'critical', 'critical', 'critical']}]]\n",
      "[[{'text': 'ist bei', 'confidence': 0.4095497727394104, 'severity': 'critical', 'start': 13, 'end': 21}, {'text': 'Tory-Abgeordneten', 'confidence': 0.2736634612083435, 'severity': 'major', 'start': 21, 'end': 39}, {'text': 'völlig in der Gunst', 'confidence': 0.5219249129295349, 'severity': 'critical', 'start': 39, 'end': 59}]]\n",
      "[[{'token': 'Boris', 'probabilities': [0.5845839381217957, 0.1495356559753418, 0.15199054777622223, 0.11388987302780151]}, {'token': 'Johnson', 'probabilities': [0.6237878799438477, 0.13660618662834167, 0.1444559097290039, 0.09515005350112915]}, {'token': 'ist', 'probabilities': [0.12797817587852478, 0.15131376683712006, 0.22207924723625183, 0.49862879514694214]}, {'token': 'bei', 'probabilities': [0.31826817989349365, 0.14464114606380463, 0.21661992371082306, 0.32047075033187866]}, {'token': 'Tory-Abgeordneten', 'probabilities': [0.41008082032203674, 0.23535537719726562, 0.2197144329547882, 0.13484938442707062]}, {'token': 'völlig', 'probabilities': [0.11040766537189484, 0.14954930543899536, 0.22367152571678162, 0.516371488571167]}, {'token': 'in', 'probabilities': [0.11297599226236343, 0.13117052614688873, 0.18996915221214294, 0.5658842921257019]}, {'token': 'der', 'probabilities': [0.1128077507019043, 0.13989263772964478, 0.2106526792049408, 0.5366469621658325]}, {'token': 'Gunst', 'probabilities': [0.14387768507003784, 0.15015417337417603, 0.2106073498725891, 0.4953608214855194]}]]\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "from comet.models.utils import Prediction\n",
    "from typing import Dict, Optional\n",
    "from typing import List, Dict\n",
    "from typing import Union, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import inspect \n",
    "import torch\n",
    "import torch.nn as nn  # <-- Add thiss\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "\n",
    "    def prepare_sample(\n",
    "        self, sample: List[Dict[str, Union[str, float]]], stage: str = \"fit\"\n",
    "    ) -> Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Tokenizes input data and prepares targets for training.\n",
    "\n",
    "        Args:\n",
    "            sample (List[Dict[str, Union[str, float]]]): Mini-batch\n",
    "            stage (str, optional): Model stage ('train' or 'predict'). Defaults to \"fit\".\n",
    "\n",
    "        Returns:\n",
    "            Union[Tuple[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]]: Model input\n",
    "                and targets.\n",
    "        \"\"\"\n",
    "        # print(\"=================++++++++++++++++++++++++++++++++++==========================\")\n",
    "\n",
    "        # Get the caller's function name and file location\n",
    "        # caller_frame = inspect.stack()[1]\n",
    "        # caller_function = caller_frame.function\n",
    "        # caller_file = caller_frame.filename\n",
    "    \n",
    "        # # Print caller details\n",
    "        # print(f\"prepare_sample called by: {caller_function} (from {caller_file})\")\n",
    "        # print(\"sample in the prepare_sample: \", sample)\n",
    "        # print(\"the stage is: \", stage)\n",
    "        # for k in sample[0]:\n",
    "        #     print(\"K is: \", k)\n",
    "        inputs = {k: [d[k] for d in sample] for k in sample[0]}\n",
    "        print(f'''inputs[\"mt\"]: {inputs[\"mt\"]}''')\n",
    "        # only this will return word_ids from self.encoder.prepare_sample ->\n",
    "        # self.encoder.subword_tokenize as it is set self.word_level\n",
    "        # for src and ref, self.word_level = False by default\n",
    "        input_sequences = [\n",
    "            self.encoder.prepare_sample(inputs[\"mt\"], self.word_level, None),\n",
    "        ]\n",
    "        input_sequences_mt = input_sequences.copy()  # Now independent of input_sequences #added by me\n",
    "        print(\"input input_sequences just with MT: in the prepare_sample in unified_metric \", input_sequences_mt)\n",
    "\n",
    "        src_input, ref_input = False, False\n",
    "        if (\"src\" in inputs) and (\"src\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"src\"]))\n",
    "            src_input = True\n",
    "\n",
    "        if (\"ref\" in inputs) and (\"ref\" in self.hparams.input_segments):\n",
    "            input_sequences.append(self.encoder.prepare_sample(inputs[\"ref\"]))\n",
    "            ref_input = True\n",
    "        # print(\"input_sequences after adding source and ref: \")\n",
    "        # for inp in input_sequences:\n",
    "        #     print(inp)\n",
    "        # print(\"input_sequences after adding source and ref: \")\n",
    "        unified_input = src_input and ref_input\n",
    "        model_inputs = self.concat_inputs(input_sequences, unified_input) #updated unified_metric's\n",
    "        #concat_inputs function to return word_ids\n",
    "        #print(\"model inputs ++++++++++++++: \", model_inputs)\n",
    "        if stage == \"predict\":\n",
    "            #print(\"word ids: \", model_inputs[])\n",
    "            #return model_inputs[\"inputs\"]\n",
    "            #, model_inputs[\"word_ids\"] #model_inputs[\"word_ids\"] added by me\n",
    "            all_inputs = model_inputs[\"inputs\"] \n",
    "            #added by me\n",
    "            words_id_dict = {\n",
    "                \"words_id\": model_inputs[\"word_ids\"],\n",
    "                \"mt_sentences\": inputs[\"mt\"],\n",
    "                \"mt_sentences_tokenized\": input_sequences_mt\n",
    "            }\n",
    "            updated = all_inputs + (words_id_dict,)\n",
    "            #print(\"updated dict ========\", updated)\n",
    "            #Update the OrderedDict\n",
    "            model_inputs[\"inputs\"] = updated\n",
    "            return model_inputs[\"inputs\"]\n",
    "        scores = [float(s) for s in inputs[\"score\"]]\n",
    "        targets = Target(score=torch.tensor(scores, dtype=torch.float))\n",
    "\n",
    "        if \"system\" in inputs:\n",
    "            targets[\"system\"] = inputs[\"system\"]\n",
    "\n",
    "        if self.word_level:\n",
    "            # Labels will be the same accross all inputs because we are only\n",
    "            # doing sequence tagging on the MT. We will only use the mask corresponding\n",
    "            # to the MT segment.\n",
    "            seq_len = model_inputs[\"mt_length\"].max()\n",
    "            targets[\"mt_length\"] = model_inputs[\"mt_length\"]\n",
    "            targets[\"labels\"] = model_inputs[\"inputs\"][0][\"label_ids\"][:, :seq_len]\n",
    "\n",
    "        return model_inputs[\"inputs\"], targets\n",
    "        \n",
    "    def word_level_prob(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        word_ids: Dict[\n",
    "        str,\n",
    "        Union[\n",
    "            List[List[Optional[int]]],  # words_id\n",
    "            List[str],  # mt_sentences\n",
    "            List[Dict[\n",
    "                str,\n",
    "                Union[\n",
    "                    torch.Tensor,  # input_ids/label_ids/attention_mask\n",
    "                    List[List[Tuple[int, int]]],  # offsets\n",
    "                    List[List[Optional[int]]]  # word_ids\n",
    "            ]]\n",
    "        ]\n",
    "    ]\n",
    "        ]\n",
    "    ) -> List[List[Dict[str, float]]]:\n",
    "        \"\"\" Returns word level probability score\n",
    "        word_ids = {\n",
    "    'words_id': [\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "    ],\n",
    "    'mt_sentences': [\n",
    "        \"Can I receive my food in 10 to 15 minutes?\",\n",
    "        \"Can you send it for 10 to 15 minutes?\"\n",
    "    ],\n",
    "    'mt_sentences_tokenized': [\n",
    "        {\n",
    "            'input_ids': tensor([\n",
    "                [0, 4171, 87, 53299, 759, 15381, 23, 209, 47, 423, 14633, 32, 2],\n",
    "                [0, 4171, 398, 25379, 442, 100, 209, 47, 423, 14633, 32, 2, 1]\n",
    "            ], device='cuda:0'),\n",
    "            'label_ids': tensor([\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n",
    "            ], device='cuda:0'),\n",
    "            'attention_mask': tensor([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "            ], device='cuda:0'),\n",
    "            'offsets': [\n",
    "                [(0, 0), (0, 3), (3, 5), (5, 13), (13, 16), (16, 21), \n",
    "                 (21, 24), (24, 27), (27, 30), (30, 33), (33, 41), (41, 42), (0, 0)],\n",
    "                [(0, 0), (0, 3), (3, 7), (7, 12), (12, 15), (15, 19), \n",
    "                 (19, 22), (22, 25), (25, 28), (28, 36), (36, 37), (0, 0)]\n",
    "            ],\n",
    "            'word_ids': [\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "            ]\n",
    "            }\n",
    "                ]\n",
    "                    }\n",
    "\n",
    "        \"\"\"\n",
    "        tokenizer = self.encoder.tokenizer\n",
    "        # ====== Reconstruct MT sentence from batch ======\n",
    "        # input_ids = batch[0][\"input_ids\"]  # Tokenized MT input\n",
    "        # mt_sentence = self.encoder.tokenizer.decode(\n",
    "        #         input_ids[0],\n",
    "        #         skip_special_tokens=True,\n",
    "        #         clean_up_tokenization_spaces=True\n",
    "        # )\n",
    "        # print(\"mt sentence:++++++++++++++++++ \", mt_sentence)\n",
    "\n",
    "        \n",
    "        ## run over the mt sentences in the dict word_ids\n",
    "        word_level_prob = []\n",
    "        all_tokenized_sentences = []\n",
    "        for index, item in enumerate(word_ids[\"words_id\"]):\n",
    "            mt_sentence = word_ids[\"mt_sentences\"][index]\n",
    "            print(\"mt_sentence: \", mt_sentence)\n",
    "            # Tokenize the MT sentence to get subword-to-token alignment\n",
    "            tokenized = self.encoder.tokenizer(\n",
    "                    mt_sentence,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                            \n",
    "            )\n",
    "        \n",
    "            #print(\"tokenized: \", tokenized)\n",
    "            #subword_ids = tokenized.word_ids()\n",
    "            #subword_ids = subword_ids[:seq_len]\n",
    "            #subword_ids[-1] = None\n",
    "            #print(\"subword_ids after mt sentence extractions: \", subword_ids)\n",
    "            # Group subword probabilities by original tokens\n",
    "            subword_ids = item\n",
    "            token_probs = {}\n",
    "            print(f\"subword_probs[{index}]:  {subword_probs[index]}\")\n",
    "            attention_mask = word_ids[\"mt_sentences_tokenized\"][0][\"attention_mask\"][index]\n",
    "            for idx, prob in enumerate(subword_probs[index]):\n",
    "                # if attention_mask[index] == 0:\n",
    "                #     break\n",
    "                if idx >= len(subword_ids):\n",
    "                    break\n",
    "                subword_idx = subword_ids[idx]\n",
    "                if subword_idx is None:  # Skip special tokens\n",
    "                    continue\n",
    "                if subword_idx not in token_probs:\n",
    "                    token_probs[subword_idx] = []\n",
    "                token_probs[subword_idx].append(prob.cpu().numpy())\n",
    "            print(\"token_probs: \", token_probs)\n",
    "      \n",
    "            # Aggregate probabilities (average for each class)\n",
    "            token_level_probs = []\n",
    "            for token_idx in sorted(token_probs.keys()):\n",
    "                # Stack subword probabilities for this token\n",
    "                subword_probs_for_token = torch.stack([torch.tensor(p) for p in token_probs[token_idx]])\n",
    "        \n",
    "                # Compute mean across subwords (dim=0 → average over subwords, per class)\n",
    "                mean_probs = torch.mean(subword_probs_for_token, dim=0)\n",
    "        \n",
    "                token_level_probs.append(mean_probs.numpy())\n",
    "            print(\"token_level_probs: \", token_level_probs)\n",
    "    \n",
    "            # After computing token_level_probs:\n",
    "    \n",
    "            # Tolerance for floating-point errors (e.g., 1e-3 = 0.1% tolerance)\n",
    "            tolerance = 1e-3\n",
    "    \n",
    "            for token_idx, probs in enumerate(token_level_probs):\n",
    "                total = numpy.sum(probs)\n",
    "                if not numpy.isclose(total, 1.0, atol=tolerance):\n",
    "                    print(f\"Token {token_idx} probabilities sum to {total:.4f} (expected ~1.0)\")\n",
    "                else:\n",
    "                    print(f\"Token {token_idx} probabilities sum to {total:.4f}\")\n",
    "\n",
    "                        \n",
    "            # Extract word IDs (index of the original word for each token)\n",
    "            #word_ids = tokenized.word_ids()[:mt_length]    #  [None, 0, 0, 1, 1, 2, ...]\n",
    "    \n",
    "            # Convert token IDs to tokens (subwords)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "            print(\"tokens: \",tokens)\n",
    "            # Group tokens by their word ID\n",
    "            word_to_tokens = {}\n",
    "            #print(\"word_ids: \", subword_ids)\n",
    "            for idx, word_id in enumerate(subword_ids):\n",
    "                if word_id is None:\n",
    "                    continue  # Skip special tokens like [CLS], [SEP]\n",
    "                if word_id not in word_to_tokens:\n",
    "                    word_to_tokens[word_id] = []\n",
    "                word_to_tokens[word_id].append(tokens[idx])\n",
    "            print(\"word_to_tokens: \", word_to_tokens)\n",
    "    \n",
    "            print(\"sorted(word_to_tokens.keys()) :\", sorted(word_to_tokens.keys()))\n",
    "            # Reconstruct original words from grouped tokens\n",
    "            word_mapping = []\n",
    "            for word_id in sorted(word_to_tokens.keys()):\n",
    "                tokens = word_to_tokens[word_id]\n",
    "                # Merge subwords into a single string (handles ## prefixes)\n",
    "                word = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "                word_mapping.append(word)\n",
    "    \n",
    "            # Print results\n",
    "            print(\"Tokenized Words:\", word_mapping)\n",
    "            all_tokenized_sentences.append(word_mapping)\n",
    "            # Map tokens to probabilities\n",
    "            token_predictions = [\n",
    "                    {\"token\": token, \"probabilities\": probs.tolist()}\n",
    "                    for token, probs in zip(word_mapping, token_level_probs)\n",
    "            ]\n",
    "    \n",
    "            # print(\"Token-Level Probabilities:\")\n",
    "            for pred in token_predictions:\n",
    "                print(f\"{pred['token']}: {pred['probabilities']}\")\n",
    "            print(\"first sentence finished=====================\")\n",
    "            word_level_prob.append(token_predictions)\n",
    "        return word_level_prob, all_tokenized_sentences\n",
    "\n",
    "    def correct_span(\n",
    "        self,\n",
    "        track_token_to_words: List[int],\n",
    "        mt_offsets: List[Tuple[int, int]],\n",
    "        word_ids: Dict,\n",
    "        Tokenized_Words: List[List[str]]\n",
    "    ):\n",
    "        # track_token_to_words = [-1, -1, -1, 3, 4, -1, -1, -1, 8, -1, 10, 11, 12, 13, 14, -1]\n",
    "        # mt_offsets = [[(0, 0), (0, 5), (5, 13), (13, 17), (17, 21), (21, 25), (25, 26), (26, 27), (27, 38), (38, 39), (39, 46), (46, 49), (49, 53), (53, 57), (57, 59), (0, 0)]]\n",
    "        # word_ids =  [None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, 8, 8, None]\n",
    "        mapping = {}\n",
    "        print(\"==========================================\")\n",
    "        print(\"mt_offsets in the correct_span: \", mt_offsets)\n",
    "        for index, item in enumerate(word_ids):\n",
    "            if item is None:\n",
    "                continue\n",
    "        \n",
    "            if item in mapping:\n",
    "                # Append to existing lists\n",
    "                mapping[item]['subwords'].append(index)\n",
    "                # last_offset = mt_offsets[0][index][1]\n",
    "                last_offset = mt_offsets[index][1]\n",
    "                mapping[item]['offsets'][1] = last_offset\n",
    "            else:\n",
    "                # Initialize new entry\n",
    "                \n",
    "                mapping[item] = {\n",
    "                    'subwords': [index],\n",
    "                    'offsets': list(mt_offsets[index])\n",
    "                }\n",
    "        \n",
    "        print(mapping)\n",
    "                \n",
    "        start = False\n",
    "        from collections import OrderedDict\n",
    "        words_in_span = []  # Creating an ordered set\n",
    "        all_word_spans = defaultdict()\n",
    "        set_to_check_multiple_subwords = set()\n",
    "        index = 0\n",
    "        for item in track_token_to_words:\n",
    "            if item == -1:\n",
    "                if start == True:\n",
    "                    start = False\n",
    "                    text = \"\"\n",
    "                    print(words_in_span)\n",
    "                    for item in words_in_span:\n",
    "                        text += f\" {Tokenized_Words[item]}\"\n",
    "                    print(text)\n",
    "                    word_span = defaultdict()\n",
    "                    word_span['text'] = text.strip()\n",
    "                    word_span['start'] = mapping[words_in_span[0]]['offsets'][0]\n",
    "                    word_span['end'] = mapping[words_in_span[-1]]['offsets'][1]\n",
    "                    print(\"word span: \", word_span)\n",
    "                    all_word_spans[index] = word_span\n",
    "                    index += 1\n",
    "                    words_in_span= []\n",
    "            else:\n",
    "                print(\"item: \", item)\n",
    "                start = True\n",
    "                word = word_ids[item]\n",
    "                if word not in set_to_check_multiple_subwords:\n",
    "                    set_to_check_multiple_subwords.add(word)\n",
    "                    words_in_span.append(word)\n",
    "                    \n",
    "        print(\"all_word_spans: \", all_word_spans)\n",
    "        return all_word_spans\n",
    "\n",
    "\n",
    "        \n",
    "    def decode(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        mt_offsets: torch.Tensor,\n",
    "        word_id:Dict[\n",
    "        str,\n",
    "        Union[\n",
    "            List[List[Optional[int]]],  # words_id\n",
    "            List[str],  # mt_sentences\n",
    "            List[Dict[\n",
    "                str,\n",
    "                Union[\n",
    "                    torch.Tensor,  # input_ids/label_ids/attention_mask\n",
    "                    List[List[Tuple[int, int]]],  # offsets\n",
    "                    List[List[Optional[int]]]  # word_ids\n",
    "            ]]\n",
    "        ]\n",
    "    ]\n",
    "        ]  # Added by me\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Decode error spans from subwords.\n",
    "\n",
    "        Args:\n",
    "            subword_probs (torch.Tensor): probabilities of each label for each subword.\n",
    "            input_ids (torch.Tensor): input ids from the model.\n",
    "            mt_offsets (torch.Tensor): subword offsets.\n",
    "            word_id(dict): A dictionary that contains words_id mapping to all MT sentences,\n",
    "            raw MT sentences and tokenized mt sentences\n",
    "        Return:\n",
    "            List with of dictionaries with text, start, end, severity and a\n",
    "            confidence score which is the average of the probs for that label.\n",
    "        \"\"\"\n",
    "        print(\"====================== decode function ========================\")\n",
    "        # print(\"subword_probs: \", subword_probs)\n",
    "        # print(\"input_ids: \", input_ids)\n",
    "        # print(\"mt_offsets: \", mt_offsets)\n",
    "        decoded_output = []\n",
    "        decoded_output_corrected = []\n",
    "        print(\"now the word_level_prob function is being called: \")\n",
    "        word_level_prob, all_tokenized_sentences = self.word_level_prob(subword_probs,word_id)\n",
    "        #all_tokenized_sentences:List[List[str]]\n",
    "        print(\"all_tokenized_sentences: \", all_tokenized_sentences)\n",
    "        #print(\"length of mt_offsets: \", len(mt_offsets))\n",
    "        for i in range(len(mt_offsets)):\n",
    "            print(\"the value of i is ========= before inner for loop: \", i)\n",
    "            seq_len = len(mt_offsets[i])\n",
    "            #print(\"seq_len: \", seq_len)\n",
    "            error_spans, in_span, span = [], False, {}\n",
    "\n",
    "            track_token_to_words = []\n",
    "            count_index = 0\n",
    "            for token_id, probs, token_offset in zip(\n",
    "                input_ids[i, :seq_len], subword_probs[i][:seq_len], mt_offsets[i]\n",
    "            ):  \n",
    "                #print(\"the value of i is: \", i)\n",
    "                #print(\"token_id :\", token_id, \", probs: \", probs, \", token_offset: \", token_offset)\n",
    "                if self.decoding_threshold:\n",
    "                    if torch.sum(probs[1:]) > self.decoding_threshold:\n",
    "                        \n",
    "                        print(\"token_id who has higher error sums than threshold: \",token_id)\n",
    "                        print(\"and sum of it: \", torch.sum(probs[1:]))\n",
    "                        probability, label_value = torch.topk(probs[1:], 1)\n",
    "                        label_value += 1  # offset from removing label 0\n",
    "                    else:\n",
    "                        print(\"token_id who has higher error sums than threshold: \",token_id)\n",
    "\n",
    "                        # This is just to ensure same format but at this point\n",
    "                        # we will only look at label 0 and its prob\n",
    "                        probability, label_value = torch.topk(probs[0], 1)\n",
    "                        #print(\"probs[0] =============\", probs[0])\n",
    "                else:\n",
    "                    print(\"no decoding threshold set\")\n",
    "                    probability, label_value = torch.topk(probs, 1)\n",
    "\n",
    "                # Some torch versions topk returns a shape 1 tensor with only\n",
    "                # a item inside\n",
    "                label_value = (\n",
    "                    label_value.item()\n",
    "                    if label_value.dim() < 1\n",
    "                    else label_value[0].item()\n",
    "                )\n",
    "                label = self.label_encoder.ids_to_label.get(label_value)\n",
    "                #print(\"===================================================\")\n",
    "                #print(\"label: \", label)\n",
    "                # Label set:\n",
    "                # O I-minor I-major\n",
    "                # Begin of annotation span\n",
    "                if label.startswith(\"I\") and not in_span:\n",
    "                    in_span = True\n",
    "                    span[\"tokens\"] = [\n",
    "                        token_id,\n",
    "                    ]\n",
    "                    span[\"severity\"] = label.split(\"-\")[1]\n",
    "                    span[\"offset\"] = list(token_offset)\n",
    "                    #span[\"offset_word] = [list(token_offset)]\n",
    "                    span[\"confidence\"] = [\n",
    "                        probability,\n",
    "                    ]\n",
    "                    span[\"check severity\"] = [label.split(\"-\")[1]]\n",
    "                    track_token_to_words.append(count_index)\n",
    "                    #span[\"word_indices\"] =  set(word_id) if isinstance(word_id, list) else {word_id},  # Track word indices\n",
    "                # Inside an annotation span\n",
    "                elif label.startswith(\"I\") and in_span:\n",
    "                    span[\"tokens\"].append(token_id)\n",
    "                    span[\"confidence\"].append(probability)\n",
    "                    # Update offset end\n",
    "                    span[\"offset\"][1] = token_offset[1]\n",
    "                    #span[\"offset_word] = [list(token_offset)]\n",
    "                    span[\"check severity\"].append(label.split(\"-\")[1])\n",
    "                    # if isinstance(word_id, list):\n",
    "                    #     span[\"word_indices\"].update(word_id)\n",
    "                    # else:\n",
    "                    #     span[\"word_indices\"].add(word_id)\n",
    "                    track_token_to_words.append(count_index)\n",
    "                # annotation span finished.\n",
    "                elif label == \"O\" and in_span:\n",
    "                    error_spans.append(span)\n",
    "                    in_span, span = False, {}\n",
    "                    track_token_to_words.append(-1)\n",
    "                #added by me\n",
    "                elif label == \"O\" and not in_span:\n",
    "                    track_token_to_words.append(-1)\n",
    "                count_index = count_index + 1\n",
    "\n",
    "            print(\"track_token_to_words: \", track_token_to_words)\n",
    "\n",
    "            sentence_output = []\n",
    "            for span in error_spans:\n",
    "                # # Collect unique word indices for the span\n",
    "                # unique_word_indices = sorted(set(span[\"word_indices\"]) - {None})  # Remove None safely\n",
    "\n",
    "                # # Extract words belonging to this specific span\n",
    "                # span_words = [tokenized_words[idx] for idx in unique_word_indices]\n",
    "\n",
    "                sentence_output.append(\n",
    "                    {\n",
    "                        \n",
    "                        \"text\": self.encoder.tokenizer.decode(span[\"tokens\"]),\n",
    "                        #\"text\": \" \".join(span_words),  # Use words instead of tokens\n",
    "                        \"confidence\": torch.concat(span[\"confidence\"]).mean().item(),\n",
    "                        \"severity\": span[\"severity\"],\n",
    "                        \"start\": span[\"offset\"][0],\n",
    "                        \"end\": span[\"offset\"][1],\n",
    "                        \"check severity\": span[\"check severity\"]\n",
    "                    }\n",
    "                )\n",
    "            decoded_output.append(sentence_output)\n",
    "            print(\"sentence_output: \", sentence_output)\n",
    "            \n",
    "            #get corrected word level error span\n",
    "            corrected_span = self.correct_span(track_token_to_words,mt_offsets[i], word_id[\"words_id\"][i],all_tokenized_sentences[i])\n",
    "            sentence_out_word_level = []\n",
    "            count = 0\n",
    "            for error_span in sentence_output:\n",
    "                sentence_out_word_level.append({\n",
    "                    \"text\": corrected_span[count]['text'],\n",
    "                    \"confidence\": error_span['confidence'],\n",
    "                    \"severity\": error_span[\"severity\"],\n",
    "                    \"start\": corrected_span[count]['start'],\n",
    "                    \"end\": corrected_span[count]['end'],\n",
    "                })\n",
    "                count += 1\n",
    "                    \n",
    "            print(\"sentence_out_word_level: \",sentence_out_word_level)\n",
    "            decoded_output_corrected.append(sentence_out_word_level)\n",
    "        print(\"decoded_output_corrected\", decoded_output_corrected)\n",
    "        #decoded_output.append(decoded_output_corrected)\n",
    "        return decoded_output, decoded_output_corrected, word_level_prob\n",
    "\n",
    "    \n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: Dict[str, torch.Tensor],\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Prediction:\n",
    "        \"\"\"PyTorch Lightning predict_step\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, torch.Tensor]): The output of your prepare_sample function\n",
    "            batch_idx (Optional[int], optional): Integer displaying which batch this is\n",
    "                    Defaults to None.\n",
    "            dataloader_idx (Optional[int], optional): Integer displaying which\n",
    "                dataloader this is. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Prediction: Model Prediction\n",
    "        \"\"\"\n",
    "        # # Get the caller's function name and file location\n",
    "        # caller_frame = inspect.stack()[1]\n",
    "        # caller_function = caller_frame.function\n",
    "        # caller_file = caller_frame.filename\n",
    "    \n",
    "        # # Print caller details\n",
    "        # print(f\"predict_step called by: {caller_function} (from {caller_file})\")\n",
    "        if len(batch) == 4: # after adding word_ids, the batch length will increase by 1S\n",
    "            print(\"batch: \", batch)\n",
    "            print(\"i am inside when len of the batch is 3\")\n",
    "            #print(\"word ids: \", batch[-1])\n",
    "            # predictions = [self.forward(**input_seq) for input_seq in batch]\n",
    "            # now the length of the batch is 4, 4th dictionary is the words_id \n",
    "            #dictionary, and we will skip the forward call to it\n",
    "            predictions = [self.forward(**input_seq) for input_seq in batch[:-1]]\n",
    "            \n",
    "            #print(\"predictions: \", predictions)\n",
    "            avg_scores = torch.stack([pred.score for pred in predictions], dim=0).mean(dim=0)\n",
    "            #print(\"avg scores\", avg_scores)\n",
    "            batch_prediction = Prediction(\n",
    "                scores=avg_scores,\n",
    "                metadata=Prediction(\n",
    "                    src_scores=predictions[0].score,\n",
    "                    ref_scores=predictions[1].score,\n",
    "                    unified_scores=predictions[2].score,\n",
    "                ),\n",
    "            )\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max() \n",
    "                subword_probs = [\n",
    "                    nn.functional.softmax(o.logits, dim=2)[:, :seq_len, :] * w\n",
    "                    for w, o in zip(self.input_weights_spans, predictions)\n",
    "                ]\n",
    "                subword_probs = torch.sum(torch.stack(subword_probs), dim=0)\n",
    "                \n",
    "                ########################################################3\n",
    "                ## create error span using decode function\n",
    "                word_ids = batch[-1].copy()\n",
    "                error_spans, corrected_error_spans, word_level_prob = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"], word_ids\n",
    "                    #,word_ids, word_mapping\n",
    "                )\n",
    "                batch_prediction.metadata[\"error_spans\"] = error_spans\n",
    "                batch_prediction.metadata[\"corrected_error_spans\"] = corrected_error_spans\n",
    "                batch_prediction.metadata[\"word_level_probability\"]=word_level_prob\n",
    "        else:\n",
    "            print(\"i am inside when len of the batch is not 3\")\n",
    "            model_output = self.forward(**batch[0])\n",
    "            batch_prediction = Prediction(scores=model_output.score)\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = nn.functional.softmax(model_output.logits, dim=2)[:, :seq_len, :]\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction = Prediction(\n",
    "                    scores=model_output.score,\n",
    "                    metadata=Prediction(error_spans=error_spans),\n",
    "                )\n",
    "        return batch_prediction\n",
    "\n",
    "# Load checkpoint into your custom class\n",
    "path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(path,strict = False)\n",
    "data = [\n",
    "    {\n",
    "        \"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n",
    "        \"mt\": \"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\",\n",
    "        \"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n",
    "    }\n",
    "]\n",
    "# data = [\n",
    "#     {\n",
    "#         \"src\": \"10 到 15 分钟可以送到吗\",\n",
    "#         \"mt\": \"Can I receive my food in 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"src\": \"Pode ser entregue dentro de 10 a 15 minutos?\",\n",
    "#         \"mt\": \"Can you send it for 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     }\n",
    "# ]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)\n",
    "\n",
    "#added by me\n",
    "print (model_output.metadata.corrected_error_spans)\n",
    "print (model_output.metadata.word_level_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08a596-ed05-4bd8-9084-2bd183591135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d5f4f-bce7-4a98-a396-f2caa58fedbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
