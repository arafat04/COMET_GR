{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7881b545-7b37-4549-84a7-227e49f8dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-7a094b53-c5ef-9efd-bb6d-2ffdd67ae03d]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8439337015151978]\n",
      "0.8439337015151978\n",
      "[[{'text': 'ist', 'confidence': 0.45241352915763855, 'severity': 'minor', 'start': 13, 'end': 17}, {'text': 'y-Abgeordneten völlig in der Gunst', 'confidence': 0.43448933959007263, 'severity': 'minor', 'start': 25, 'end': 59}]]\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "from comet.models.utils import Prediction\n",
    "from typing import Dict, Optional\n",
    "from typing import List, Dict\n",
    "from typing import Union, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import inspect \n",
    "import torch\n",
    "import torch.nn as nn  # <-- Add thiss\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "\n",
    "    \n",
    "    def word_level_prob(\n",
    "        self,\n",
    "        subword_probs: torch.Tensor,\n",
    "        batch\n",
    "    ) -> List[List[Dict[str, float]]]:\n",
    "        \"\"\" Returns word level probability score\n",
    "        word_ids = {\n",
    "    'words_id': [\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "        [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "    ],\n",
    "    'mt_sentences': [\n",
    "        \"Can I receive my food in 10 to 15 minutes?\",\n",
    "        \"Can you send it for 10 to 15 minutes?\"\n",
    "    ],\n",
    "    'mt_sentences_tokenized': [\n",
    "        {\n",
    "            'input_ids': tensor([\n",
    "                [0, 4171, 87, 53299, 759, 15381, 23, 209, 47, 423, 14633, 32, 2],\n",
    "                [0, 4171, 398, 25379, 442, 100, 209, 47, 423, 14633, 32, 2, 1]\n",
    "            ], device='cuda:0'),\n",
    "            'label_ids': tensor([\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n",
    "            ], device='cuda:0'),\n",
    "            'attention_mask': tensor([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "            ], device='cuda:0'),\n",
    "            'offsets': [\n",
    "                [(0, 0), (0, 3), (3, 5), (5, 13), (13, 16), (16, 21), \n",
    "                 (21, 24), (24, 27), (27, 30), (30, 33), (33, 41), (41, 42), (0, 0)],\n",
    "                [(0, 0), (0, 3), (3, 7), (7, 12), (12, 15), (15, 19), \n",
    "                 (19, 22), (22, 25), (25, 28), (28, 36), (36, 37), (0, 0)]\n",
    "            ],\n",
    "            'word_ids': [\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, None],\n",
    "                [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n",
    "            ]\n",
    "            }\n",
    "                ]\n",
    "                    }\n",
    "\n",
    "        \"\"\"\n",
    "        tokenizer = self.encoder.tokenizer\n",
    "        # ====== Reconstruct MT sentence from batch ======\n",
    "        input_ids = batch[0][\"input_ids\"]  # Tokenized MT input\n",
    "        mt_sentence = self.encoder.tokenizer.decode(\n",
    "                input_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        print(\"mt sentence:++++++++++++++++++ \", mt_sentence)\n",
    "\n",
    "        \n",
    "        ## run over the mt sentences in the dict word_ids\n",
    "        word_level_prob = []\n",
    "        all_tokenized_sentences = []\n",
    "       \n",
    "        # Tokenize the MT sentence to get subword-to-token alignment\n",
    "        tokenized = self.encoder.tokenizer(\n",
    "                mt_sentence,\n",
    "                return_offsets_mapping=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                        \n",
    "        )\n",
    "\n",
    "        #print(\"tokenized: \", tokenized)\n",
    "        subword_ids = tokenized.word_ids()\n",
    "        #subword_ids = subword_ids[:seq_len]\n",
    "        #subword_ids[-1] = None\n",
    "        #print(\"subword_ids after mt sentence extractions: \", subword_ids)\n",
    "        # Group subword probabilities by original tokens\n",
    "        #subword_ids = item\n",
    "        token_probs = {}\n",
    "        #print(f\"subword_probs[{index}]:  {subword_probs[index]}\")\n",
    "        #attention_mask = word_ids[\"mt_sentences_tokenized\"][0][\"attention_mask\"][index] #did not use this\n",
    "        for idx, prob in enumerate(subword_probs[0]):\n",
    "            # if attention_mask[index] == 0:\n",
    "            #     break\n",
    "            if idx >= len(subword_ids):\n",
    "                break\n",
    "            subword_idx = subword_ids[idx]\n",
    "            if subword_idx is None:  # Skip special tokens\n",
    "                continue\n",
    "            if subword_idx not in token_probs:\n",
    "                token_probs[subword_idx] = []\n",
    "            token_probs[subword_idx].append(prob.cpu().numpy())\n",
    "        print(\"token_probs: \", token_probs)\n",
    "\n",
    "        # Aggregate probabilities (average for each class)\n",
    "        token_level_probs = []\n",
    "        for token_idx in sorted(token_probs.keys()):\n",
    "            # Stack subword probabilities for this token\n",
    "            subword_probs_for_token = torch.stack([torch.tensor(p) for p in token_probs[token_idx]])\n",
    "\n",
    "            # Compute mean across subwords (dim=0 → average over subwords, per class)\n",
    "            mean_probs = torch.mean(subword_probs_for_token, dim=0)\n",
    "\n",
    "            token_level_probs.append(mean_probs.numpy())\n",
    "        print(\"token_level_probs: \", token_level_probs)\n",
    "        # After computing token_level_probs:\n",
    "        # Tolerance for floating-point errors (e.g., 1e-3 = 0.1% tolerance)\n",
    "        tolerance = 1e-3\n",
    "        for token_idx, probs in enumerate(token_level_probs):\n",
    "            total = numpy.sum(probs)\n",
    "            if not numpy.isclose(total, 1.0, atol=tolerance):\n",
    "                print(f\"Token {token_idx} probabilities sum to {total:.4f} (expected ~1.0)\")\n",
    "            else:\n",
    "                print(f\"Token {token_idx} probabilities sum to {total:.4f}\")\n",
    "                    \n",
    "        # Extract word IDs (index of the original word for each token)\n",
    "        #word_ids = tokenized.word_ids()[:mt_length]    #  [None, 0, 0, 1, 1, 2, ...]\n",
    "        # Convert token IDs to tokens (subwords)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "        print(\"tokens: \",tokens)\n",
    "        # Group tokens by their word ID\n",
    "        word_to_tokens = {}\n",
    "        #print(\"word_ids: \", subword_ids)\n",
    "        for idx, word_id in enumerate(subword_ids):\n",
    "            if word_id is None:\n",
    "                continue  # Skip special tokens like [CLS], [SEP]\n",
    "            if word_id not in word_to_tokens:\n",
    "                word_to_tokens[word_id] = []\n",
    "            word_to_tokens[word_id].append(tokens[idx])\n",
    "        print(\"word_to_tokens: \", word_to_tokens)\n",
    "        print(\"sorted(word_to_tokens.keys()) :\", sorted(word_to_tokens.keys()))\n",
    "        # Reconstruct original words from grouped tokens\n",
    "        word_mapping = []\n",
    "        for word_id in sorted(word_to_tokens.keys()):\n",
    "            tokens = word_to_tokens[word_id]\n",
    "            # Merge subwords into a single string (handles ## prefixes)\n",
    "            word = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "            word_mapping.append(word)\n",
    "        # Print results\n",
    "        print(\"Tokenized Words:\", word_mapping)\n",
    "        all_tokenized_sentences.append(word_mapping)\n",
    "        # Map tokens to probabilities\n",
    "        token_predictions = [\n",
    "                {\"token\": token, \"probabilities\": probs.tolist()}\n",
    "                for token, probs in zip(word_mapping, token_level_probs)\n",
    "        ]\n",
    "        # print(\"Token-Level Probabilities:\")\n",
    "        for pred in token_predictions:\n",
    "            print(f\"{pred['token']}: {pred['probabilities']}\")\n",
    "        print(\"first sentence finished=====================\")\n",
    "        word_level_prob.append(token_predictions)\n",
    "        return word_level_prob, all_tokenized_sentences\n",
    "\n",
    "    \n",
    "    #word_ids = batch[-1].copy()\n",
    "    #           word_level_prob, all_tokenized_sentences = self.word_level_prob(subword_probs,word_ids)\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: Dict[str, torch.Tensor],\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Prediction:\n",
    "        \"\"\"PyTorch Lightning predict_step\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, torch.Tensor]): The output of your prepare_sample function\n",
    "            batch_idx (Optional[int], optional): Integer displaying which batch this is\n",
    "                Defaults to None.\n",
    "            dataloader_idx (Optional[int], optional): Integer displaying which\n",
    "                dataloader this is. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Prediction: Model Prediction\n",
    "        \"\"\"\n",
    "        if len(batch) == 3:\n",
    "            predictions = [self.forward(**input_seq) for input_seq in batch]\n",
    "            # Final score is the average of the 3 scores!\n",
    "            avg_scores = torch.stack([pred.score for pred in predictions], dim=0).mean(\n",
    "                dim=0\n",
    "            )\n",
    "            batch_prediction = Prediction(\n",
    "                scores=avg_scores,\n",
    "                metadata=Prediction(\n",
    "                    src_scores=predictions[0].score,\n",
    "                    ref_scores=predictions[1].score,\n",
    "                    unified_scores=predictions[2].score,\n",
    "                ),\n",
    "            )\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = [\n",
    "                    nn.functional.softmax(o.logits, dim=2)[:, :seq_len, :] * w\n",
    "                    for w, o in zip(self.input_weights_spans, predictions)\n",
    "                ]\n",
    "                subword_probs = torch.sum(torch.stack(subword_probs), dim=0)\n",
    "                word_level_prob, all_tokenized_sentences = self.word_level_prob(subword_probs, batch)\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction.metadata[\"error_spans\"] = error_spans\n",
    "\n",
    "        else:\n",
    "            model_output = self.forward(**batch[0])\n",
    "            batch_prediction = Prediction(scores=model_output.score)\n",
    "            if self.word_level:\n",
    "                mt_mask = batch[0][\"label_ids\"] != -1\n",
    "                mt_length = mt_mask.sum(dim=1)\n",
    "                seq_len = mt_length.max()\n",
    "                subword_probs = nn.functional.softmax(model_output.logits, dim=2)[\n",
    "                    :, :seq_len, :\n",
    "                ]\n",
    "                error_spans = self.decode(\n",
    "                    subword_probs, batch[0][\"input_ids\"], batch[0][\"mt_offsets\"]\n",
    "                )\n",
    "                batch_prediction = Prediction(\n",
    "                    scores=model_output.score,\n",
    "                    metadata=Prediction(error_spans=error_spans),\n",
    "                )\n",
    "        return batch_prediction\n",
    "# Load checkpoint into your custom class\n",
    "path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = load_from_checkpoint(path)\n",
    "# from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "# model = load_from_checkpoint(model_path)\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n",
    "        \"mt\": \"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\",\n",
    "        #\"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n",
    "    }\n",
    "]\n",
    "# data = [\n",
    "#     {\n",
    "#         \"src\": \"10 到 15 分钟可以送到吗\",\n",
    "#         \"mt\": \"Can I receive my food in 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"src\": \"Pode ser entregue dentro de 10 a 15 minutos?\",\n",
    "#         \"mt\": \"Can you send it for 10 to 15 minutes?\",\n",
    "#         \"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     }\n",
    "# ]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d342692-91f4-42c4-8de2-9c9f84942b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
