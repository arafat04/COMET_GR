{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31984b68-9df0-48bd-b0e9-54024ad469e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f207b3-818e-4a2b-9daa-47dc0f241754",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_edit_file = \"/storage/brno2/home/rahmang/xcomet/arafat_comet/COMET_GR/postedition_aligned.community.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a45a169-a0e0-4220-a93d-2f29a55f8561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id_hal  Translation_id  Postedit_id  line_id  \\\n",
      "0  1988871            7033          402        0   \n",
      "1  1988871            7033          402        1   \n",
      "2  1988871            7033          402        2   \n",
      "3  1988871            7033          402        3   \n",
      "4  1988871            7033          402        4   \n",
      "5  1988871            7033          402        5   \n",
      "6  1821049            6181          159        0   \n",
      "7  1821049            6181          159        1   \n",
      "8  1821049            6181          159        2   \n",
      "9  1821049            6181          159        3   \n",
      "\n",
      "                                              source  \\\n",
      "0  Transforming Dependency Structures to LTAG Der...   \n",
      "1  We propose a new algorithm for parsing Lexical...   \n",
      "2  That is, given a sentence and its correspondin...   \n",
      "3  Moreover, we prove that this algorithm has a l...   \n",
      "4  This algorithm returns all compatible derivati...   \n",
      "5  This result is of practical interest to the de...   \n",
      "6  Corpus Based Machine Translation for Scientifi...   \n",
      "7  From many years, machine translation and compu...   \n",
      "8  In order to fulfill the goal of machine transl...   \n",
      "9  All of these translation methods differ in the...   \n",
      "\n",
      "                                         translation  \\\n",
      "0  Transformation de structures de dépendances en...   \n",
      "1  Nous proposons un nouvel algorithme pour analy...   \n",
      "2  C'est-à-dire, compte tenu d'une phrase et de s...   \n",
      "3  Par ailleurs, on prouve que cet algorithme pré...   \n",
      "4  Cet algorithme retourne tous les arbres de dér...   \n",
      "5  Ce résultat présente un intérêt pratique pour ...   \n",
      "6  Traduction automatique basée sur Corpus pour t...   \n",
      "7  Depuis de nombreuses années, la communauté de ...   \n",
      "8  Afin d'atteindre l'objectif de la traduction a...   \n",
      "9  Toutes ces méthodes de traduction diffèrent da...   \n",
      "\n",
      "                                         postedition  \n",
      "0  Transformation de structures de dépendances en...  \n",
      "1  Nous proposons un nouvel algorithme pour analy...  \n",
      "2  C'est-à-dire, étant donné une phrase et de sa ...  \n",
      "3  Par ailleurs, on prouve que cet algorithme a u...  \n",
      "4  Cet algorithme retourne tous les arbres de dér...  \n",
      "5  Ce résultat présente un intérêt pratique pour ...  \n",
      "6  Traduction automatique basée sur corpus pour l...  \n",
      "7  Depuis de nombreuses années, la communauté des...  \n",
      "8  Afin d'atteindre l'objectif de la traduction a...  \n",
      "9  Toutes ces méthodes de traduction diffèrent au...  \n"
     ]
    }
   ],
   "source": [
    "# Read TSV file into DataFrame\n",
    "df = pd.read_csv(post_edit_file, sep='\\t')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe9409f-bff5-4c79-a0cc-b85a577134ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_hal', 'Translation_id', 'Postedit_id', 'line_id', 'source',\n",
       "       'translation', 'postedition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7382b811-31c7-4604-a1c4-93eef3078120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362e2236-1ce0-40d3-b700-d5e430f06cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Postedit_id and line_id, collect translations into lists\n",
    "grouped = df.groupby(['Postedit_id', 'line_id'])['translation'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c1eed7-5a9e-49e1-a17d-b67d85dc0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postedit_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bien que les récents changements apportés à l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>[Cet article propose un nouveau type d'évaluat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>[Notre approche utilise des paires de phrases ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>[Cette méthodologie est utilisée pour comparer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Postedit_id  line_id                                        translation\n",
       "0           60        0  [Évaluation de la compétence morphologique des...\n",
       "1           60        1  [Bien que les récents changements apportés à l...\n",
       "2           60        2  [Cet article propose un nouveau type d'évaluat...\n",
       "3           60        3  [Notre approche utilise des paires de phrases ...\n",
       "4           60        4  [Cette méthodologie est utilisée pour comparer..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec77fd90-65c1-46e1-8c4c-39d8d7d547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by Postedit_id and aggregate translations/postedits into flat lists\n",
    "grouped_df = df.groupby(\"Postedit_id\").agg(\n",
    "    source=(\"source\", list), # list of source sentences against the same Postedit_id\n",
    "    translation=(\"translation\", list),  # Collect all translations\n",
    "    postedition=(\"postedition\", list)           # Collect all posteds (replace \"postedit\" with your actual column name)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313d428b-ad63-494d-88dd-08ca835e24c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postedit_id</th>\n",
       "      <th>source</th>\n",
       "      <th>translation</th>\n",
       "      <th>postedition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>[Evaluating the morphological competence of Ma...</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "      <td>[Évaluation de la compétence morphologique des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>[One Source, Two Targets: Challenges and Rewar...</td>\n",
       "      <td>[Une source, deux cibles: Défis et récompenses...</td>\n",
       "      <td>[Une source, deux cibles: Défis et bénéfices d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>[Neural Baselines for Word Alignments, Word al...</td>\n",
       "      <td>[Lignes de base neuronales pour les alignement...</td>\n",
       "      <td>[Modèles neuronaux de base pour l'alignement d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>[Multilingual Lexicalized Constituency Parsing...</td>\n",
       "      <td>[Lexicalized Constituency Parsing multilingue ...</td>\n",
       "      <td>[Tâches auxiliaires au niveau des mots pour l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>[Novel elicitation and annotation schemes for ...</td>\n",
       "      <td>[Nouveaux schémas d'élicitation et d'annotatio...</td>\n",
       "      <td>[Nouveaux schémas d'élicitation et d'annotatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Postedit_id                                             source  \\\n",
       "0           60  [Evaluating the morphological competence of Ma...   \n",
       "1           61  [One Source, Two Targets: Challenges and Rewar...   \n",
       "2           62  [Neural Baselines for Word Alignments, Word al...   \n",
       "3           80  [Multilingual Lexicalized Constituency Parsing...   \n",
       "4           82  [Novel elicitation and annotation schemes for ...   \n",
       "\n",
       "                                         translation  \\\n",
       "0  [Évaluation de la compétence morphologique des...   \n",
       "1  [Une source, deux cibles: Défis et récompenses...   \n",
       "2  [Lignes de base neuronales pour les alignement...   \n",
       "3  [Lexicalized Constituency Parsing multilingue ...   \n",
       "4  [Nouveaux schémas d'élicitation et d'annotatio...   \n",
       "\n",
       "                                         postedition  \n",
       "0  [Évaluation de la compétence morphologique des...  \n",
       "1  [Une source, deux cibles: Défis et bénéfices d...  \n",
       "2  [Modèles neuronaux de base pour l'alignement d...  \n",
       "3  [Tâches auxiliaires au niveau des mots pour l'...  \n",
       "4  [Nouveaux schémas d'élicitation et d'annotatio...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bbc2b5-f7dd-429a-b49b-b7eb8f18d400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36fcb59-37eb-460f-bb52-78bceb2c2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Postedit IDs in the original df: 95\n"
     ]
    }
   ],
   "source": [
    "num_unique_ids = df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the original df: {num_unique_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e98ca14-18df-4bfc-a430-5e9f5c52abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Postedit IDs in the grouped_df: 95\n"
     ]
    }
   ],
   "source": [
    "num_unique_ids_grouped_df = grouped_df['Postedit_id'].nunique()\n",
    "print(f\"Number of unique Postedit IDs in the grouped_df: {num_unique_ids_grouped_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4bac853-fb10-425e-8238-ca1e8a8949a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neural Baselines for Word Alignments',\n",
       " 'Word alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation.',\n",
       " 'In most areas of natural language processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word alignment models.',\n",
       " 'In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models.',\n",
       " 'We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts.',\n",
       " 'We also analyze typical alignment errors of the baselines that our models overcome to illustrate the benefits --- and the limitations --- of these new models for morphologically rich languages.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['source'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc38d56-e964-4826-ae16-7851a0f37b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lignes de base neuronales pour les alignements de mots',\n",
       " \"Les alignements de mots identifient les correspondances traductionnelles entre les mots d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour effectuer une estimation de la qualité.\",\n",
       " \"Dans la plupart des domaines du traitement du langage naturel, les modèles de réseaux neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.\",\n",
       " \"Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux pour l'alignement de mots non supervisé pour quatre paires de langues, en comparant plusieurs variantes de modèles neuronaux.\",\n",
       " 'Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalents discrets.',\n",
       " \"Nous analysons également les erreurs d'alignement typiques des lignes de base que nos modèles surmontent pour illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['translation'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634ca612-ee12-4cdc-b911-e481be087ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_df['translation'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e6e105-9ab4-4155-acf2-d4d0fd1f9e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Modèles neuronaux de base pour l'alignement de mots\",\n",
       " \"Les alignements de mots identifient les correspondances traductionnelles entre mots au sein d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour estimer la qualité d'une traduction.\",\n",
       " \"Dans la plupart des domaines du traitement des langues, les modèles neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots.\",\n",
       " \"Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux d'alignement de mots non supervisés pour quatre paires de langues, en comparant plusieurs variantes de ces modèles.\",\n",
       " 'Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalentes discrètes.',\n",
       " \"Nous analysons également les erreurs d'alignement typiques des modèles de base que les versions neuronales surmontent afin d'illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['postedition'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86af496a-84ce-46ff-9db6-90a5b317593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "    \n",
    "# # Load checkpoint into your custom class\n",
    "# path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "# model = load_from_checkpoint(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0226a9f-0aa1-48de-b555-623dc1082972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "# model = load_from_checkpoint(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6abfbfd-7b45-4719-adca-6c259502e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239d024a-ecca-4293-b9bd-a407410ae368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 7145.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "#model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9b0a31-cd4a-497d-9847-2919a92bcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom unified_metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "class CustomXCOMET(UnifiedMetric):\n",
    "    print(\"custom unified_metric\")\n",
    "    \n",
    "# Load checkpoint into your custom class\n",
    "#path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "model = CustomXCOMET.load_from_checkpoint(model_path,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a306c7e-1615-41ec-bd98-fbe5cfc66bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-cb604f23-af18-ccc6-f07f-d40b1562bed4]\n",
      "/storage/brno2/home/rahmang/envs/xcomet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6365968585014343]\n",
      "0.6365968585014343\n",
      "[[{'text': ['ist', 'bei', 'Tory-Abgeordneten', 'völlig', 'in', 'der', 'Gunst'], 'confidence': 0.46279841661453247, 'severity': 'critical', 'start': 13, 'end': 59, 'check severity': ['critical', 'critical', 'major', 'critical', 'critical', 'critical', 'critical', 'critical']}]]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": \"To do so, we propose to map the ontol-ogy elements to multilingual knowledge defined in a SKOS ontology.\", \n",
    "        \"mt\": \"Pour ce faire, nous proposons de mettre en correspondance les éléments d'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\", \n",
    "        \"ref\": \"Pour ce faire, nous proposons de mettre en correspondance les éléments de l'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.\"\n",
    "    }\n",
    "]\n",
    "# data = [\n",
    "#     {\n",
    "#         \"src\": \"10 到 15 分钟可以送到吗\",\n",
    "#         \"mt\": \"Can I receive my food in 10 to 15 minutes?\",\n",
    "#         #\"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"src\": \"Pode ser entregue dentro de 10 a 15 minutos?\",\n",
    "#         \"mt\": \"Can you send it for 10 to 15 minutes?\",\n",
    "#         #\"ref\": \"Can it be delivered between 10 to 15 minutes?\"\n",
    "#     }\n",
    "# ]\n",
    "data = [\n",
    "    {\n",
    "        \"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n",
    "        \"mt\": \"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst\",\n",
    "        \"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n",
    "    }\n",
    "]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "# Segment-level scores\n",
    "print (model_output.scores)\n",
    "\n",
    "# System-level score\n",
    "print (model_output.system_score)\n",
    "\n",
    "# Score explanation (error spans)\n",
    "print (model_output.metadata.error_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb8b8547-d434-40da-9cae-ccd696f9a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'word': 'Boris', 'probabilities': [0.5845839381217957, 0.1495356559753418, 0.15199054777622223, 0.11388987302780151]}, {'word': 'Johnson', 'probabilities': [0.6237878799438477, 0.13660618662834167, 0.1444559097290039, 0.09515005350112915]}, {'word': 'ist', 'probabilities': [0.12797817587852478, 0.15131376683712006, 0.22207924723625183, 0.49862879514694214]}, {'word': 'bei', 'probabilities': [0.31826817989349365, 0.14464114606380463, 0.21661992371082306, 0.32047075033187866]}, {'word': 'Tory-Abgeordneten', 'probabilities': [0.41008082032203674, 0.23535537719726562, 0.2197144329547882, 0.13484938442707062]}, {'word': 'völlig', 'probabilities': [0.11040766537189484, 0.14954930543899536, 0.22367152571678162, 0.516371488571167]}, {'word': 'in', 'probabilities': [0.11297599226236343, 0.13117052614688873, 0.18996915221214294, 0.5658842921257019]}, {'word': 'der', 'probabilities': [0.1128077507019043, 0.13989263772964478, 0.2106526792049408, 0.5366469621658325]}, {'word': 'Gunst', 'probabilities': [0.14387768507003784, 0.15015417337417603, 0.2106073498725891, 0.4953608214855194]}]]\n"
     ]
    }
   ],
   "source": [
    "print (model_output.metadata.word_level_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2ebc00-d88c-4b53-b1c2-bd55192b8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a09f139-3860-4fc9-9474-0cff8d13ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critical\n",
      "critical\n",
      "minor\n",
      "minor\n",
      "major\n",
      "major\n",
      "critical\n",
      "critical\n",
      "critical\n",
      "critical\n",
      "defaultdict(None, {'critical': 3, 'minor': 1, 'major': 2})\n"
     ]
    }
   ],
   "source": [
    "severity_ranking = {\n",
    "            \"minor\":1,\n",
    "            \"major\":2,\n",
    "            \"critical\":3,\n",
    "        }\n",
    "severity = ['critical', 'critical', 'minor', 'minor', 'major', 'major', 'critical', 'critical', 'critical', 'critical']\n",
    "severity_dict = defaultdict()\n",
    "for sev in severity:\n",
    "    if sev in severity_ranking:\n",
    "        print(sev)\n",
    "        severity_dict[sev] = severity_ranking[sev]\n",
    "print(severity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43ae51af-cfb5-45c9-aee4-e9c5c1704516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critical\n"
     ]
    }
   ],
   "source": [
    "original_dict = severity_dict\n",
    "max_key = max(original_dict, key=lambda k: original_dict[k])\n",
    "print(max_key)  # Output: 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0087c0f-edb9-4de2-aaa9-4444026b1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minor': 1, 'major': 2, 'critical': 3}\n"
     ]
    }
   ],
   "source": [
    "sorted_dict = dict(sorted(severity_dict.items(), key=lambda item: item[1]))\n",
    "print(sorted_dict)  # Output: {'b': 1, 'c': 2, 'a': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94880a06-3832-433c-8055-ac6f07cb4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# from comet.models.multitask.unified_metric import UnifiedMetric\n",
    "# class CustomXCOMET(UnifiedMetric):\n",
    "#     print(\"custom unified_metric\")\n",
    "    \n",
    "# # Load checkpoint into your custom class\n",
    "# path = \"/storage/brno2/home/rahmang/xcomet/downloadedxcomet/models--Unbabel--XCOMET-XL/snapshots/50d428488e021205a775d5fab7aacd9502b58e64/checkpoints/model.ckpt\"\n",
    "\n",
    "# model = CustomXCOMET.load_from_checkpoint(path,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68159f82-7f70-460f-8dff-03cd5782fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate and extract values from two columns\n",
    "# data = []\n",
    "# for index, row in df.iterrows():\n",
    "#     Postedit_id = row['Postedit_id']\n",
    "#     source      = row['source']\n",
    "#     translation = row['translation']\n",
    "#     postedition  = row['postedition']\n",
    "#     #print(\"index number of the dataset: \", index)\n",
    "#     #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "    \n",
    "#     src = sentence\n",
    "#     mt  = translation[i]\n",
    "#     ref = postedition[i]\n",
    "#     data.append({\n",
    "#         \"src\": src,\n",
    "#         \"mt\" : mt,\n",
    "#         \"ref\": ref,\n",
    "#     })\n",
    "\n",
    "# model_output = model.predict(data, batch_size=8, gpus=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79245868-01e0-491c-ba7e-bcc8529dfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (model_output.scores)\n",
    "\n",
    "# # System-level score\n",
    "# print (model_output.system_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1509b2-d37b-4b8c-b267-6ba2760134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed7a69c1-dfa6-4f55-bfda-b81554f785e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# paragraph = defaultdict()\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Postedit_id'] not in paragraph.keys():\n",
    "#         source      = row['source']\n",
    "#         translation = row['translation']\n",
    "#         postedition  = row['postedition']\n",
    "#         paragraph[row['Postedit_id']] = {'src': source,\n",
    "#                                          'mt': translation,\n",
    "#                                          'ref': postedition\n",
    "#                                         }\n",
    "#     else:\n",
    "#         source      += row['source']\n",
    "#         translation += row['translation']\n",
    "#         postedition  += row['postedition']\n",
    "#         paragraph[row['Postedit_id']] = {'src': source,\n",
    "#                                          'mt': translation,\n",
    "#                                          'ref': postedition\n",
    "#                                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79e0b3b7-08ef-412d-99a0-17557274cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1679fcb5-1019-40e5-bb00-825d49ca5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate and extract values from two columns\n",
    "# data = []\n",
    "# count = 0\n",
    "# for index, value in paragraph.items():\n",
    "#     data.append(value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c737683-30d2-4a3f-8a0e-f010d63b3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e89606a3-781a-447b-9f95-2c37a225674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for item in data:\n",
    "#     print(\"=====================count=========================:\", count)\n",
    "#     count += 1\n",
    "#     model_output = model.predict([item], batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0592298b-e1c3-4fd9-adb6-e3e5f62748ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate and extract values from two columns\n",
    "# data = []\n",
    "# count = 0\n",
    "# for index, row in grouped_df.iterrows():\n",
    "#     Postedit_id = row['Postedit_id']\n",
    "#     source      = row['source']\n",
    "#     translation = row['translation']\n",
    "#     postedition  = row['postedition']\n",
    "#     #print(\"index number of the dataset: \", index)\n",
    "#     #print(f\"Postedit ID: {Postedit_id}, Translation: {translation}\")\n",
    "#     src = \"\"\n",
    "#     mt  = \"\"\n",
    "#     ref = \"\"\n",
    "#     for i, sentence in enumerate(source):\n",
    "#         src += sentence\n",
    "#         mt  += translation[i]\n",
    "#         ref += postedition[i]\n",
    "#     data.append({\n",
    "#         \"src\": src,\n",
    "#         \"mt\" : mt,\n",
    "#         \"ref\": ref,\n",
    "#     })\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63f6dd0e-2733-44d2-ac2c-f8c582447667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = [data[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebc3b007-3248-4c77-961e-dffeddf08933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d400f61-233b-4a0f-bfd4-79179a1ec5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_output = model.predict(data, batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd071b23-b301-49f4-82e3-0d6ba741e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping=  {0: {'subwords': [1], 'offsets': [0, 4]}, 1: {'subwords': [2], 'offsets': [4, 7]}, 2: {'subwords': [3, 4], 'offsets': [7, 14]}, 3: {'subwords': [5], 'offsets': [14, 19]}, 4: {'subwords': [6], 'offsets': [19, 29]}, 5: {'subwords': [7], 'offsets': [29, 32]}, 6: {'subwords': [8], 'offsets': [32, 39]}, 7: {'subwords': [9], 'offsets': [39, 42]}, 8: {'subwords': [10, 11], 'offsets': [42, 57]}, 9: {'subwords': [12], 'offsets': [57, 61]}, 10: {'subwords': [13], 'offsets': [61, 70]}, 11: {'subwords': [14, 15, 16, 17], 'offsets': [70, 82]}, 12: {'subwords': [18], 'offsets': [82, 87]}, 13: {'subwords': [19], 'offsets': [87, 91]}, 14: {'subwords': [20, 21], 'offsets': [91, 105]}, 15: {'subwords': [22, 23, 24], 'offsets': [105, 118]}, 16: {'subwords': [25, 26], 'offsets': [118, 127]}, 17: {'subwords': [27], 'offsets': [127, 132]}, 18: {'subwords': [28], 'offsets': [132, 136]}, 19: {'subwords': [29, 30], 'offsets': [136, 146]}, 20: {'subwords': [31, 32, 33], 'offsets': [146, 152]}}\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a01f363-49e2-463e-b8cf-37bd212b43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, values in mapping.items():\n",
    "#     print(\"keys: \", key, \" and values: \", values)\n",
    "#     for item in values['subwords']:\n",
    "#         print(\"subwords in the subwords list: \", item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
